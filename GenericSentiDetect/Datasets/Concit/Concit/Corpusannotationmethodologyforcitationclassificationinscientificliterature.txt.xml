<?xml version="1.0" encoding="ISO-8859-1"?>
<annotatedpaper> 
    <paper title="Corpus annotation methodology for citation classification in scientific literature" author=" Myriam Hernandez Alvarez,Jose Gomez Soriano " email="myriam.hernandez@epn.edu.ec,jmgomez@ua.es"> 
        <section>
            Escuela Polititecnica Nacional 
            Facultad de Ingenieria de Sistemas 
            Quito, Ecuador 
            myriam.hernandez@epn.edu.ec	
            Universidad de Alicante 
            Dpto. de Lenguajes y Sistemas Informaticos 
            Alicante, España 
            jmgomez@ua.es 
        </section>
        <section>
            <title>
                Abstract 
            </title>
            <paragraph>
                Since, at the moment, there is not a gold-standard annotated corpus to allow generation and testing of automatic systems for classifying the purpose or function of a citation referenced in an article; it is necessary to build one, for this objective. The development of this kind of corpus is subject to two conditions: the first one is to present a clear and unambiguous classification scheme. The second one is to secure an initial manual process of labeling to reach a sufficient inter-coder agreement among annotators to validate the annotation scheme and to be able to reproduce it even with coders who do not know in depth the topic of the analyzed articles. This paper proposes and validates a methodology for corpus annotation for citation classification in scientific literature that facilitate annotation and produces substantial inter-annotator agreement. 
            </paragraph>
        </section>
        <section imrad="i">
            <title>
                1	Introduction 
            </title>
            <paragraph>
                Not all citations have the same effect in a citing article. The impact of a cited paper may vary considerably. It could go from being a criticism, or a starting point for a job or simply an acknowledge of the work of other authors. However, accepted methods available today are variations of citation counting where all citations are considered equal and are evaluated with the same weight. Current methods of measuring impact fall into one of three techniques: simple count of citations (more citations, more impact); co citation which adds as a measure of similarity between two works the number of common documents that cited them; and the Googleï¿½s PageRank that measure citation relevance using the relevance and frequency of the citing document. Not all citations are equal, so they should not weigh equally in the impact calculation. None of the above mentioned counting methods takes into account whether the citation context is positive or negative, the purpose of the citing article, or if the citation has or not have influence on it.   
            </paragraph>
            <paragraph>
                It becomes important to identify more complete metrics that take into account the content about cited work to assess its impact and relevance. It is necessary the construction of a new impact index enriched with qualitative criteria regarding the citation. This process requires a content analysis of the context containing citations to obtain certain important features such as intent or purpose of the citing author when made the reference.  
            </paragraph>
            <paragraph>
                <context>
                    Content analysis is a group of procedures to recollect and organized information in standard format to make inferences about its characteristics and meaning using manual or automatic methods  <cite id="1" function="ack" polarity="neu">(Ding, Zhang, Chambers, Song, Wang, and Zhai, 2014</cite>). 
                </context>
                This analysis could be automatic starting from a tagged corpus to build a model.    
            </paragraph>
            <paragraph>
                Since, there is not a gold-standard annotated corpus for citation analysis data, it is necessary to work in the generation of one in order to facilitate collaborative work and results comparison among researches. Development of a corpus starts from the definition of a citation classification scheme that considers function (purpose), polarity (disposition) and influence of cited paper to produce a reliable and reproducible data set that could be the basis for future work in this area. This tagged corpus will allow overcoming problems currently present that make very difficult to strengthen collaborative efforts in this field (Hernï¿½ndez y Gï¿½mez, 2014). Present problems are, for instance, the lack of a standard classification scheme and of sufficient public data available such that researchers could test their systems and compare results.  
            </paragraph>
            <paragraph>
                <context>
                    According to Arstein and <cite id="2" function="ack" polarity="neu">Poesio (2008)</cite>, a corpus is reliable if annotators agree in the assigned categories because it displays a similar understanding of the classification scheme.
                </context>
                This criterion is a prerequisite to demonstrate validity of a scheme. If there is no consistency among the obtained results, the representation may be inappropriate for the data.  
            </paragraph>
            <paragraph>
                In our experiment, we pose a scheme to classify citation functions and we defined an annotation methodology to allow a greater accuracy in the process to facilitate decision-making and generate a greater inter-coder agreement. The subject of this article focus in the proposed annotation methodology which could be applied to any scheme with the only condition that the scheme is not ambiguous i.e. its categories are clearly differentiated. 
            </paragraph>
        </section>
        <section imrad="m">
            <title>
                2	 Method 
            </title>
            <paragraph>
                We applied different citation classification schemes according to citation function. The condition for these schemes were that categories were well distinguished.
            </paragraph>  
            <paragraph>
                In this process, we detected two sources or error that affected results and did not allow good agreement among coders. One had to do with usage by the annotators of context of different length, which lend them to obtain discrepant results; the other was lack of clarity in the analyzed articles that made difficult to find enough sense in text to reach a unique citation classification.  
            </paragraph>
            <paragraph>
                <context>
                    <author>We</author> 
                    <kw>corrected the first factor setting</kw> fixed criteria for determining context length.
                
                    <cite id="3" function="bas" polarity="pos">(Hernandez and Gomez, (2014))</cite> highlighted the need for defining context in view of argument detection, so the context include all sentences around citation that are talking about it.
                </context>

                However, due to the complexity of this task, we decided to replace argument detection by fixing a context delimited by a complete paragraph. The rationale for this decision was that, by definition, a paragraph is a group of related sentences about the same idea.  We assumed that authorï¿½s purpose when making a citation could be found using cue words and ontological concepts that are within the same paragraph.  
            </paragraph>
            <paragraph>
                To avoid the second error source, we proposed a new annotation methodology to help coders to organize the ideas expressed in the text, to take them to decide citation function classification in an orderly way. With the proposed annotation methodology, we could achieve a minimized human effort with a clear understanding of the structure of the presented ideas, so that we could generate a natural association of functions and their classes. As an additional advantage, the methodology includes detection of patterns formed by lexical values (cue words) and ontological classes related to a function. We could convert this information to regular expressions to be the foundation for automatic citation classification.  Without regular expressions, to annotate a corpus of sufficient size we would require too much effort to detect patterns statistically. In fact, in the initial experiments, the original intention for pattern use was to annotate them in conjunction with function definition, so that, these patterns included in the corpus, would facilitate model detection in an automatic tagging process. We changed this approach and decided to pre-annotate first in an attempt to improve a very low annotation agreement, and with this change, we obtained a new and more effective way to data-set annotation.   
            </paragraph>
            <paragraph>
                The proposed annotation methodology consists of two phases. In the first, we perform a pre-annotation process in which we define patterns. These patterns help coders to understand structure of sentences within context and help them to define citation function. In this step, the annotator detects a sentence type, saving the original sentence order to maintain relevant information related with citation purpose. 
                <context>
                    <cite id="4" function="use" polarity="neu">Zock, 2012</cite>, <kw>presented</kw> 
                    <data>patterns</data> 
                    <kw>that</kw> 
                    <task>link ontological and syntactic categories to generate sentences</task> maintaining the author's original intention.  
                </context>
                These techniques allow associating between purpose and an ontological pattern. We adapt this basic idea to the solution or our problem and develop concepts and notation to our method.  
            </paragraph>
            <paragraph>
                In the pre-annotation stage, coders identify manually ontological and lexical patterns that are near of citations within the content defined as a paragraph. A pattern consists of a fixed part and a variable part. The fixed part is underlined and corresponds to cue words related to a function. We label the variable part as XML, according to ontological concepts as cited work, author, theory, action, method, used material, concept, task, result, quoted text, assumption, person, experiment, positive feature, negative feature, etc.  We design the group of tags so that we cover without ambiguity the largest number of possibilities.   
            </paragraph>
            <paragraph>
                For instance, if we have the text: "This feature set is based on Dong and Schafer, 2011. Pre-annotation result will be: &lt;material&gt;this feature set &lt;/material&gt; is based on &lt;cited&gt; Dong and Schï¿½fer, 2011&lt;/cited&gt;. In addition, the pattern will be MATERIAL is based CITED, where MATERIAL and CITED are the variable part and is based is the fixed part that corresponds to cue words. In this case, it is clear that citation function has to do with the use other author's material as a base for own work.  
            </paragraph>
            <paragraph>
                <context>
                    Other sentences can be generated with this pattern, for instance,<method> The algorithm </method>
                    <kw>is based in</kw> 
                    <method>the Vector Space Model  VSM</method> (<cite id="5" function="bas" polarity="pos">Salton et al., 1975</cite>). 
                </context>
                <context>
                    <data>This sentence</data> pre-annotated is &lt;material&gt;The algorithm&lt;/material&gt; <kw>is based</kw> 
                    <kw>in the</kw>
                    <method>Vector Space Model</method>  VSM &lt;cited&gt; (<cite id="6" function="bas" polarity="pos">Salton et al., 1975</cite>)&lt;/cited&gt;. 
                </context>
                The pattern is the same that the one in the previous example ï¿½MATERIAL is based CITEDï¿½, with the equal function type than last example because pattern is identical. 
            </paragraph>
            <paragraph> 
                
                Fixed part is a skip-gram with 1 to 4 length. Each group of words is a sequence.
                <context>
                    A skip-gram, according to  <cite id="7" function="ack" polarity="neu">Guthrie, Allison, Liu, Guthrie, and Wilks (2006)</cite>, is a generalization of an n-gram, where text leave not considered spaces, while a skip-gram does consider spaces between word sequences.  
                </context>
            </paragraph>
            
                2.1	 Examples of pre-annotation process  
            
            <paragraph>
                To understand better the pre-annotation scheme, we show three examples of how to apply it to the context of scientific citations. First, consider the following sentence containing a citation: 
            </paragraph>
            <paragraph>
                We compare our zone classifier to a reimplementation of Teufel and Moens's NB classifier and features on their original Computational Linguistic corpus
            </paragraph>
            <paragraph>
                After applying the ontological pattern annotation scheme, we obtained the following result: 
                ï¿½&lt; author &gt; We &lt;\author&gt; compare our &lt;material&gt;zone classifier&lt;\material&gt; to a reimplementation of &lt;cited&gt;Teufel and Moens&lt;\cited&gt;'s &lt;material&gt;NB classifier&lt;\material&gt; and &lt;material&gt;features&lt;material&gt; on their original &lt;material&gt;Computational Linguistic corpus&lt;\material&gt;
            </paragraph>
            <paragraph>
                Its ontological pattern is: AUTHOR compare our MATERIAL to CITED MATERIA
            </paragraph>
            <paragraph>
                This pattern contains a skip-gram, which is formed by two word sequences: "compare our" and "to". The idea behind the whole sentence is that the authors compare their own material with a cited material. The classification of authorï¿½s sentiment is not part of this work, but the pattern clearly reveals a comparison between authors contribution with other researchers.
            </paragraph>
            <paragraph>
                Let us take a second example out of the literature to illustrate our method. Consider the following paragraph containing a citation: 
            </paragraph>
            <paragraph>
                <context>
                    <method>Comprehension-based summarization</method>, e.g.  <cite id="7">Kintsch and Van Dijk (1978)</cite> and <cite id="8" function="hed" polarity="neg">Brown et al. (1983)</cite>,  <posfeature>is the most ambitious model</posfeature> 
                    <kw>of </kw> 
                    <task>automatic summarization</task>, requiring a complete understanding of the text. <kw>Due to</kw> 
                    <negfeature>the failure of rule-based NLP and knowledge representation</negfeature>, <kw>other less</kw> 
                    <method>knowledge-intensive methods</method> 
                    <kw>now dominate</kw>.
                </context>
                
            </paragraph>
            <paragraph>
                  
                Annotating this paragraph, we have: Comprehension-based summarization, e.g. &lt;cited1&gt;Kintsch and Van <cite id="9">Dijk (1978)</cite>&lt;/cited1&gt; and &lt;cited2&gt;<cite id="10">Brown et al. (1983)</cite> &lt;\cited2&gt;, is the most ambitious model of automatic summarization, requiring a complete understanding of the text. Due to the failure of &lt;method&gt;rule-based NLP&lt;\method&gt; and &lt;method&gt; knowledge representation &lt;\method&gt;, other less knowledge-intensive methods now dominate
            </paragraph>
            <paragraph>             
                Its ontological pattern is: 
                CITED ambitious * .Due to * failure of METHOD 
                CITED ambitious * .Due to * failure of METHOD   
            </paragraph>
            <paragraph>                  
                The pattern contains a skip-gram having three word sequences: ambitious Due to and failure of. The skip-grams are indicated by a star symbol * in between the sequences. The variable parts are two: &lt;cited&gt; and &lt; method &gt;. The idea behind this pattern is that the cited researchers were ambitious, but they failed on the authorsï¿½ point of view. This pattern clearly reveals authors negative impression or a weakness regarding the cited work. 
            </paragraph>  
            <paragraph>                            
                Finally, we present a third example by taking the following sentence containing a citation: 
            </paragraph>
            <paragraph>
                The baseline score shown in bold, is obtained with no context window and is comparable to the results reported by <cite id="11">Athar (2011)</cite>
            </paragraph>       
            <paragraph>
                Applying our annotation scheme, we produce: 
            </paragraph>
            <paragraph>
                The <result>&lt;result&gt;baseline score &lt;\result&gt;</result>, shown in bold, is obtained with no context window and<kw> is comparable to</kw> the<result>&lt;result&gt;results&lt;\result&gt;</result> 
                <kw>reported by</kw> &lt;cited&gt; <cite id="12" function="con" polarity="neu">Athar (2011)</cite>&lt;\cited&gt;
            </paragraph>        
            <paragraph>                                          
                Its ontological pattern is: 
                RESULT is * comparable to RESULT CITED
                                                             
            </paragraph>  
            <paragraph>            
                Again, the pattern contains a skip-gram having two word sequences:  and i"s "and comparable to. Additionally, it contains three variable parts: &lt;result&gt; &lt;result&gt;  &lt;cited&gt;. From this pattern, we clear see that authors are comparing their results with other researchersï¿½. Independently of the function classification, the ontological patterns are supposed to reveal the authorsï¿½ intention concerning the cited work. 
            </paragraph>
            <paragraph>
                The application of this strategy allows identifying punctual lexical entries and their relation to semantic features. An ontological pattern here is a structure that conveys authorsï¿½ purpose to cite. By using that, we expect not only to obtain a good level of agreement among the annotators, but also to minimize the human effort needed for annotating papers and populate a big corpus by converting the patterns into regular expressions. 
            </paragraph>
        </section>
        <section>
                
            <title>
         
                3	 Experiment setup and results 
            </title>
            <paragraph>
                <context>
                    Three annotators collaborated. The annotation process comply three requirements in order to achieve reliability and reproducibility (<cite id="13" function="ack" polarity="neu">Krippendorff, 2004</cite>).

                </context>
                The annotators had a profile that allow them a good understanding of the scientific texts in computational linguistics; they worked in an independent way and they had a clear function classification scheme with detail instructions. 
            </paragraph>
            <paragraph> 
                To test annotation reliability, we measured inter-annotator agreement in a small section of the corpus; the same people must review this sample.
                <context>
                    It is necessary to achieve a good rate in this agreement because it certifies that the process is reliable and reproducible and that results may be generalized to the complete process in which probably are going to work new annotators and not only the ones that coded the trial  <cite id="14" function="ack" polarity="neu">(Artstein y Poesio, 2008</cite>).  
                </context>
            </paragraph>
            <paragraph>
                We analyzed 101 citations to classify them according to their function without pre-annotation and 101 different citations with pre-annotation. We measured inter-annotator agreement in each case.  
            </paragraph>
        </section>
        <section>
                
            <title>
                4	Results and discussion 
            </title>
            <paragraph>
                We computed Fleiss, Krippendorff indexes and Pairwise average using Geertzen, J. (2012) software. Calculations were made for processes without and with pre-annotation. Pre-annotation applies the explained methodology. We present results in Table 1 and 2.  
            </paragraph>
            
                4.1	Results without applying pre-annotation  
            
            <paragraph>
                The experiment had 3 annotators, 101 cases, and 1 variable with 303 decisions.  
 
            </paragraph>
            <paragraph>
                Fleiss	Krippendorff	Pairwise avg. 
                A_obs = 0.554 
                A_exp= 0.274 
                Kappa = 0.386	D_obs = 0.446 
                D_exp = 0.728 
                Alpha = 0.388	% agr = 55.4 
                Kappa=0.405 
                
                Table 1: Results for inter-annotator agreement without pre-annotation 
            </paragraph>
            
                4.2	Results applying pre-annotation  
            
            <paragraph>
                The experiment had 3 annotators, 101 cases, and 1 variable with 303 decisions.  
 
 
                Fleiss 	Krippendorff	Pairwise avg. 
                A_obs=0.845 
                A_esp=0.365 
                Kappa=0.756	D_obs = 0.155 
                D_esp = 0.637 
                Alpha = 0.756	% agr= 84.5 
                Kappa=0.756 
 
 
 
  
                Table 2: Results for inter-annotator agreement with pre-annotation 
            </paragraph>
        </section>
        <section>
            <title>
                5	 Conclusions and future work 
            </title>
            <paragraph>
                Results without pre-annotation presented low inter-annotator agreement values. We could explicate this, due to the complexity that have the process for defining functions in a medium granularity scheme with at least five functions. We consider that a five-function scheme allows differentiating citation functions. We tested the methodology with a scheme with this number of classes. Annotators read carefully the articles but, without a pre-annotation process, results were poor because annotators had to take into account too many details and even with a through reading, text structure is difficult to appreciate.  
            </paragraph>
            <paragraph>
                There is a big improvement in inter-annotator agreement using the proposed methodology that includes a pre-annotation process of a citation context with a fixed one-paragraph length. The previous process of extracting ontological concepts and cue words allowed that annotator could see more clearly sentence structure and facilitate decision making about the citation function classification.  The result is a very significant enhancement of inter-annotator agreement that validates the use of the proposed methodology.  
            </paragraph>
            <paragraph>
                With the proposed annotation methodology the agreement percentage, without a random correction is 84.5% and Kappa index is 0,756. 
                <context>
                    According  <cite id="15" function="ack" polarity="neu">Landis  and Koch (1977)</cite>, a K = 0,756 corresponds to a substantial annotator agreement, while the initial results, without pre-annotation corresponded to a minimum value which was not enough to keep on working in the topic. 
                </context>
            </paragraph>
            <paragraph>
                We plan to annotate a sufficient number of articles using this methodology together with a non-ambiguous and complete scheme of annotation. The annotations generated, ontological patterns and cue words will serve to mine in an automatic way in a non-annotated corpus. Thus, we will continue to expand a basic corpus for the development of research in citation function analysis.
            </paragraph>
            <paragraph> 
                Our intention is to make available to the scientific community this dataset to facilitate research in order to develop better systems to evaluate the citation impact in scientific literature. The purpose of these systems will be to take into account new factors that can be incorporated in the calculation of indexes to better assess function, significance and disposition of an author towards the scientific work of another that was referenced.    
            </paragraph>
            <paragraph>
                Acknowledgement 
 
                This research work has been partially funded by the Spanish Government and the European Commission through the project, ATTOS (TIN2012-38536-C03- 03), LEGOLANG (TIN2012-31224), SAM (FP7- 611312) and FIRST (FP7-287607). 
            </paragraph>
        </section>
    </paper>
</annotatedpaper>