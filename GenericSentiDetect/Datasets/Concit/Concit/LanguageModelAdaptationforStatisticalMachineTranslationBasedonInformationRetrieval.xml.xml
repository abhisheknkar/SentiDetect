<annotatedpaper>
    <paper title= "Language Model Adaptation for Statistical Machine Translation Based on Information Retrieval" authors= "Matthias Eck, Stephan Vogel, Alex Waibel" year="2004">
        <section>
            Matthias Eck, Stephan Vogel, Alex Waibel
            Carnegie Mellon University
            5000 Forbes Avenue, Pittsburgh, PA, 15213, USA
            matteck@cs.cmu.edu, vogel+@cs.cmu.edu, ahw@cs.cmu.edu 
        </section>
        <section>
            <title>
                Abstract
            </title>
            Language modeling is an important part for both speech recognition and machine translation systems. Adaptation has been successfully applied to language models for speech recognition. In this paper we present experiments concerning language model adaptation for statistical machine translation. We develop a method to adapt language models using information retrieval methods. The adapted language models drastically reduce perplexity over a general language model and we can show that it is possible to improve the translation quality of a statistical machine translation using those adapted language models instead of a general language model.
        </section>
        <section imrad="i">
            <title>
                1.Introduction
            </title>
            <paragraph>
                Statistical translation systems use the well-known n-gram language models to predict words. Typically, the more data used to estimate the parameters of the language model, the better the translation results. The main problem is that a general language model does not adapt to the topic or the style of individual texts. It is also obvious that during a longer text the topic of discussion will change. Experience for speech recognition indicates that even better results might be possible with adapted language models.
            </paragraph>
            <paragraph>
                In the experiments reported in this paper, the test set consisted of texts from Chinese and Arabic news wires, where different news stories cover different topics. The adaptation of the language models is done by selecting for each news story, or even each sentence, similar stories or sentences from a large English news corpus, using methods of information retrieval, and building smaller, but more specific, language models.
            </paragraph>
            <paragraph>
                The questions we tried to answer were which kind of adaptation unit (news story or sentence) would be best suited for this idea and how much data should be used for the adapted language models. Other points to examine are the effect of the amount of data that is used to select the language models from and the effect of the quality of the translations used as query.
            </paragraph>
            <paragraph>
                It is also interesting to see if some special approaches, that showed good improvements in information retrieval like the usage of stemmers and stopword-lists can have a positive effect in this adaptation task.
            </paragraph>
            
            <subtitle>
                1.1.Basic Idea
            </subtitle>
            <paragraph>
                for each test document • translate with general language model • use this translation to select most similar documents • build adapted language model using these similar documents • re-translate with adapted language model (documents can be stories or individual sentences)
            </paragraph>
            
            
            <subtitle>
                1.2. Previous Work
            </subtitle>
            <paragraph>
                <context>
                    <concept>The main idea</concept> 
                    <kw>is based on</kw>
                    <paper>the paper</paper> 
                    <kw>by</kw>  
                    <cite id="1" function="bas" polarity="pos">Mahajan, Beeferman and Huang (1999)</cite> in which they used similar techniques for language model adaptation.
                </context>
                Mahajan, Beeferman and Huang applied the adapted language models on speech recognition and they could significantly reduce perplexity in this task.
            </paragraph>
            <paragraph>
                <context>
                    Other methods for language model adaptation are presented and reviewed in the paper by DeMori and <cite id="2" function="ack" polarity="neu">Federico (1999)</cite> and in the paper by Janiszek, DeMori and <cite id="3" function="ack" polarity="neu">Bechet (2001)</cite>. 
                </context>
                
            </paragraph>
            <paragraph>
                <context>
                    According to Janiszek, DeMori and Bechet <kw>the following</kw> 
                    <method>basic approaches</method> 
                    <kw>to</kw> 
                    <task>language model adaptation</task> 
                    <kw>exist</kw>.
                    •
               
                    <task>Training a language model in a new domain</task> if sufficient data is available. • Pooling data of many domains with the data of the new domain. • <method>Linear interpolation</method> of a general and a domain specific model (<cite id="4" function="use" polarity="neu">Seymore, Rosenfeld, 1997</cite>)
               
                    . • <method>Back-off of domain specific probabilities</method> with those of a specific model (<cite id="5" function="use" polarity="neu">Besling, Meier, 1995</cite>).
               
                    • <task>Retrieval of documents pertinent to the new domain</task> 
                    <kw>and</kw> 
                    <task>training an language model on-line with those data</task> (<cite id="6" function="use" polarity="neu">Iyer, Ostendorf, 1999</cite>). 
                    • <method>Maximum entropy, minimum discrimination adaptation</method> , <cite id="7" function="use" polarity="neu">(Chen Seymore, Rosenfeld, 1998</cite>). • <method>Adaptation by linear transformation of vectors</method> of bigram counts in a reduced space (<cite id="8" function="use" polarity="neu">DeMori, Federico, 1999</cite>).
                </context>
                
                
            </paragraph>
            <paragraph>
                Here we try to apply the approach "Retrieval of documents pertinent to the new domain and training a language model on-line with those data" on a machine translation task. We also use a local index unlike the approach presented by Zhu and Rosenfeld in (2001), who query web search engines to improve their language models.
            </paragraph>
            
        </section>
        <section imrad="m">
            <title>
                2. Language Model Adaptation based on Information Retrieval
            </title>
            
            <subtitle>
                2.1. Selecting similar documents with TF-IDF
            </subtitle>
            <paragraph>
                For the first experiments we used the TF-IDF similarity measure.
            </paragraph>
            <paragraph>
                TF-IDF is a way of weighting the relevance of a query to a document. TF-IDF is widely used in information retrieval. The main idea of TF-IDF is to represent each document by a vector in the size of the overall vocabulary. Each document Di is then represented as a vector (wi1, wi2,...,win ) if n is the size of the vocabulary. The entry Wij is calculated as: tf y is the term frequency of the j-th word in the vocabulary in the document Di i.e. the number of occurrences. /fy is the inverse document frequency of the j-th term, given as # documents # documents containing j-th term
            </paragraph>
            <paragraph>
                The similarity between two documents is then defined as the cosine of the angle between the two vectors.
            </paragraph>
            
            
            <subtitle>
                2.2. Test and training data
            </subtitle>
            <paragraph>
                The test data for all experiments translating from Chinese to English consisted of 993 sentences in 105 news stories. (TIDES test data December 2001)
            </paragraph>
            <paragraph>
                The data for the information retrieval index is data from Xinhua news service from the years 1991-2001. For the 200 million word index we used all available data and only a part of this data for the 40 million word index. For all information retrieval applications the Lemur-Toolkit (Lemur Toolkit) was used.
            </paragraph>
            
            
            <subtitle>
                2.3.Language Model Adaptation using story-level retrieval
            </subtitle>
            <paragraph>
                In the first experiment the index contained approximately 180 000 stories with 40 million words. We calculated the perplexity of the adapted language models with the top 10, top 100 and top 1000 stories for each 105 stories and for a general language model using all 40 million words. The story selection for the adapted language model was done based on the reference translation. This showed a perplexity reduction of up to 39%. Table 1 : Perplexity results for 40 million word index and reference translation (LM: language model)
            </paragraph>
            <paragraph>
                Using a larger index of 200 million words (1 million documents) we could reduce the perplexity by 32%.
            </paragraph>
            <paragraph>
                It is not realistic to use the reference translation as a query so besides using the reference translation we also used automatic translations with NIST scores (mteval metric) of 7.18 and 7.90 respectively.
            </paragraph>
            <paragraph>
                As expected the perplexity reduction was lower than in the preceding experiments using the reference translation as queries. But 28% perplexity reduction was still possible when using the translation with a NIST score of 7.90. The translation with a score of 7.18 gave a perplexity reduction of up to 23%. Table 2: Perplexity results for 200 million word index and reference translation Table 3: Perplexity results for 200 million word index and 7.18 translation Table 4: Perplexity results for 200 million word index and 7.90 translation But even when using those language models with minimal perplexity the translation quality did not improve.
            </paragraph>
            <paragraph>
                Figure 1 : Overview of perplexity reduction using 200 million word index the sum of the perplexity percentages (compared to the general language model with 1 000 000 documents). The perplexity graphs for each story do not show a clear picture (Figure 2 shows graphs for 4 different stories) Figure 2: Perplexity percentage for 4 news stories
            </paragraph>
            <paragraph>
                Figure 3 shows the sum of the perplexity percentages (compared to the general language model with 1 million documents). Here we get a nice graph and clear minimum at around 10 000 (exact number: 9 700) news stories with an average reduction in perplexity of 39%.
            </paragraph>
            
            
            <subtitle>
                2.4.Language Model Adaptation using sentence-level retrieval
            </subtitle>
            <paragraph>
                Using the same index of 200 million words (and in this case 7 million sentences) of Xinhua news data we did similar experiments using sentences as documents, i.e. building an individual language model for each sentence to be translated.
            </paragraph>
            <paragraph>
                We used two different translation systems for these experiments, an Chinese—English system, that had a baseline score of 7.12 (NIST) and an Arabic- English system with a baseline score of 7.32 (NIST).
            </paragraph>
            <paragraph>
                In this case perplexity reduction of up to 81% was observed at 1000 sentence size of the language model.
            </paragraph>
            <paragraph>
                Table 5 shows the actual numbers for different language model sizes and the graph in Figure 4 illustrates the relationships. Figure 3: Perplexity percentage sum for first 20 news stories Figure 4: Perplexity results for 7.12 translation and 7.32 translation
            </paragraph>
            <paragraph>
                As the perplexity results in the document-level experiments did not show any significant correlation to the later translation performance we did not calculate further perplexity values but relied more on actual translation experiments.
            </paragraph>
            <paragraph>
                In these translation experiments for Chinese to English translation we compared the translation result using a 20 million word general language model with the translations when using specific language models. Starting from an initial translation scoring 7.12 (NIST) the best improvement could be observed when using 15 000 sentences to build the adapted language models. However, the improvement of 0.06 in NIST score is not statistically significant. Table 5: Perplexity results for 200 million word index and 7.12 translation/7.32 translation Using the same index data but in this case translating from Arabic to English we could improve the translation score from 7.32 to 7.61 (NIST). In this case, the improvement is highly significant.
            </paragraph>
        </section>
        <section imrad="r">
            
            <subtitle>
                2.5. Further experiments and observations
            </subtitle>
            <paragraph>
                We did further experiments to see if other information retrieval techniques could be used in this task. An often used method in information retrieval is stemming. Stemming reduces derivative word forms to one root form of a word (stem). Stemming takes the assumption that different derivative forms of words do not have different meanings so concerning topic they can be treated as the same word. By using these stems instead of the actual words it should be possible to improve the calculation of the topic similarity. 
                <context>
                    Stemming was reported to be quite useful in the paper by  <cite id="9" function="ack" polarity="neu">Mahajan, Beeferman and Huang (1999)</cite>. 
                </context>
                
                In this case the use of Porter's stemmer when building an index did not show any improvements in translation quality.
            </paragraph>
            <paragraph>
                A similar idea commonly used in information retrieval is the omission of stopwords. Stopwords are words of little intrinsic meaning that occur too frequently to be useful in searching text. Typical examples for stopwords are: "a", "by", "neither", "seem", "those", "how", "no" etc. By just leaving out these words the performance of information retrieval applications can usually be improved In this case the usage of a stopword-list did not have a positive effect on the translation quality of the resulting language models.
            </paragraph>
            <paragraph>
                We also tried two other retrieval methods that are offered by the Lemur-Toolkit in addition to TF-IDF. But both Okapi and SimpleKL gave no improvement compared to selecting the language models using TF-IDF.
            </paragraph>
            
        </section>
        <section imrad= "d">
            <title>
                3.Conclusion
            </title>
            <paragraph>
                In this paper we have shown that language model adaptation can be successfully applied to statistical machine translation. Not only did the adapted language models drastically reduce perplexity but also a NIST score improvement of up to 0.29 was possible.
            </paragraph>
            <paragraph>
                The results show that sentence level adaptation outperforms document level adaptation.
            </paragraph>
            <paragraph>
                The results also indicate that the correlation between perplexity of a language model and actual improvement is rather weak.
            </paragraph>
            <paragraph>
                When even better translations are possible to use as queries this should further improve. Table 5: Translation scores for 200 million word index and 7.12 translation/7.32 translation 
            </paragraph>
        </section>
    </paper>
</annotatedpaper>