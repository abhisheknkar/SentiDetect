<annotatedpaper> 
    <paper title="A Fully Coreference-annotated Corpus of Scholarly Papers from the ACL Anthology" authors="Ulrich Schäfer, Christian Spurk, Jörg Steffen " year="2012"> 
       <section>
            <title>A Fully Coreference-annotated Corpus of Scholarly Papers from the ACL Anthology</title> 
            Ulrich Schäfer Christian Spurk Jörg Steffen 
            German Research Center for Artificial Intelligence (DFKI), Language Technology Lab 
            Campus D 3 1, D-66123 Saarbrücken, Germany 
            {ulrich.schaefer,christian.spurk, joerg.steffen}@dfki.de 
       </section>
       
            <title>Abstract</title> 
            <paragraph> 
                We describe a large coreference annotation task performed on a corpus of 266 papers from the ACL Anthology, a publicly, electronically available collection of scientific papers in the domain of computational linguistics and language technology. The annotation comprises mainly noun phrase coreference of the full textual content of each paper in the Anthology subset. It has been performed carefully and at least twice for each paper (initial annotation and secondary correction phase). The purpose of this paper is to summarize the comprehensive annotation schema and release the corpus publicly, along with this paper. The corpus is by far larger than the ACE coreference corpora. It can be used to train coreference resolution systems in the Computational Linguistics and Language Technology domain for semantic search, taxonomy extraction, question answering, citation analysis, scientific discourse analysis, etc. 
            </paragraph> 
         
        <section imrad="i"> 
            <title>1 Introduction and Motivation</title> 
            <paragraph> 
                Coreference resolution (CR), mostly on newspaper text, has been studied in extenso during the last decades. The task consists in
                finding all mentions of real-world entities such as persons or organizations in a text, regardless of their textual representation. 
                It could be a proper name, a role (e.g. 'the director'), a pronoun, or a similar circumscribing expression (mention). Related (with
                an overlap) to this task is anaphora resolution. Here, the interpretation of a mention, called anaphor (e.g. a pronoun), depends on 
                a previous mention, called antecendent, or on context. Different relations may hold between an anaphor and its antecedent, e.g. part-of, 
                subset, etc. If both refer to the same entity, the anaphoric relation is also coreferential. In contrast to coreference, the order matters,
                but in general, not every anaphoric relation is coreferential and vice versa.<contexr> A<kw> more detailed discussion</kw> of the coreference vs. anaphora
                distinction <kw>can be found in </kw>  
                <cite id="1" function="ack" polarity="neu" > van Deemter and Kibble (2000)</cite>
                </contexr>. 
            </paragraph> 
            <paragraph> 
                For 15 years, significant progress in terms of robustness and coverage has been made by applying machine learning and including semantic
                information. Instead of enumerating the history of CR literature, <context><kw>we refer</kw> the interested reader <kw>to </kw> 
                <cite id="2" function="ack" polarity= "neu">Ng (2010)</cite> and <cite id="3" function="ack" polarity="neu">Mitkov (1999)</cite>
                </context>. They present comprehensive, though compact, surveys of the CR 
                research for this period. <context>However, most of the research so far <kw>was done in</kw> the news text domain only, as the largest available annotated 
                corpora such as MUC (<cite id="4" function="use" polarity="neu">Grishman and Sundheim, 1996</cite>) and ACE (<cite id="5" function="use" polarity="neu">Doddington et al., 2004</cite>)
                mostly were of this origin.</context> 
            </paragraph> 
            <paragraph> 
                The purpose of our endeavor is to move to a different domain, namely scientific texts, extracted from proceedings papers and journal articles. 
                We do this because our evaluations have shown that systems trained on news text, where mainly persons and organizations are the subject of coreference
                phenomena, perform worse on scientific text. 
            </paragraph> 
            <paragraph> 
                <context><kw>We observe</kw> a general rising research interest in <kw>applying</kw>  
                <method>CR </method> to real-world texts other than newswire, e.g. the CoNLL Shared Task on 
                Modeling Unrestricted Coreference in <data>OntoNotes </data> (<cite id="6" function="use" polarity="neu">Pradhan et al., 2011</cite>) and the BioNLP Shared Task supporting task: 
                Protein/Gene Coreference Task (<cite id="7" function="use" polarity="neu" >Nguyen et al., 2011</cite>)</context>. 
            </paragraph> 
            <paragraph> 
                <context><cite id="8" function="ack" polarity="neu">Watson et al. (2003)</cite> 
                <kw>argue</kw> that <method>CR</method> in scientific text may be harder than in newspaper text, as scientific text 
                tends to be more complex and contains relatively high proportions of definite descriptions, which are the most challenging to resolve. </context>
                Conversely, the proportion of easier-to-determine entities such as person names, is inferior in papers. 
            </paragraph> 
            <paragraph> 
                In scientific texts, anaphoric expression referring to named entities are less frequent. Instead, references to domain terminology
                and abstract entities such as results, variables and values are more important. 
            </paragraph> 
            <paragraph> 
                Motivation for improving CR in scientific text is manifold. <context><kw>Applications 
                    such as </kw> Question Answering (<cite id="9" function="use"
                                                            polarity="neu">Watson et al., 2003</cite>)
                , Information Extraction (<cite id="10" function="use" polarity="neu">Gaizauskas and Humphreys, 2000</cite>), ontology extraction, or
                accurate semantic search in digital
                libraries <cite id="11" function="use" polarity="neu">(Schäfer et al., 2011)</cite> 
                <kw>can benefit</kw> from identified coreferences in running text</context>. 
            </paragraph> 
            <paragraph> 
                On the one hand, making implicit or hidden relations explicit and complete by resolving e.g. anaphora may increase redundancy and 
                hence the chance for answer candidates to be found. On the other hand, coreferences, if resolved correctly, may help to increase
                precision. Without anaphora replaced by their antecedents, certain propositions simply cannot be found. The same holds for variants
                of coreferring expressions. <context>A recent work also <kw>shows the benefits</kw> of CR for biomedical event extraction from scientific literature
                (<cite id="12" function="use" polarity="pos">Miwa et al., 2012</cite>)</context>. 
            </paragraph> 
            <paragraph> 
                The paper is structured as follows. In Section 2, we discuss recent related work. We present details of the coreference annotation task
                and parts of the annotation guidelines in Section 3. Section 4 discusses error analysis, inter-annotator agreement and correction. Finally,
                we give a conclusion. 
            </paragraph> 
        </section>       
        <section imrad="m">            
            
                <title>2 Related Work</title> 
                <paragraph> 
                    While a considerable part of previous and current research concentrates on news texts as provided by the MUC and ACE corpora, coreference
                    annotation of scientific papers is a relatively new area. In particular, work on coreference phenomena in scientific text mostly seems to 
                    focus on the biomedical domain. There is only a small number of publications dealing with other science domains. 
                </paragraph> 
                <paragraph> 
                    One of the earliest approaches is performed in the context of the Genia project and corpus of medical texts from MEDLINE. <context>In a first stage
                    , only MEDLINE <data>abstracts</data> 
                    <kw>are used</kw> (<cite id="13" function="con" polarity="neg">Yang et al., 2004</cite>), later
                    <method>other-anaphora</method>, a very specific sub-task, are using full paper content (<cite id="14" function="con" polarity="pos">Chen et al., 2008</cite>). 
                
                    <cite id="15" function="wea" polarity="neg">Gasperin (2009)</cite> 
                    presents a <kw>full</kw> annotation of anaphora and coreference in <data>biomedical 
                        text</data>, <kw>but only</kw> noun phrases 
                        referring to biomedical entities are considered</context>. On the basis of this annotation, she implements a probabilistic anaphora resolution system. 
                </paragraph> 
                <paragraph> 
                   <context> <kw>In contrast</kw>, <cite id="16" function="con" polarity="pos">Cohen et al. (2010)</cite> 
                    <action>build</action> a <data>corpus</data> of 97 full-text journal article  in the biomedical domain where every
                    co-referring noun phrase is annotated (CRAFT - Colorado Richly Annotated Full Text)</context>. <context>Their annotation 
                    <method>guidelines</method> 
                    <kw>follow</kw> those of the  <data>OntoNotes</data> project (<cite id="17" function="use" polarity="neu">
                        Hovy et al., 2006</cite>), adapted to the biomedical domain. OntoNotes itself is a text <data>corpus</data> of approx.
                        one million words from mainly news texts (newswire, magazines, broadcast conversations, web pages)</context>. <context>It also <action>contains</action> general 
                    anaphoric coreference <data>annotations </data> (<cite id="18" function="ack" polarity="pos">Pradhan et al., 2007</cite>): events and 
                    (like in our annotation) unlimited noun
                    phrase entity types</context>. 
                </paragraph> 
                <paragraph> 
                   <context> <cite id="19" function="use" polarity="neu"> Kim and Webber (2006)</cite> 
                    investigate a special aspect, citation sentences where a pronoun such as "they" refers to a 
                    previous citation. The study <action>is performed</action> on astronomy journal <data>articles</data> and a maximum-entropy 
                    <tool>classifier</tool> is trained. </context>
                </paragraph> 
                <paragraph> 
                   <context> <cite id="20" function="wea" polarity="neg">Kaplan et al. (2009)</cite> investigate coreferences and citations as well, 
                    <kw>but only</kw> at a <kw>very small</kw> scale (4 articles from 
                    the Computational Linguistics journal)</context>. They focus on so-called c-sites which are the sentences following a citation that also refer to the
                    same paper (typically by anaphora). The authors train a specific coreference model for this phenomenon. They show that exploitation of 
                    coreference chains improves the extraction of citation contexts which they then use for research paper summarization.  
                </paragraph>
             
            
                <title>3 Corpus Creation</title> 
                <paragraph> 
                   <context> <author>Our</author> corpus <action>comprises</action>  266 scientific <data>papers</data> 
                    <kw>from</kw> the ACL Anthology 
                    (<cite id="21" function="bas" polarity="pos">Bird et al., 2008</cite>)</context> sections P08 
                    (ACL-2008 long papers), D07 (EMNLP-CoNLL-2007) and C02 (COLING 2002). Texts were extracted from PDF using a commercial OCR program
                    which gurantees uniform, though not perfect, quality of the resulting text files. We did not rely on the original ACL-ARC text files 
                    converted with PDFBox because they contained considerable extraction errors depending on the (various) PDF tools that were used to
                    generate the PDFs. Hence quality of extraction would have dependent on the PDF generator, and OCR-based extraction is much more 
                    independent from the generation process. 
                </paragraph> 
                <paragraph> 
                    <context>Moreover, PDFBox cannot reliably recover reading order from text typeset in multiple columns (again, depending on the PDF generator used).
                    OCR introduces sporadic character and layout recognition errors,
                    but overall <action>works</action> 
                    <kw>robustly</kw>, cf. discussion and a
                    recent and even more accurate approach in <cite id="22" function="bas" polarity="pos">Schäfer et al. (2012)</cite>
                    </context>. <context>The main part of the 
                    corpus creation endeavor consisted in <action>manual annotation</action> 
                    <kw>assisted by</kw> a 
                    customized version of the MMAX2 annotation <tool>tool</tool> 
                    <cite id="23" function="bas" polarity="pos">(Müller and Strube, 2006)</cite>
                 , operating 
                    on the extracted raw <data>texts</data> 
                    </context>(the annotators had the possibility to open and view the original PDF files). In a second step, the corpus was then augmented with 
                    automatically created annotations. 
                </paragraph> 
               
                    <subtitle>3.1 Manual Annotations</subtitle> 
                    <paragraph> 
                        The annotators were not trained in the Computational Linguistics domain, but as advanced students of English language and literature
                        studies, they had some prior knowledge of general linguistics. 
                    </paragraph> 
                    <paragraph> 
                        <context><kw>We</kw> gave our human annotators the <kw>same</kw> 
                        <action>task</action> that a coreference resolution system shall solve. This task <kw>
                            is similar to</kw> the core annotation task of the ACE program: in the so-called "<method>entity detection and tracking (EDT)</method> task, all mentions of an entity, whether
                        a name, a description, or a pronoun, are to be found and collected into equivalence classes based on reference to the same entity" 
                            (<cite id="24" function="bas" polarity="neu">Doddington et al., 2004</cite>
                        </context>, p. 837). However, unlike in ACE, we did not
                        restrict the type of entities to be annotated. Because of this we do not even distinguish entity types in our annotation scheme.  
                    </paragraph> 
                    <paragraph> 
                        In terms of ACE entity classes, <context><kw>we</kw> only <kw>consider</kw> referential mentions for annotation, i.e. only "Specific Referential (SPC)", 
                        "Generic Referential (GEN)", and "Under-specified Referential (USP)" (<cite id="25" function="bas" 
                                                                                                    polarity="neu">LDC, 2004</cite>, p. 17f.) entities</context>, but we do 
                        not explicitly differentiate between these classes in our corpus. 
                    </paragraph> 
                    <paragraph> 
                        Only noun phrases (NPs, including coordinations of NPs), possessive determiners ("my", "your", ... ) and proper names (which may
                        be part of NPs as in "Sheffield's GATE system") were considered as possible entity mentions. As for mention types we asked our
                        annotators to classify each mention as one of the types listed in Table 1. It also contains the different kinds of pronouns and 
                        other noun phrases that can be mentions.  
                    </paragraph> 
                    <paragraph> 
                        Just like the annotators in ACE, our annotators were advised to identify the maximal extent of entity mentions. The mention extent 
                        thus includes all modifiers of the mention's head, i.e. any preceding adjectives and determiners, post-modifying phrases like 
                        prepositional phrases and relative clauses or parenthetical modifiers. 
                    </paragraph> 
                    <paragraph> 
                        Coreferences between mention pairs were marked, i.e. coreferential entity mentions were put into the same coreference set. Only 
                        those entity mentions were marked which are coreferential with any other mention in the text, i.e. a coreference set always contains 
                        at least two mentions. 
                    </paragraph> 
                    <paragraph> 
                        The annotators were asked to annotate maximal coreference sets, i.e., whenever they found two coreferential markables, they only 
                        created a new coreference set if there was not any other coreference set whose elements refer to the same referent as the two newly 
                        found markables. In other words, for every pair of document and real-world referent there should be only one coreference set with
                        markables of the document refering to the referent. In the extreme case, coreferential markables in the abstract at the beginning 
                        of a paper and in the conclusion at the end, and all markables refering to the same entity in between were to be put into the same set. 
                        This is useful because subsets for smaller ranges (e.g. for paragraphs or sections) can easily be derived from the complete annotation.  
                    </paragraph> 
                    <paragraph> 
                        Our manually annotated corpus contains 1,326,147 tokens (ACE 2004: 189,620) in 48,960 sentences (ACE: 5,654). The number of 
                        coreferring mentions in non-singleton coreference sets is 65,293 (ACE: 22,293). This number is plausible because scientific 
                        text typically contains less person and organization mentions than newspaper text.  
                    </paragraph> 
               
                    <subtitle>3.2 Annotation Guidelines and Corpus Data</subtitle> 
                    <paragraph> 
                        The full annotation guidelines (approx. 20 printed pages) with many examples and special hints for the corpus-specific phenomena 
                        such as citations, as well as user guides for the annotation tool MMAX2 are too comprehensive to be discussed here in detail. 
                        Therefore, they are part of the attached, compressed archive in file AnnotationGuidelines .html (along with A4 and letter paper 
                        versions in PDF format for printing). Independently of the conference proceedings, the data will be made available on
                        http : //take .dfki .de/#2Q12 and in the ACL Anthology as supplementary material to the electronic version of this publication. 
                    </paragraph> 
                    <paragraph> 
                        The archive also contains the complete annotated data in MMAX2 format for each of the 266 papers in the subdirectory annot at ion.
                        The file README.txt contains instructions on how to download, install and run MMAX2 and open the annoted corpus files for inspection.
                        MMAX2 needs to be downloaded separately, a screenshot is depicted in Figure 1. 
                    </paragraph> 
                   
                        <subtitle>3.2.1 Markables to Annotate</subtitle> 
                        <paragraph> 
                            Named Entities.Names and named entities (NEs) are usually (definite) NPs and as such can enter into coreference relations,
                            i.e., they are relevant markables. NEs may be, among others, names of companies, organizations, persons, locations, languages, 
                            currencies, programming languages, standards, scientific fields, systems, frameworks, etc. As a special case for our ACL
                            Anthology corpus, we consider citations in scientific papers as NEs, too. 
                        </paragraph> 
                        <paragraph> 
                            Definite Noun Phrases. Definite noun phrases are NPs which correspond to a specific and identifiable entity in a given
                            context. In many cases this definiteness is marked by the definite article "the" or a demonstrative determiner such as 
                            "these" or "that". 
                        </paragraph> 
                        <paragraph> 
                            Indefinite Noun Phrases. Indefinite noun phrases are NPs which do not correspond to a specific and identifiable 
                            entity in a given context. In many cases this indefiniteness is marked by the indefinite articles "a" and "an"
                        </paragraph>
                            <paragraph> 
                                Conjunctions. For our annotation task, we define a conjunction to be an NP which results by conjoining other NPs.
                                The most common junctor which is used for conjoining NPs is "and". Other junctors include, for example, "or", 
                                "as well as" or the discontinuous junctor "both ... and". 
                            </paragraph> 
                            <paragraph> 
                                Personal Pronouns. Personal pronouns are pronouns which stand for other NPs and which are even complete 
                                NPs themselves. The most common personal pronouns in English are "I", "you", "he", "she", "they", "it", "me", "him", "us",
                                "them", "her" and "we". 
                            </paragraph> 
                            <paragraph> 
                                Possessive Pronouns. Possessive pronouns in a strict sense are NPs which stand for another NP and which attribute 
                                ownership to the NP they substitute, e.g., "mine", "hers" or "ours". In our annotation task, we assume a broader sense 
                                in which possessive determiners are also considered to be possessive pronouns, e.g., "his", "her" or "my". 
                            </paragraph> 
                            <paragraph> 
                                Reflexive Pronouns. Reflexive pronouns are pronouns that substitute the NP to which they refer in the same clause as the NI? 
                                The most common reflexive pronouns in English are "myself", "yourself", "himself", "herself", "themself", "itself", "ourselves", 
                                "yourselves" and "themselves". 
                            </paragraph> 
                            <paragraph> 
                                Demonstrative Pronouns. In our annotation task, demonstrative pronouns are pronouns which are NPs that stand for some other NP of the discourse. As such they are very similar to personal pronouns. The most common demonstrative pronouns in English are "this", "that", "these" and "those" while for the annotations in our corpus, "these" will mostly be found as markable. 
                            </paragraph> 
                            <paragraph> 
                                Relative Pronouns. Relative pronouns are pronouns which introduce relative clauses. We are only interested in relative
                                pronouns that introduce non-restrictive relative clauses. In restrictive relative clauses, the relative pronoun does not refer
                                to any real-world entity and therefore it can't be a coreferential markable (The relative pronoun refers syntactically to the 
                                head noun of the noun phrase (NP) to which the relative clause belongs, however, this is no coreference; the NP is semantically 
                                incomplete without the relative clause). In non-restrictive clauses, the relative pronoun really corefers with the noun phrase
                                (NP) to which the relative clause belongs. The relative pronouns in English which are also relevant for annotation are "who",
                                "which", "whose", "where", "whom" and "when" as well as sometimes "that" and rarely "why".  
                            </paragraph> 
                 
                        <subtitle>3.2.2 Markables not to Annotate</subtitle> 
                        <paragraph> 
                            Our detailed annotation guidelines do not only give definitions, explanations and examples of markables and coreference 
                            phenomena to annotate, they also explain what shouldn't be annotated. Here is an excerpt. 
                        </paragraph> 
                        <paragraph> 
                            Relative Clauses. In general, relative clauses alone are not markables themselves, but only part of other markables. 
                        </paragraph> 
                        <paragraph> 
                            Restrictive relative pronouns and clauses should not be annotated, as the NP the relative pronoun refers to is
                            semantically incomplete without the relative clause. 
                        </paragraph> 
                        <paragraph> 
                            Only Definite Predicate Nominatives with Definite Subjects. A predicate nominative can only be a coreferential markable if 
                            it is definite and connected to a definite subject. 
                        </paragraph> 
                        <paragraph> 
                            Predicate nominatives are only to be annotated if they are definite and connected with a definite subject ("A mason is a 
                            workman" is indefinite and thus not coreferential). 
                        </paragraph> 
                        <paragraph> 
                            Bound anaphora (e.g. in "Every teacher likes his job.") should not be annotated because the referents do not necessarily refer
                            to the same. 
                        </paragraph> 
                        <paragraph> 
                            Indirect anaphora or bridging references (e.g. in "The bar is crowded. The waitress is stressed out.") should not be annotated
                            because the referents are not identical. 
                        </paragraph> 
                    
                    <subtitle>3.3 Automatic Annotations</subtitle> 
                    <paragraph> 
                        Because the main purpose of the corpus will be training machine learning systems, we also need examples of mentions that are
                        not coreferential with any other mention in the text - as kind of negative examples. These single mentions were automatically
                        annotated. To achieve this, we looked up all NPs (including coordinations functioning as NPs), possessive determiners and proper
                        names (including citations as a special case) in the corpus that were not part of any entity mention, yet. All these were then 
                        automatically classified into the above-mentioned mention types and stored with the manually annotated mentions. 
                    </paragraph> 
                    <paragraph> 
                       <context> The automatic annotations were 
                        <action>generated</action> 
                        <kw>using</kw> the Stanford <tool>Parser</tool> (<cite id="26" function="bas" polarity="neu">Klein and Manning, 2003</cite>)
                        in version  1.6.3 for NPs and possessive determiners</context>. <context>Proper nouns are <action>detected</action> 
                        <kw>using</kw> the <tool>SProUT system</tool> (<cite id="27" function="bas" polarity="neu">Drozdzynski et al., 2004
                        </cite>) with its generic named entity grammar for English</context>. SProUT robustly recognizes inter alia person names and locations in 
                        MUC style in running text, without any domain-specific adaptations or extensions except for citations. For the detection of 
                        citations, we have created an elaborate regular expression that reliably matches all kinds of citation patterns. 
                    </paragraph> 
               
                    <subtitle>3.4 Citations</subtitle> 
                    <paragraph> 
                        <context>Coreferences in <data>citation context</data> 
                        <kw>have</kw> special properties (<cite id="26" function="ack" polarity="neu">Kim and Webber, 2006</cite>)</context>. When they exist,
                        they are in most cases anaphoric pairs, typically the antecedent is a name. Roughly 10 % of the sentences in the corpus
                        contain citations. Therefore, special care has been taken to coreference phenomena in conjunction with citations. 
                    </paragraph> 
                    <paragraph> 
                        In the ACL Anthology, citations could be quite reliably identified automatically by regular expression patterns, as the citation 
                        styles are restricted. The annotators then only had to connect with e.g. pronouns in follow-up sentences. 
                    </paragraph> 
        </section>
                
        <section imrad="r"> 
          
                <title>4 Error Analysis, Inter-annotator Agreement and Correction</title> 
                <paragraph> 
                    In the initial annotation phase, 13 % of our corpus was annotated twice by different annotators in order to measure inter-annotator
                    agreement. <context><author>We</author> did this measurement <kw>as it </kw> was done for MUC (<cite id="27" function="bas" polarity="neu">
                        Hirschman et al., 1997</cite>
                    </context>  and in the same way as a
                    coreference resolution system is evaluated against some gold standard: one annotation was set to be the gold standard ("key") and
                    the second annotation was set to be the "response". Herewith <context><author>we</author> reached an inter-annotator agreement of 49.5 MUC points (for 
                    MUC score <method>calculation</method> 
                    <kw>see</kw> 
                    <cite id="28" function="bas" polarity="neu">Vilain et al. (1995)</cite>)</context>. <context><kw>Although</kw> the <method>MUC measure</method> 
                    is <kw>questionable</kw> 
                    (<cite id="29" function="wea" polarity="neg">Luo, 2005</cite>) and the task is difficult, this number is too low and asked for improvements</context>. 
                </paragraph> 
                <paragraph> 
                    Therefore in a second phase, the annotation guidelines have been improved in order to cover more corner cases and to resolve 
                    possible ambiguities. Additionally, all annotations were checked and corrected at least a second time in order to find accidental
                    annotation mistakes and to be consistent with the updated guidelines. <context>This <method>procedure</method> 
                    <kw>has also been suggested</kw> by 
                    <cite id="30" function="ack" polarity="neu">Hirschman et al. (1997)</cite> 
                    </context> for the MUC data, who - after optimizing their annotation guidelines and changing 
                    the annotation process to a two-step process - could improve their inter-annotator agreement by about 12 %. 
                </paragraph> 
                <paragraph> 
                    The second round has been performed by a single person over approx. 9 months part-time (8-10 hrs/week).Therefore we did not measure 
                    the inter-annotator agreement a second time, but on the other hand a single corrector ensures the annotation is of uniform quality 
                    throughout the whole corpus. 
                </paragraph> 
           
        </section>
        <section imrad="d">
                         
                <title>Conclusion</title> 
                <paragraph> 
                    We have developed a comprehensive annotation schema and annotation guidelines for corefer-ence in scientific text and 
                    fully annotated a 266 paper subset of the ACL Anthology. <context>The corpus is publicly available along with this paper. By a coreference 
                    resolution system built on top of it, e.g. training available <tool>tools</tool> such as LBJ (<cite id="31" function="use" polarity="neu">Bengtson and Roth, 2008</cite>;
                    <cite id="32" function="use" polarity="neu">Rizzolo and Roth, 2010</cite>), Reconcile 
                    (<cite id="33" function="use" polarity="neu">Stoyanov et al., 2010</cite>, 2011), Stanford's dcoref 
                    (<cite id="34" function="use" polarity="neu">Raghunathan et al., 2010</cite>; <cite id="35" function="use" polarity="neu">Lee et al., 2011</cite>), 
                    or (<cite id="36" function="use" polarity="neu">Haghighi and Klein, 2009</cite>, 2010), it could <kw>serve</kw> 
                    <kw>to improve</kw> other NLP <task>tasks</task> 
                    </context>
                    such as semantic search, 
                    taxonomy extraction, question answering, citation analysis, scientific discourse analysis, etc. 
                </paragraph> 
                <paragraph> 
                    The corpus in its current state is not perfect. It will probably be necessary to add a further round of annotation assessment and
                    correction. At this point, our project ends and we release the annotation data to the public along with the hope that the scientific
                    community finds it useful and further improves the corpus. 
                </paragraph> 
            
                <title>Acknowledgments</title> 
                <paragraph> 
                    We would like to thank the student annotators, most notably Leonie Grön and Philipp Schu, for their intelligent, careful and patient 
                    work. We also thank the three anonymous reviewers for helpful comments. The work described in this paper has been funded by the German
                    Federal Ministry of Education and Research, projects TAKE (FKZ 01IW08003) and Deependance (FKZ 01IW11003),
                    and under the Seventh Framework Programme of the European Commission through the T4ME contract (grant agreement no.: 249119). 
                </paragraph> 
            </section>
      
    </paper> 
</annotatedpaper>