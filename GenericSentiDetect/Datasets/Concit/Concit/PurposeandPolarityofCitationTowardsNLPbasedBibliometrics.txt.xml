<annotatedpaper>
    <paper title="Purpose and Polarity of Citation - Towards NLP-based Bibliometrics" authors="Amjad Abu-Jbara, Jefferson Ezra, Dragomir Radev" year="2004"> 
        <section> 
            <title>Purpose and Polarity of Citation - Towards NLP-based Bibliometrics</title> 
            Amjad Abu-Jbara 
            Department of EECS 
            University of Michigan 
            Ann Arbor, MI, USA 
            amjbara@umich.edu 
            Jefferson Ezra 
            Department of EECS 
            University of Michigan 
            Ann Arbor, MI, USA 
            jezra@umich.edu 
            Dragomir Radev 
            Department of EECS 
            and School of Information 
            University of Michigan 
            Ann Arbor, MI, USA 
            radev@umich.edu 
        </section> 
        <section> 
            <title>Abstract</title> 
            <paragraph> 
                Bibliometric measures are commonly used to estimate the popularity and the impact of published research. 
                Existing bibliometric measures provide "quantitative" indicators of how good a published paper is. 
                This does not necessarily reflect the "quality" of the work presented in the paper. 
                For example, when h-index is computed for a researcher, all incoming citations are treated equally, ignoring the fact that some of these citations might be negative. 
                In this paper, we propose using NLP to add a "qualitative" aspect to biblometrics. 
                We analyze the text that accompanies citations in scientific articles (which we term citation context). 
                We propose supervised methods for identifying citation text and analyzing it to determine the purpose (i.e. author intention) and the polarity (i.e. author sentiment) of citation. 
            </paragraph> 
        </section> 
        <section imrad="i"> 
            <title>1 Introduction</title> 
            <paragraph> 
                An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. 
                <context>Existing bibliometric measures such as <concept>H-Index</concept> (<cite id="1" function="wea" polarity="neg">Hirsch, 2005</cite>; <cite id="2" function="wea" polarity="neg">Hirsch, 2010</cite>), <concept>G-index</concept> (<cite id="3">Egghe, 2006</cite> function="wea" polarity="neg"), and <concept>Impact Factor</concept> (<cite id="4" function="wea" polarity="neg">Garfield, 1994</cite>) focus on the quantitative aspect of this evaluation which dose <kw>not always</kw>
                    <action>correlate</action> with the qualitative aspect. 
                </context>
            </paragraph> 
            <paragraph> 
                For example, the number of papers published by a researcher only tells how productive she or he is. 
                It does not say anything about the quality or the impact of the work. 
                <context>Similarly, the number of citations that a paper receives should not be used to gauge the quality of the work as it really only measures the popularity of the work and the interest of other researchers in it (<cite id="5" function="ack" polarity="neu">Garfield, 1979</cite>). 
                </context>
                Controversial papers or those based on fabricated data or experiments may receive a large number of citations. 
                A popular example of fraudulent research that deceived many researchers and caught media attention was the case of a South Korean research scientist, Hwang Woo-suk, who was found to have faked his research results in the area of human stem cell cloning. 
                His research was published in Science and received close to 200 citations after the fraud was discovered. 
                The vast majority of those citations were negative. 
            </paragraph> 
            <paragraph> 
                This suggests that the purpose of citation should be taken into consideration when biblometric measures are computed. 
                Negative citations should be weighted less than positive or neutral citations. 
                This motivates the need to automatically distinguish between positive, negative, and neutral citations and to identify the purpose of a citation; i.e. the author's intention behind choosing a published article and citing it. 
            </paragraph> 
            <paragraph> 
                This analysis of citation purpose and polarity can be useful for many applications. 
                For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers' work more accurately. 
                It can also be used as a preprocessing step in systems that process scholarly data. 
                <context>For example, <tool>citation-based summarization systems</tool> (<cite id="6" function="use" polarity="pos">Qazvinian and Radev, 2008</cite>; <cite id="7" function="use" polarity="pos">Qazvinian et al., 2010</cite>; <cite id="8" function="use" polarity="pos">Abu-Jbara and Radev, 2011</cite>) and <tool>survey generation systems</tool> (<cite id="9" function="use" polarity="pos">Mohammad et al., 2009</cite>; <cite id="10" function="use" polarity="pos">Qazvinian et al., 2013</cite>) <kw>can benefit from</kw> citation purpose and polarity analysis to improve paper and content selection.
                </context>
            </paragraph> 
            <paragraph> 
                In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation. 
                We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it. 
                We use the term citation context to refer to this text. 
                Next, we use supervised classification techniques to analyze this text and identify the purpose and polarity of citation. 
            </paragraph> 
            <paragraph> 
                The rest of this paper is organized as follows. 
                Section 2 reviews the related work. 
                We present our approach in Section 3. 
                We then describe the data and experiments in Section 4. 
                Finally, Section 5 concludes the paper and suggests directions for future work. 
            </paragraph> 
        </section> 
        <section imrad="i"> 
            <title>2 Related Work</title> 
            <paragraph> 
                Our work is related to a large body of research on citations. 
                <context>Studying citation patterns and referencing practices has interested researchers for many years (<cite id="11" function="ack" polarity="neu">Hodges, 1972</cite>; <cite id="12" function="ack" polarity="neu">Garfield et al., 1984</cite>). 
                </context>
                <context>
                    <cite id="13" function="use" polarity="pos">White (2004)</cite> 
                    <action>provides</action> a <kw>good</kw> 
                    <data>survey</data> of the different research directions that study or use citations.
                </context>
                In the following subsections, we review three lines of research that are closely related to our work. 
            </paragraph> 
            <subsection> 
                <title>2.1 Citation Context Identification</title> 
                <paragraph> 
                    The first line of related research addresses the problem of identifying citation context. 
                    The context of a citation that cites a given target paper can be a set of sentences, one sentence, or a fragment of a sentence. 
                </paragraph> 
                <paragraph> 
                    <context>
                        <cite id="14" function="use" polarity="pos">Nanba and Okumura (1999)</cite> 
                        <kw>use</kw> the term <concept>citing area</concept> to refer to the same concept.
                    </context> 
                    They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. 
                    Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. 
                    Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. 
                    <context>In (<cite id="15" function="use" polarity="pos">Nanba et al., 2000</cite>), they <kw>use</kw> their <tool>algorithm</tool> to improve citation type classification and automatic survey generation. </context>
                </paragraph> 
                <paragraph> 
                    <context>Qazvinian and <cite id="16" function="use" polarity="pos">Radev (2010)</cite> addressed a similar problem. 
                        <person>They</person> 
                        <kw>proposed</kw> a <method>method</method> based on probabilistic inference to extract non-explicit citing sentences; i.e., sentences that appear around the sentence that contains the target reference and are related to it.</context>
                    They showed experimentally that citation-based survey generation produces better results when using both explicit and non-explicit citing sentences rather than using the explicit ones alone. 
                </paragraph> 
                <paragraph> 
                    In previous work, we addressed the issue of identifying the scope of a given target reference in citing sentences that contain multiple references (2012). 
                    Our definition of reference scope was limited to fragments of the explicit citing sentence (i.e. the sentence in which actual citation appears). 
                    That method does not identify related text in surrounding sentences. 
                </paragraph> 
                <paragraph> 
                    In this work, we propose a supervised sequence labeling method for identifying the citation context of given reference which includes the explicit citing sentence and the related surrounding sentences. 
                </paragraph> 
                <title>2.2 Citation Purpose Classification</title> 
                <paragraph> 
                    <context>Several research efforts have focused on studying the different purposes for citing a paper (<cite id="17" function="ack" polarity="neu">Garfield, 1964</cite>; <cite id="18" function="ack" polarity="neu">Weinstock, 1971</cite>; Moravcsik and Muruge-san, 1975; and <cite id="19" function="ack" polarity="neu">Moitra, 1975</cite>; <cite id="20" function="ack" polarity="neu">Bonzi, 1982</cite>). </context>
                    <context>
                        <cite id="21" function="ack" polarity="neu">Bonzi (1982)</cite> studied the characteristics of citing and cited works that may aid in determining the re-latedness between them. </context>
                    <context> 
                        <cite id="22" function="ack" polarity="neu">Garfield (1964)</cite> enumerated several reasons why authors cite other publications, including "alerting researchers to forthcoming work", paying homage to the leading scholars in the area, and citations which provide pointers to background readings. </context>
                    <context> 
                        <cite id="23" function="use" polarity="pos">Weinstock (1971)</cite> 
                        <kw>adopted</kw> the same <concept>scheme</concept> that Garfield proposed in her study of citations. </context>
                </paragraph> 
                <paragraph> 
                    <context> 
                        <cite id="24" function="use" polarity="pos">Spiegel-Rosing (1977)</cite> 
                        <action>proposed</action> 13 <feature>categories</feature> for citation purpose <kw>based on her analysis</kw> 
                         of the first four volumes of Science Studies.</context>
                    Some of them are: Cited source is the specific point of departure for the research question investigated, Cited source contains the concepts, definitions, interpretations used, Cited source contains the data used by the citing paper. 
                    <context>
                        <cite id="25" function="use" polarity="pos">Nanba and Okumura (1999)</cite> 
                        <kw>came up with</kw> a simple <concept>schema</concept> composed of only three categories: Basis, Comparison, and other Other.</context> 
                    They proposed a rule-based method that uses a set of statistically selected cue words to determine the category of a citation. 
                    They used this classification as a first step for scientific paper summarization. 
                    <context>
                        <cite id="26" function="use" polarity="pos">Teufel et al. (2006)</cite>, in their work on citation function classification, <kw>adopted</kw> 12 <concept>categories</concept> 
                        <kw>from</kw> 
                        <paper>Spiegel-Rosing's taxonomy</paper>.</context>
                    They trained an SVM classifier and used it to label each citing sentence with exactly one category.
                    Further, they mapped the twelve categories to four top level categories namely: weakness, contrast (4 categories), positive (6 categories) and neutral. 
                </paragraph> 
                <paragraph> 
                    The taxonomy that we use in this work is based on previous work. 
                    We adopt a scheme that contains six categories. 
                    We selected the six categories after studying all the previously used citation taxonomies. 
                    We included the ones we believed are important for improving bibliometric measures and for the applications that we are planning to pursue in the future (Section 5). 
                </paragraph> 
                <title>2.3 Citation Polarity Classification</title> 
                <paragraph> 
                    The polarity (or sentiment) of a citation has also been studied previously. 
                    <context>
                        Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (<cite id="27" function="ack" polarity="neu">Ziman, 1968</cite>; <cite id="28" function="ack" polarity="neu">Mac-Roberts and MacRoberts, 1984</cite>; <cite id="29" function="ack" polarity="neu">THOMPSON and YIYUN, 1991</cite>).</context> 
                    <context>
                        <cite id="30" function="use" polarity="pos">Athar (2011)</cite> addressed the problem of identifying sentiment in citing sentences. 
                        <person>He</person> 
                        <kw>used</kw> a set of structure-based <feature>features</feature> to train a machine learning classifier using annotated data.</context> 
                    This work uses the citing sentence only to predict sentiment. 
                    Context sentences were ignored. 
                    <context>
                        <cite id="31" function="ack" polarity="neu">Athar and Teufel (2012a)</cite> observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3.  
                        <person>They</person> 
                        <kw>proposed</kw> two <method>methods</method> 
                        <kw>for utilizing</kw> the context.</context> 
                    In the first method, they treat the citing sentence and a fixed context (a window of four sentences around the citing sentence) as if they were a single sentence.
                    They extract features from the merged text and train a classifier similar to what they did in their 2011 paper. 
                    In the second method, they use a four-class annotation scheme. 
                    Each sentence in a window of four sentences around the citation is labeled as positive, negative, neutral, or excluded (unrelated to the cited work). 
                    There experiments surprisingly gave negative results and showed that classifying sentiment without considering the context achieves better results. 
                    They attributed this to the small size of their training data and to the noise that including the context text introduces to the data. 
                    <context>In (<cite id="32" function="use" polarity="neu">Athar and Teufel, 2012</cite>b), the <person>authors</person> 
                        <kw>present</kw> a <method>method for</method> automatically identifying all the mentions of the cited paper in the citing paper.</context> 
                    They show that considering all the mentions improves the performance of detecting sentiment in citations. 
                </paragraph> 
                <paragraph> 
                    In our work, we propose a sequence labeling method for identifying the citation context first, and then use a supervised approach to determine the polarity of a given citation. 
                </paragraph> 
            </subsection> 
        </section> 
        <section imrad="m"> 
            <title>3 Approach</title> 
            <paragraph> 
                In this section, we describe our approach to three tasks: citation context identification, citation purpose classification, and citation polarity identification. 
                We also describe a preprocessing stage that is applied to the citation text before performing any of the three tasks. 
            </paragraph> 
            <subsection> 
                <title>3.1 Preprocessing</title> 
                <paragraph> 
                    The goal of the preprocessing stage is to clean and prepare the citation text for part-of-speech tagging and parsing. 
                    The available POS taggers and parsers are not trained on citation text. 
                    Citation text is different from normal text in that it contains references written in a special format (e.g., author names and publication year written in parentheses; or reference indices written in square brackets). 
                    Many citing sentences contain multiple references, some of which might be grouped together in a pair of parentheses and separated by a comma or a semi-colons. 
                    These references are usually not syntactic nor semantic constituents of the sentences they appear in. 
                    This results in many POS tagging and parsing errors. 
                    We address this issue in the pre-processing stage to improve the performance of the feature extraction component. 
                    We perform three pre-processing steps: 
                </paragraph> 
                <paragraph> 
                    a. Reference Tagging: In the first step, we find and tag all the references that appear in the text. 
                    We use a regular expression to find references and replace each reference with a placeholder. 
                    The reference to the target paper is replaced by the placeholder TREF. 
                    Each other reference is replaced by REF. 
                </paragraph> 
                <paragraph> 
                    b. Reference Grouping: In this step, we identify grouped references (i.e. multiple references listed between one pair of parentheses separated by semicolons). 
                    Each such group is replaced by a placeholder, GREF. 
                    If the target reference is a member of the group, we use a different placeholder: GTREF. 
                </paragraph> 
                <paragraph> 
                    <context>c. Non-syntactic Reference Removal: A <paper>reference</paper> or a group of references <kw>could either be</kw> a <concept>syntactic constituent</concept> and has a semantic role in the sentence or not (<cite id="33" function="ack" polarity="neu">Whidby, 2012</cite>;  <cite id="34" function="ack" polarity="neu">Abu Jbara and Radev, 2012</cite>). </context>
                    If the reference is not a syntactic component in the sentence, we remove it to reduce parsing errors. 
                    <context>Following <author>our previous work</author> (<cite id="35" function="bas" polarity="pos">Abu Jbara and Radev, 2012</cite>), <author>we</author> 
                        <kw>use</kw> a <method>rule-based algorithm</method> to determine whether a reference should be removed from the sentence or kept.</context> 
                    The algorithm uses stylistic and linguistic features such as the style of the reference, the position of the reference, and the surrounding words to make the decision. 
                    When a reference is removed, the head of the closest noun phrase (NP) immediately before the position of the removed reference is used as a representative of the reference. 
                    This is needed for feature extraction as shown later in the paper. 
                </paragraph> 
                <title>3.2 Citation Context Identification</title> 
                <paragraph> 
                    The task of identifying the citation context of a given target reference can be formally defined as follows. 
                    Given a scientific article A that cites another article B, find a set of sentences in A that talk about the work done in B such that at least one of these sentences contains an explicit reference to B. 
                </paragraph> 
                <paragraph> 
                    We treat this problem as a sequence labeling problem. 
                    The goal is to find the globally best sequence of labels for all the sentences that appear within a window around the citing sentence. 
                    The citing sentence is the one that contains an explicit reference to the cited paper. 
                    Each sentence within the window is labeled as INCLUDED or EXCLUDED from the citation context of the given target paper. 
                    To determine the size of the window, we examined a development set of 300 sentences. 
                    We noticed that the related context almost always falls within a window of four sentences. 
                    The window includes the citing sentence, one sentence before the citing sentence, and two sentences after the citing sentence. 
                </paragraph> 
                <paragraph> 
                    We use Conditional Random Fields (CRFs) for sequence labeling. 
                    In particular, we use a first-order chain-structured CRF. 
                    The chain consists of two sets of nodes: 1) a set of hidden nodes Y which represent the context labels of sentences (INCLUDED or EXCLUDED), and 2) a set of observed nodes X which represent the features extracted from the sentences. 
                    The task is to estimate the probability of a sequence of labels Y given the sequence of observed features X: P(Y\!X) 
                    <context>
                        <cite id="36" function="ack" polarity="neu">Lafferty et al. (2001)</cite> define this probability to be a normalized product of potential functions ip:</context>
                    Where ipk (yt, Vt-1, x) is defined as 
                    where f (yt,yt-1, x) is a transition feature function of the label at positions i â€” 1 and i and the observation sequence x; and Xj is a parameter that the algorithm estimates from training data. 
                </paragraph> 
                <paragraph> 
                    The features we use to train the CRF model include structural and lexical features that attempt to capture indicators of relatedness to the given target reference. 
                    The features that we used and their descriptions are listed in table 1. 
                    Table 1: Features used for citation context identification 
                    Category Description Example 
                    Table 2: Annotation scheme for citation purpose. 
                    Motivated by the work of (Spiegel-Rosing, 1977) and (Teufel et al., 2006) 
                </paragraph> 
                <title>3.3 Citation Purpose Classification</title> 
                <paragraph> 
                    In this section, we describe the citation purpose classification task. 
                    <context>Given a target paper B and its citation context (extracted using the method described above) in a given article A, <author>we</author> want to determine the <kw>purpose</kw> of citing B by A. The purpose <kw>is defined as</kw> 
                        <concept>intention behind selecting B and citing it by the author of A</concept> (<cite id="37" function="bas" polarity="pos">Garfield, 1964</cite>).</context> 
                </paragraph> 
                <paragraph> 
                    We use a taxonomy that consists of six categories. 
                    We designed this taxonomy based on our study of similar taxonomies proposed in previous work. 
                    We selected the categories that we believe are more important and useful from a bibliometric point of view, and the ones that can be detected through citation text analysis. 
                    We also tried to limit the number of categories by grouping similar categories proposed in previous work under one category. 
                    The six categories, their descriptions, and an example for each category are listed in Table 2. 
                </paragraph> 
                <paragraph> 
                    We use a supervised approach whereby a classification model is trained on a number of lexical and structural features extracted from a set of labeled citation contexts. 
                    Some of the features that we use to train the classifier are listed in table 3. 
                </paragraph> 
                <title>3.4 Citation Polarity Identification</title> 
                <paragraph> 
                    In this section, we describe the citation polarity identification task. 
                    Given a target paper B and its citation context in a given article A, we want to determine the polarity of the citation text with respect to B. The polarity can be: positive, negative, or neutral (objective). 
                    Positive, negative, and neutral in this context are defined in a slightly different way than their usual sense. 
                    A citation is marked positive if it either explicitly states a strength of the target paper or indicates that the work done in the target paper has been used either by the author or a third-party. 
                    It is also marked as positive if it is compared to another paper (possibly by the same authors) and deemed better in some way. 
                    A citation is marked negative if it explicitly points to a weakness of the target paper. 
                    It is also marked as negative if it is compared to another paper and deemed worse in some way. 
                    A citation is marked as neutral if it is only descriptive. 
                </paragraph> 
                <paragraph> 
                    Similar to citation purpose classification, we use a supervised approach for this problem. 
                    We train a classification model using the same features listed in Table 3. 
                    Due to the high skewness in the data (more than half of the citations are neutral), we use two setups for binary classification. 
                    In the first setup, the citation is classified as Polarized (Subjective) or (Neutral) Objective. 
                    In the second one, Subjective citations are classified as Positive or Negative. 
                    We find that this method gives more intuitive results than using a 3-way classifier. 
                </paragraph> 
            </subsection> 
        </section> 
        <section imrad="r"> 
            <title>4 Evaluation</title> 
            <paragraph> 
                In this section, we describe the data that we used for evaluation and the experiments that we conducted. 
            </paragraph> 
       
            <title>4.1 Data</title> 
            <paragraph> 
                <context>
                    <author>We</author> 
                    <kw>use</kw> the <data>ACL Anthology Network corpus</data> (<data>AAN</data>) (<cite id="38" function="bas" polarity="pos">Radev et al., 2009</cite>; <cite id="39" function="bas" polarity="pos">Radev et al., 2013</cite>) in our evaluation.</context>
                AAN is a publicly available collection of more than 19,000 NLP papers.
                It includes a manually curated citation network of its papers as well as the full text of the papers and the citing sentences associated with each edge in the citation network. 
                From this set, we selected 30 papers that have different numbers of incoming citations and that were consistently cited since they were published. 
                These 30 papers received a total ofabout 3,500 citations from within AAN (average =115 citation/paper, Min = 30, and Max = 338). 
                These citations come from 1,493 unique papers. 
                For each of these citations, we extracted a window of 4 sentences around the reference position. 
                This brings the number of sentences in our dataset to a total of roughly 14,000 sentences. 
                We refer to this dataset as training/testing dataset. 
            </paragraph> 
            <paragraph> 
                In addition to this dataset, we created another dataset that contains 300 citations that cite 5 papers from AAN. 
                We refer to this dataset as the development dataset. 
                This dataset was used to determine the size of the citation context window, and to develop the feature sets used in the three tasks described in Section 3 above. 
            </paragraph> 
            <title>4.2 Annotation</title> 
            <paragraph> 
                In this section, we describe the annotation process. 
                We asked graduate students with good background in NLP (the topic of the annotated sentences) to provide three annotations for each citation example (a window of 4 sentences around the reference anchor) in the training/testing dataset. 
                We asked them to mark the sentences that are related to a given target reference. 
                In addition, we asked them to determine the purpose of citing the target reference by choosing from the six purpose categories that we described earlier. 
                We also asked them to determine whether the citation is negative, positive, or neutral. 
            </paragraph> 
            <paragraph> 
                To estimate the inter-annotator agreement, we picked 400 sentences from the training/testing dataset and assigned them to two different annota-tors. 
                <context>
                    <author>We</author>
                    <kw>use</kw> the <concept>Kappa coefficient</concept> (<cite id="40" function="bas" polarity="pos">Cohen, 1968</cite>) to measure the agreement.</context> 
                The Kappa coefficient is defined as follows: 
                where P(A) is the relative observed agreement among annotators and P(E) is the hypothetical probability of chance agreement. 
                The agreement between the two annotators on the context identification task was K = 0.89. 
                On <method>Landis and Kochs</method> (<cite id="41" function="bas" polarity="pos">Lan-dis and Koch, 1977</cite>) scale, this value <kw>indicates</kw> 
                <result>almost perfect agreement</result>. 
                The agreement on the purpose and the polarity classification task were K = 0.61 and K = 0.66, respectively; which indicates substantial agreement on the same scale. 
                Table 3: The features used for citation purpose and polarity classification 
            </paragraph> 
            <paragraph> 
                The annotation shows that in 22% of the citation examples, the citation context consists of 2 or more sentences. 
                The distribution of the purpose categories in the data was: 14.7% criticism, 8.5% comparison, 17.7% use, 7% substantiation, 5% basis, and 47% other. 
                The distribution of the polarity categories was: 30% positive, 12% negative, and 58% neutral. 
            </paragraph> 
            <title>4.3 Experimental Setup</title> 
            <paragraph> 
                We use the CRF++ toolkit for CRF training and testing. 
                We use the Stanford parser to parse the citation text and generate the dependency parse trees of sentences. 
                We use Weka for classification experiments. 
                We experimented with several classifiers including: SVM, Logistic Regression (LR), and Naive Bayes. 
                All the experiments that we conducted used the training/testing dataset in a 10-fold cross validation mode. 
                All the results have been tested for statistical significance using a 2-tailed paired t-test. 
            </paragraph> 
            <title>4.4 Evaluation of Citation Context Identification</title> 
            <paragraph> 
                We compare the CRF approach to three baselines. 
                The first baseline (ALL) labels all the sentences in the citation window of size 4 as INCLUDED in the citation context. 
                The second baseline (CS-ONLY) labels the citing sentence only as INCLUDED in the citation context. 
                In the third baseline, we use a supervised classification method instead of sequence labeling. 
                We use Support Vector Machines (SVM) to train a model using the same set of features as in the CRF approach. 
            </paragraph> 
            <paragraph> 
                Table 4 shows the precision, recall, and F1 score of the CRF approach and the baselines. 
                The results show that our CRF approach outperforms all the baselines. 
                It also asserts our expectation that addressing this problem as a sequence labeling problem leads to better performance than individual sentence classification, which is also clear from the nature of the task. 
            </paragraph> 
            <paragraph> 
                http://crfpp.googlecode.com/svn/trunk/doc/index.html 
            </paragraph> 
            <paragraph> 
                Feature Analysis: We evaluated the importance of the features listed in Table 1 by computing the chi-squared statistic for every feature with respect to the class. 
                We found that the lexical features (such as determiners and conjunction adverbs) are generally more important than the structural features (such as position and reference count). 
                The features shown in Table 1 are listed in the order of their importance based on this analysis. 
            </paragraph> 
            <title>4.5 Evaluation of Citation Purpose Classification</title> 
            <paragraph> 
                Our experiments with several classification algorithms showed that the SVM classifier outperforms Logistic Regression and Naive Bayes classifiers. 
                Due to space limitations, we only show the results for SVM. 
                Table 5 shows the precision, recall, and F1 for each of the six categories. 
                It also shows the overall accuracy and the Macro-F measure. 
            </paragraph> 
            <paragraph> 
                Feature Analysis: The chi-squared evaluation of the features listed in Table 3 shows that both lexical and structural features are important. 
                It also shows that among lexical features, the ones that are limited to the existence of a direct relation to the target reference (such as closest verb, adjective, adverb, subjective cue, etc.) are most useful. 
                This can be explained by the fact that the restricting the features to having direct dependency relation introduces much less noise than other features (such as Dependency Triplets). 
                Among the structural features, the number of references in the citation context showed to be more useful. 
            </paragraph> 
            <title>4.6 Evaluation of Citation Polarity Identification</title> 
            <paragraph> 
                Similar to the case of citation purpose classification, our experiments showed that the SVM classifier outperforms the other classifiers that we experimented with. 
                Table 6 shows the precision, recall, and F1 for 
                Table 4: Results of citation context identification 
                Table 5: Summary of Citation Purpose Classification Results (10-fold cross validation, SVM: Linear Kernel, c = 1.0) 
                each of the three categories. 
                It also shows the overall accuracy and the Macro-F measure. 
                The analysis of the features used to train this classifier using chi-squared analysis leads to the same conclusions about the relative importance of the features as described in the previous subsection. 
                However, we noticed that features that are related to subjectivity (Subjectivity Cues, Negation, Speculation) are ranked higher which makes sense in the case of polarity classification. 
            </paragraph> 
            <title>4.7 Impact of Context on Classification</title> 
            <paragraph> 
                Accuracy 
                To study the impact of using citation context in addition to the citing sentence on classification performance, we ran two polarity classification experiments. 
                In the first experiment, we used the citing sentence only to extract the features that are used to train the classifiers. 
                In the second experiment, we used the gold context sentences (the ones labeled INCLUDED by human annotators). 
                Table 6 shows the results of the first experiment between rounded parentheses and the results of the second experiments in square brackets. 
                The results show that adding citation context improves the classification accuracy especially in the subjective categories, specially in the negative category if we want to be more specific. 
                This supports our intuition about polarized citations that authors start their review of the cited work with an objective (neutral) sentence and then follow it with their criticism if they have any. 
                We also reached to similar conclusions with purpose classification, but we are not showing the numbers due to space limitations. 
            </paragraph> 
            <title>4.8 Other Experiments</title> 
            <subsection> 
                <title>4.8.1 Can We Do Better?</title> 
                <paragraph> 
                    In this section, we investigate whether it is possible to improve the performance in the two classification tasks. 
                    One factor that we believe could have an 
                    Table 6: Summary of Citation Polarity Classification Results (10-fold cross validation, SVM: Linear Kernel, c = 1.0). 
                    Numbers between rounded parentheses are when only the explicit citing sentence is used (i.e. no context). 
                    Numbers in square brackets are when the gold standard context is used. 
                    impact on the result is the size of the training data. 
                    To examine this hypothesis, we ran the experiment on different sizes of data. 
                    Figure 1 shows the learning curve of the two classifiers for different sizes of training data. 
                    The accuracy increases as more training data is available so we can expect that with even more data, we can do even better. 
                </paragraph> 
                <title>4.8.2 Relation Between Citation</title> 
                <paragraph> 
                    Purpose/Polarity and Citation Count 
                    The main motivation of this work is our hypothetical assumption that using NLP for analyzing citations gives a clearer picture of the impact of the cited work. 
                    As a way to check the validity of this assumption, we study the correlation between the counts of the different purpose and polarity categories. 
                    We also study the correlation between these categories and the total number of citations that a paper received since it was published. 
                    We use the training/testing dataset and the gold annotations for this study. 
                </paragraph> 
                <paragraph> 
                    We compute the Pearson correlation coefficient between the counts of citations from the different categories that a paper received per year since its publication. 
                    We found that, on average, the correlation between positive and negative citations is negative (AVG P = -0.194) and that the correlation between the count of positive citations and the total number of citations is higher than the correlation between negative citations and total citations (AVG P = 0. 
                    531 for positive vs. AVG P = 0.054 for negative). 
                    Figure 1: The effect of size of the data set size on the classifiers accuracy. 
                </paragraph> 
                <paragraph> 
                    Similarly, we noticed that there is a higher positive correlation between Use citations and total citations than in the case of both Substantiation and Basis. 
                    This can be explained by the intuition that publications that present new algorithms, tools, or corpora that are used by the research community become more and more popular with time and thus receive more and more citations. 
                </paragraph> 
                <paragraph> 
                    <context>
                        <data>Figure</data> 2 <kw>shows</kw> the <result>result</result> of running our purpose classifier on all the citations to <cite id="44">Papineni et al.'s (2002)</cite> paper about Bleu, an automatic metric for evaluating Machine Translation (MT) systems.</context> 
                    The figure shows that this paper receives a high number of Use citations. 
                    This makes sense for a paper that describes an evaluation metric that has been widely used in the MT area. 
                    The figure also shows that in the recent years, this metric started to receive some Criticizing citations that resulted in a slight decrease in the number of Use citations. 
                    Such a temporal analysis of citation purpose and polarity is useful for studying the dynamics of research. 
                    It can also be used to detect the emergence or de-emergence of research techniques. 
                    Figure 2: Change in the purpose of the citations to Pap-ineni et al. (2002) 
                </paragraph>
             
            </subsection> 
        </section> 
   
      
        <section> 
            <title>5 Conclusion</title> 
            <paragraph> 
                In this paper, we presented methods for three tasks: citation context identification, citation purpose classification, and citation polarity classification. 
                This work is motivated by the need for more accurate bibliometric measures that evaluates the impact of research both qualitatively and quantitatively. 
                Our experiments showed that we can classify the purpose and polarity of citation with a good accuracy. 
                It also showed that using the citation context improves the classification accuracy and increases the number of polarized citations detected. 
                For future work, we plan to use the output of this research in several applications such as predicting future prominence of publications, studying the dynamics of research, and designing more accurate bibliometric measures. 
            </paragraph> 
        </section> 
        <section> 
            <title>Acknowledgement</title> 
            <paragraph> 
                This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D11PC20153. 
                The U.S. 
                Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. 
                Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. 
                Government. 
            </paragraph> 
        </section> 
 
    </paper>
</annotatedpaper>