<annotatedpaper>
    <paper title="Modeling Category Structures with a Kernel Function" authors="Hiroya Takamura,Yuji Matsumoto, Hiroyasu Yamada" year="2004"> 
        <section>
            <title>Modeling Category Structures with a Kernel Function</title>
            <paragraph>Hiroya Takamura
                Precision and Intelligence Laboratory
                Tokyo Institute of Technology
                4259 Nagatsuta Midori-ku Yokohama,
                226-8503 Japan
                takamura@pi.titech.ac.jp</paragraph>
            <paragraph>Yuji Matsumoto
                Department of Information Technology
                Nara Institute of Science and Technology
                8516-9 Takayama Ikoma Nara,
                630-0101 Japan
                matsu@is.aist-nara.ac.jp</paragraph>
            <paragraph>Hiroyasu Yamada
                School of Information Science
                Japan Advanced Institute of Science and Technology
                1-1 Asahidai Tatsunokuchi Ishikawa, 923-1292 Japan
                h-yamada@jaist.ac.jp</paragraph>

        </section>
        <section>
            <title>Abstract</title>
            <paragraph> We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization. In a number of categorization tasks including text categorization, negative examples are usually more common than positive examples and there may be several different types of negative examples. Therefore, we construct a TOP kernel, regarding the probabilistic model of negative examples as a mixture of several component models respectively corresponding to given categories. Since each component model of our mixture model is expressed using a one-dimensional Gaussian-type function, the proposed kernel has an advantage in computational time. We also show that the computational advantage is shared by a more general class of models.In our experiments, the proposed kernel used with Support Vector Machines outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model.</paragraph>
        </section>
        <section imrad="i">
            <title>1 Introduction</title>
            <paragraph>
                <context>
                    Recently, Support Vector Machines (SVMs) have been actively studied because of their high generalization ability (<cite id="1" function="ack" polarity="neu">Vapnik, 1998</cite>). 
                </context>
                In the formulation of SVMs, functions which measure the similarity of two examples take an important role. These functions are called kernel functions.The usual dot-product of two vectors respectively corresponding to two examples is often used. Although some variants to the usual dot-product are sometimes used (for example, higher-order polynomial kernels and RBF kernels), the distribution of examples is not taken into account in such kernels.</paragraph>
            <paragraph>
                <context>
                    However, <kw>new types of</kw> 
                    <tool>kernels</tool> 
                    <kw>have more recently been proposed</kw>; <kw>they are based on</kw> 
                    <method>the probability distribution of examples</method>. <kw>One is</kw> 
                    <tool>Fisher kernels </tool>(<cite id="2" function="use" polarity="neu">Jaakkola and Haussler, 1998</cite>). <kw>The other is</kw> 
                    <method>TOP (Tangent vector Of the Posterior log-odds)</method> kernels (<cite id="3" function="use" polarity="neu">Tsuda et al., 2002</cite>).
                </context>
                While Fisher kernels are constructed on the basis of a generative model of data, TOP kernels are based on the class-posterior probability, that is, the probability that the positive class occurs given an example. However, in order to use those kernels, we have to select a probabilistic model of data. The selection of a model will affect categorization result. The present paper provides one solution to this issue. Specifically, we proposed one type of TOP kernel, because it has been reported that TOP kernels perform better than Fisher kernels in terms of categorization accuracy.</paragraph>
            <paragraph>We briefly explain our kernel.We focus on negative examples in binary classification.Negative examples are usually more common than positive examples. There may be several different types of negative examples.Furthermore, the categories of negative examples are sometimes explicitly given (for example, the situation where we are given documents, each of which has one of three categories "sports","politics" and "economics", and we are to extract documents with "politics").In such a situation, the probabilistic model of negative examples can be regarded as a mixture of several component models. We effectively use this property. Although many other models can be used, we propose a model based on the separating hyperplanes in the original feature space. Specifically, a one-dimensional Gaussian-type function normal to a hyperplane corresponds to a category. The negative class is then expressed as a kind of Gaussian mixture. The reason for the selection of this model is that the resulting kernel has an advantage in computational time. The kernel based on this mixture model, what we call Hyperplane-based TOP (HP-TOP) kernel, can be computed efficiently in spite of its high dimensionality. We later show that the computational advantage is shared by a more general class of models.</paragraph>
            <paragraph>
                <context>
                    <kw>In the experiments</kw> of text categorization, in which SVMs are used as classifiers, <author>our</author> 
                    <tool>kernel</tool> 
                    <posfeature>outperformed</posfeature> 
                    <tool>the linear kernel</tool> 
                    <kw>and</kw> 
                    <tool>the Fisher kernel</tool> 
                    <kw>based on</kw> 
                    <method>the Probabilistic Latent Semantic Indexing</method> model proposed by <cite id="4" function="bas" polarity="neu">Hofmann (2000)</cite> in terms of categorization accuracy.
                </context>
            </paragraph>
        </section>
        <section imrad="m">
            <title>2 SVMs and Kernel Method</title>
            <paragraph>In this section, we explain SVMs and the kernel method, which are the basis of our research.     
                <context>
                    SVMs have achieved high accuracy in various tasks including text categorization (<cite id="5" function="ack" polarity="neu">Joachims, 1998</cite>; <cite id="6" function="ack" polarity="neu">Dumais et al., 1998</cite>).
                </context>
                Suppose a set Dl of ordered pairs consisting of a feature vector and its label is given. Dl is called training data. I is the set of feature indices. In SVMs, a separating hyperplane (/(x) = w ■ x - b) with the largest margin (the distance between the hyperplane and its nearest vectors) is constructed. Skipping the details of SVMs' formulation, here we just show the conclusion that, using some real numbers a* (Vi) and b*, the optimal hyperplane is expressed as follows:</paragraph>
            <paragraph>We should note that only dot-products of examples are used in the above expression.</paragraph>
            <paragraph>Since SVMs are linear classifiers, their separating ability is limited.
                <context>
                    To compensate for this limitation, the kernel method is usually combined with SVMs (<cite id="7" function="ack" polarity="neu">Vapnik, 1998</cite>).
                </context>
            </paragraph>
            <paragraph>In the kernel method, the dot-products in (2) are replaced with more general inner-products K(xi, x) (kernel functions). The polynomial kernel (xi ■ xj + 1)d (d e N+) and the RBF kernel exp{ — yxi — xj\\/2a} are often used. Using the kernel method means that feature vectors are mapped into a (higher dimensional) Hilbert space and linearly separated there. This mapping structure makes non-linear separation possible, although SVMs are basically linear classifiers.</paragraph>
            <paragraph>Another advantage of the kernel method is that although it deals with a high dimensional (possibly infinite) space, explicit computation of high dimensional vectors is not required. Only the general inner-products of two vectors need to be computed. This advantage leads to a relatively small computational overhead.</paragraph>
      
            <title>3 Kernels from Probabilistic Models</title>
            <paragraph>
                <context>
                <kw>Recently new type of kernels</kw> <kw>which</kw> <feature>connect generative models of data</feature> <kw>and </kw> <method>discriminative classifiers such as SVMs</method>, <kw>have been proposed</kw>: <tool>the Fisher kernel</tool> (<cite id="8" function="use" polarity="neu">Jaakkola and Haussler, 1998</cite>) and the <tool>TOP</tool> (Tangent vector Of the Posterior log-odds) kernel (<cite id="9" function="use" polarity="neu">Tsuda et al., 2002</cite>).
                </context>
            </paragraph>
            
            <subtitle>3.1 Fisher Kernel</subtitle>
            <paragraph>Suppose we have a probabilistic generative model p(x\6) of the data (we denote an example by x). The Fisher score of x is defined as V e logp(x\6), where Ve means partial differentiation with respect to the parameters 0. The Fisher information matrix is denoted by I(0) (this matrix defines the geometric structure of the model space). Then, the Fisher kernel at an estimate 0 is given by:</paragraph>
            <paragraph>The Fisher score of an example approximately indicates how the model will change ifthe example is added to the training data used in the estimation of the model. 
                <context>
                That means, the Fisher kernel between two examples will be large, if the influences of the two examples to the model are similar and large (<cite id="10" function="ack" polarity="neu">Tsuda and Kawanabe, 2002</cite>).
                </context>
            The matrix I(0) is often approximated by the identity matrix to avoid large computational overhead.</paragraph>
            
            
            <subtitle>3.2 TOP Kernel</subtitle>
            <paragraph>On the basis of a probabilistic model of the data, TOP kernels are designed to extract feature vectors f which are considered to be useful for categorization with a separating hyperplane.</paragraph>
            <paragraph>We begin with the proposition that, between the generalization error R(f) and the expected error of the posterior probability D(fe), the relation R(fe) - L* menor que 2D(fe) holds, where L* is the Bayes error. This inequality means that minimizing D(fg ) leads to reducing the generalization error R(fg ). D(fg) is expressed, using a logistic function F(t) = 1/(1 + exp(-t)), as where 0* denotes the actual parameters of the model. The TOP kernel consists of features which can minimize D (f $). In other words, we would like to have feature vectors f that satisfy the following: forcertainvalues ofw and b. For that purpose, we first define a function v(x, 0): The first-order Taylor expansion of v(\, 9* ) around the estimate 9 is then (5) is approximately satisfied. Thus, the TOP kernel is defined as</paragraph>

            <paragraph>
                <context>
                A detailed discussion of the TOP kernel and its theoretical analysis have been given by Tsuda et al (<cite id="11" function="ack" polarity="neu">Tsuda et al., 2002</cite>).
                </context>
            </paragraph>
            
   
            <title>4 Related Work</title>
            <paragraph>
                <context>
                <cite id="12" function="use" polarity="neu">Hofmann (2000)</cite> <kw>applied</kw> <tool>Fisher kernels</tool> <kw>to</kw><task>text categorization</task> <kw>under the</kw> <method>Probabilistic Latent Semantic Indexing (PLSI) model</method> (<cite id="13" function="use" polarity="neu">Hofmann, 1999</cite>).
                </context>
            </paragraph>
            <paragraph>
                
                In PLSI, the joint probability of document d and word w is : where variables zk correspond to latent classes. After the estimation of the model using the EM algorithm, the Fisher kernel for this model is computed. The average log-likelihood of document d normalized by the document length is given by freq(wj,d) Em freq(wm, d) 
                <context>
                <kw>They use</kw> <method>spherical parameterization</method> (<cite id="14" function="use" polarity="neu">Kass and Vos, 1997</cite>) <kw>instead of  the original</kw> <data>parameters</data> in the model.
                </context>
                They define parameters pjk = 2^JP(wj\zk) and pk = 2\JP(zk), and obtained Thus, the Fisher kernel for this model is obtained as described in Appendix A.</paragraph>
            <paragraph>The first term of (31) corresponds to the similarity through latent spaces. The second term corresponds to the similarity through the distribution of each word. The number of latent classes zk can affect the value of the kernel function. 
                <context>
                <kw>In the experiment of</kw> (<cite id="15" function="use" polarity="pos">Hofmann, 2000</cite>), <kw>they</kw> <action>computed the kernels</action> <kw>with the different numbers</kw> (1 to 64) of zk and added them together <kw>to make a</kw> <posfeature>robust</posfeature> <tool>kernel</tool> instead of deciding one specific number of latent classes zk.
                </context>
            
            </paragraph>
            <paragraph>They concluded that the Fisher kernel based on PLSI is effective when a large amount of unlabeled examples are available for the estimation of the PLSI model.</paragraph>
      
            <title>5 Hyperplane-based TOP Kernel</title>
            <paragraph>In this section, we explain our TOP kernel.</paragraph>
            
            <subtitle>5.1 Derivation of HP-TOP kernel</subtitle>
            <paragraph>Suppose we have obtained the parameters wc and bcof the separating hyperplane for each category c e Ccategory in the original feature space, where C'category denotes the set of categories. We assume that the class-posteriors Pc(+1\d) and Pc(-1\d) are expressed as where, for any category x, component function q(d\x) is ofGaussian-type: with the mean px of a random variable wx ■ d - bx and the variance ax. Those parameters are estimated with the maximum likelihood estimation, as follows:</paragraph>
            <paragraph>We choose the Gaussian-type function as an exam-ple.However, this choice is open to argument, since some other models also have the same computational advantage as described in Section 5.4.</paragraph>
            <paragraph>We cannot say q (d \!a i) is a generative probability of d given class x, because it is one-dimensional and not valid as a probability density in the original feature space.</paragraph>
            <paragraph>we parameterize this model using the parameters 0x1 , x2, wx, bx and P(x) (vx e Ccategory) for simplicity. Using this probabilistic model,we compute function v( d, 0) as described in Appendix B (0 denotes {wx, bx, x1, x2\x e Ccategory} and wxi denotes the i-thelementofthe weight vectorwx).</paragraph>
            <paragraph>The partial derivatives of this function with respect to the parameters are in Appendix C. Then we can follow the definition (10) to obtain our version of the TOP kernel. We call this new kernel a hyperplane-based TOP (HP-TOP) kernel.</paragraph>
            
            
            <subtitle>5.2 Properties of HP-TOP kernel</subtitle>
            <paragraph>In the derivatives (39), which provide the largest number of features, original features di are accompanied by other factors computed from probability distributions. This form suggests that two vectors are considered to be more similar, if they have similar distributions over categories. In other words, an occurrence of a word can have different contribution to the classification result, depending on the context (i.e., the other words in the document). This property of the HP-TOP kernel can lead to the effect of word sense disambiguation, because "bank" in a financial document is treated differently from "bank" in a document related to a river-side park.</paragraph>
            <paragraph>The derivatives (34) and (35) correspond to the firstorder differences, respectively for the positive class and the negative class. Similarly, the derivatives (36) and (37) for the second-order differences. The derivatives (40) and (41) are for the first-order differences normalized by the variances.</paragraph>
            <paragraph>The derivatives other than (38) and (38) directly depend on the distance from a hyperplane, rather than on the value of each feature. These derivatives enrich the feature set, when there are few active words, by which we mean the words that do not occur in the training data. For this reason, we expect that the HP-TOP kernel works well for a small training dataset.</paragraph>
            
            
            <subtitle>5.3 Computational issue</subtitle>
            <paragraph>Computing the kernel in this form is time-consuming, because the number of components of type (39) can be very large: where I denotes the set of indices for original features. However, we can avoid this heavy computational cost as follows. Let us compute the dot-product of derivatives (39) of two vectors d and d, which is shown in Appendix D. The last expression (45) is regarded as the scalar product of two dot-products. Thus, by preserving vectors d and we can efficiently compute the dot-product in (39); the computational complexity of a kernel function is on the condition that the original dimension is larger than the number of categories. Thus, from the viewpoint of computational time, our kernel has an advantage over some other kernels such as the PLSI-based Fisher kernel in Section 4, which requires the computational complexity of O(\I\ x \Ccluster\), where Ccluster denotes the set ofclusters.</paragraph>
            <paragraph>In the PLSI-based Fisher kernel, each word has a probability distribution over latent classes. In this sense, the PLSI-based Fisher kernel is more detailed, but detailed models are sometimes suffer overfitting to the training data and have the computational disadvantage as mentioned above.</paragraph>
            <paragraph>The PLSI-based Fisher kernel can be extended to a TOP kernel by using given categories as latent classes. However, the problem of computational time still remains.</paragraph>
            
            
            <subtitle>5.4 General statement about the computational advantage</subtitle>
            <paragraph>So far, we have discussed the computational time for the kernel constructed on the Gaussian mixture.</paragraph>
            <paragraph>However, the computational advantage of the kernel, in fact, is shared by a more general class of models.</paragraph>
            <paragraph>We examine the required conditions for the computational advantage. Suppose the class-posteriors have the mixture form as Equations (16) and (17), but function q(d\x) does not have to be a Gaussian-type function. Instead, function q(d\x) is supposed to be represented using some function r parametrized by we and b, as: where /x is a scalar function. Then, let us obtain the derivative of v(d, 0) with respect to wei, which is thebot-tleneck ofkernel computation: P-c(d) dWei The first two factors of (25) do not depend on i. Therefore, if the last factor of (25) is variable-separable with respectto e and i: where S and T are some function, then the derivative (25) is also variable-separable. In such cases, the efficient computation described in Section 5.3 is possible by preserving the vectors: Table 1: The categories and their sizes of Reuters-21578 We have now obtained the required conditions for the efficient computation: Equation (24) and the variable-separability. In case of Gaussian-type functions, function fe and its derivative with respect to wei are we ■ d - be, di.</paragraph>
            <paragraph>Thus, the conditions are satisfied.</paragraph>
            
        </section>
        <section imrad="r">
            <title>6 Experiments</title>
            <paragraph>Through experiments of text categorization, we empirically compare the HP-TOP kernel with the linear kernel and the PLSI-based Fisher kernel. 
                <context>
                <author>We</author> <kw>use</kw> <data>Reuters-21578 dataset</data> with ModApte-split (<cite id="16" function="bas" polarity="neu">Dumais et al., 1998</cite>). 
                </context>
            Inaddition, we delete some texts from the result ofModApte-split, because those texts have no text body. After the deletion, we obtain 8815 training examples and 3023 test examples. The words that occur less than five times in the whole training set are excluded from the original feature set.</paragraph>
            <paragraph>We do not use all the 8815 training examples. The size of the actual training data ranges from 1000 to 8000. For each dataset size, experiments are executed 10 times with different training sets. The result is evaluated with F-measures for the most frequent 10 categories (Table 1). The total number of categories is actually 116. However, for small categories, reliable statistics cannot be obtained. For this reason, we regard the remaining categories other than the 10 most frequent categories as one category. Therefore, the model for negative examples is a mixture of 10 component models (9 out of the 10 most frequent categories and the new category consisting ofthe remaining categories).</paragraph>
            <paragraph>
                <context>
                <author>We</author> <kw>assume</kw> <concept>uniform priors</concept> for categories <kw>as in</kw> (<cite id="17" function="bas" polarity="pos">Tsuda et al., 2002</cite>).
                </context>
                <context>
            We computed the Fisher kernels with different numbers (10, 20 and 30) of latent classes and added them together to make a robust kernel (<cite id="18" function="ack" polarity="neu">Hofmann, 2000</cite>).
                </context>
            After the learning in the original feature space, the parameters for the probability distributions are estimated with maximum likelihood estimation as in Equations (19) and (20), followed by the learning with the proposed kernel. We used an SVM package, TinySVM, for SVM computation. The soft-margin parameter C was set to 1.0 (other values of C showed no significant changes in results).</paragraph>
            <paragraph>The result is shown in Figure 1 (for macro-average) and Figure 2 (for micro-average). The HP-TOP kernel outperforms the linear kernel and the PLSI-based Fisher kernel for every number of examples. </paragraph>
            <paragraph>At each number of examples, we conducted a Wilcoxon Signed Rank test with 5% significance-level, for the HP-TOP kernel and the linear kernel, since these two are better than the other. The test shows that the difference between the two methods is significant for the training data sizes 1000 to 5000. The superiority of the HP-TOP kernel for small training datasets supports our expectation that the enrichment of feature set will lead to better performance for few active words. Although we also expected that the effect of word sense disambiguation would improve accuracy for large training datasets, the experiments do not provide us with an empirical evidence for the expectation. One possible reason is that Gaussian-type functions do not reflect the actual distribution of data. We leave its further investigation as future research.</paragraph>
            <paragraph>In this experimental setting, the PLSI-based Fisher kernel did not work well in terms of categorization accuracy. 
                <context>
                However, this Fisher kernel will perform better when the number of labeled examples is small and a number of unlabeled examples are available, as reported by <cite id="19" function="ack" polarity="neu">Hofmann (2000)</cite>.
                </context>
            </paragraph>
            <paragraph>We also measured computational time of each method (Figure 3). The vertical axis indicates the average computational time over 100 runs of experiments (10 runs for each category). Please note that training time in this fig- Available from http://www.daviddlewis.com/resources/. Available from http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM/ . HP-TOP Kernel Linear Kernel PLSI-based Fisher Kernel Figure 1: Macro-average ofF-measure PLSI-based Fisher Kernel _i_i_i_i_i_ Figure 2: Micro-average ofF-measure PLSI-based Fisher Kernel » Figure 3: Computational time of each method ure does not include the computational time required for feature extraction. This result empirically shows that the HP-TOP kernel outperforms the PLSI-based Fisher kernel in terms of computational time as theoretically expected in Section 5.3.</paragraph>
        </section>
        <section imrad="d">
            <title>7 Conclusion</title>
            <paragraph>We proposed a TOP kernel based on separating hyperplanes. The proposed kernel is created from one-dimensional Gaussians along the normal directions ofthe hyperplanes. We showed that the computational advantage that the proposed kernel has is shared by a more general class of models. We empirically showed that the proposed kernel outperforms the linear kernel in text categorization.</paragraph>
            <paragraph>Although the superiority of the proposed method to the linear kernel was shown, the proposed method has to be further investigated. Firstly, for large data sizes (namely 7000 and 8000), the proposed method was not significantly better than the linear kernel. The effectiveness of the proposed method should be confirmed by more experiments and theoretical analysis. Secondly, we have to compare the proposed method with other kernels in order to check the effectiveness of the kernel function consisting of one-dimensional Gaussians normal to the hyperplanes. The use of Gaussians is open to argument, because their symmetric form is somewhat against our intuition. If the computational time required for feature extraction is included, the HP-TOP kernel cannot be faster than the linear kernel.</paragraph>
            <paragraph>This model can be extended to incorporate unlabeled examples, for example, using the EM algorithm. In that sense, the combination of PLSI and the semi-supervised EM algorithm is also one promising model. When the category structure of the negative examples is not given, the proposed method is not applicable. We should investigate whether unsupervised clustering can substitute for the category structure.</paragraph>
        </section>
    </paper>
</annotatedpaper>