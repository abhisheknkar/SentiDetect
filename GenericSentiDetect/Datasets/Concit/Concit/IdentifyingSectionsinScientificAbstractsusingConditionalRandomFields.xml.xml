<annotatedpaper>
    <paper title= "Identifying Sections in Scientific Abstracts using Conditional Random Fields" authors= "Kenji Hirohata, Naoaki Okazaki, Sophia Ananiadou, Mitsuru Ishizuka" year="">
        <section>
            Kenji Hirohata†
            hirohata@nii.ac.jp
            Naoaki Okazaki†
            okazaki@is.s.u-tokyo.ac.jp
            Sophia Ananiadou‡
            sophia.ananiadou@manchester.ac.uk
            †Graduate School of Information
            Science and Technology,
            University of Tokyo
            7-3-1 Hongo, Bunkyo-ku,
            Tokyo 113-8656, Japan
            Mitsuru Ishizuka†
            ishizuka@i.u-tokyo.ac.jp
            ‡School of Computer Science,
            University of Manchester
            National Centre for Text Mining (NaCTeM)
            Manchester Interdisciplinary Biocentre,
            131 Princess Street, Manchester M1 7DN, UK
        </section>
        <section>
            <title>
                Abstract
            </title>
            <paragraph>
                OBJECTIVE: The prior knowledge about the rhetorical structure of scientific abstracts is useful for various text-mining tasks such as information extraction, information retrieval, and automatic summarization. This paper presents a novel approach to categorize sentences in scientific abstracts into four sections, objective, methods, results, and conclusions. METHOD: Formalizing the categorization task as a sequential labeling problem, we employ Conditional Random Fields (CRFs) to annotate section labels into abstract sentences. The training corpus is acquired automatically from Medline abstracts. RESULTS: The proposed method outperformed the previous approaches, achieving 95.5% per-sentence accuracy and 68.8% per-abstract accuracy. CONCLUSION: The experimental results showed that CRFs could model the rhetorical structure of abstracts more suitably.
            </paragraph>
        </section>
        <section imrad="i">
            <title>
                1 Introduction
            </title>
            <paragraph>
                Scientific abstracts are prone to share a similar rhetorical structure. For example, an abstract usually begins with the description of background information, and is followed by the target problem, solution to the problem, evaluation of the solution, and conclusion of the paper. 
                <context>
                    <kw> Previous</kw> 
                    <experiment>studies</experiment> 
                    <kw>observed</kw> 
                    <feature>the typical move of rhetorical roles</feature> in scientific abstracts: problem, solution, evaluation, and conclusion (<cite id="1" function="use" polarity="neu">Graetz, 1985</cite>; <cite id="2" function="use" polarity="neu">Salanger-Meyer, 1990</cite>; Naoaki Okazakit Mitsuru Ishizukat School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester Ml 7DN, UK <cite id="3" function="use" polarity="neu">Swales, 1990</cite>; <cite id="4" function="use" polarity="neu">Orasan, 2001</cite>). 
                </context>
                <context>
                    The American National Standard Institute (ANSI) recommends authors and editors of abstracts to state the purpose, methods, results, and conclusions presented in the documents (<cite id="5" function="ack" polarirty="neu">ANSI, 1979</cite>).
                </context>
            </paragraph>
            <paragraph>
                The prior knowledge about the rhetorical structure of abstracts is useful to improve the performance of various text-mining tasks. 
                <context>
                    <cite id="6" function="use" polarity="neu">Marcu (1999)</cite> 
                    <kw>proposed an</kw>
                    <method>extraction method</method> 
                    <kw>for</kw> 
                    <task>summarization</task> that captured the flow of text, based on Rhetorical Structure Theory (RST).
                </context>
                <context>
                    <kw>Some</kw>
                    <method>extraction methods</method> 
                    <kw>make use of</kw>
                    <concept>cue phrases</concept> (e.g., "in conclusion", "our investigation has shown that..."), which suggest that the rhetorical role of sentences is to identify important sentences (<cite id="7" function="use" polarity="neu">Edmundson, 1969</cite>; <cite id="8" function="use" polarity="neu">Paice, 1981</cite>).
                </context>
                We can survey the problems, purposes, motivations, and previous approaches of a research field by reading texts in background sections of scientific papers. 
                <context>
                    <cite id="9" function="ack" polarity="neu">Tbahriti (2006)</cite> improved the performance of their information retrieval engine, giving more weight to sentences referring to purpose and conclusion.
                </context>
            </paragraph>
            <paragraph>
                In this paper, we present a supervised machine-learning approach that categorizes sentences in scientific abstracts into four sections, objective, methods, results, and conclusions. Figure 1 illustrates the task of this study. Given an unstructured abstract without section labels indicated by boldface type, the proposed method annotates section labels of each sentence. 
                <context>
                    Assuming that this task is well formalized as a sequential labeling problem, <author>we</author> 
                    <kw>use</kw> 
                    <method>Conditional Random Fields (CRFs)</method> (<cite id="10" function="bas" polarity="pos">Lafferty et al., 2001</cite>) <kw>to</kw> 
                    <task>identify rhetorical roles in scientific ab-stracts</task>.
                </context>
                
                The proposed method outperforms previous approaches to this problem, achieving 95.5% per- OBJECTIVE: This study assessed the role of adrenergic signal transmission in the control of renal erythropoietin (EPO) production in humans. METHODS: Forty-six healthy male volunteers underwent a hemorrhage of 750 ml. After phlebotomy, they received (intravenously for 6 hours in a parallel, randomized, placebo-controlled and single-blind design) either placebo (0.9% sodium chloride), or the beta 2-adrenergic receptor agonist fenoterol (1.5 microgram/min), or the beta 1-adrenergic receptor agonist dobutamine (5 micrograms/kg/min), or the nonselective beta-adrenergic receptor antagonist propranolol (loading dose of 0.14 mg/kg over 20 minutes, followed by 0.63 micrograms/kg/min). RESULTS: The AUCEPO(0-48 hr)fenoterol was 37% higher (p j 0.03) than AUCEPO(0-48 hr)placebo, whereas AUCEPO(0-48 hr)dobutamine and AUCEPO(0-48 hr)propranolol were comparable with placebo. Creatinine clearance was significantly increased during dobutamine treatment. Urinary cyclic adenosine monophosphate excretion was increased only by fenoterol treatment, whereas serum potassium levels were decreased. Plasma renin activity was significantly increased during dobutamine and fenoterol infusion. CONCLUSIONS: This study shows in a model of controlled, physiologic stimulation of renal erythropoietin production that the beta 2-adrenergic receptor agonist fenoterol but not the beta 1-adrenergic receptor agonist dobutamine is able to increase erythropoietin levels in humans. The result can be interpreted as a hint that signals for the control of erythropoietin production may be mediated by beta 2-adrenergic receptors rather than by beta 1-adrenergic receptors. It appears to be unlikely that an increase of renin concentrations or glomerular filtration rate is causally linked to the control of erythropoietin production in this experimental setting. Figure 1: An abstract with section labels indicated by boldface type (<cite id="11">Gleiter et al., 1997</cite>). sentence accuracy and 68.8% per-abstract accuracy.
            </paragraph>
            <paragraph>
                This paper is organized as follows. Section 2 describes previous approaches to this task. Formalizing the task as a sequential-labeling problem, Section 3 designs a sentence classifier using CRFs. Training corpora for the classifier are acquired automatically from the Medline abstracts. Section 4 reports considerable improvements in the proposed method over the baseline method using Support Vector Machine (SVM) (<cite id="12">Cortes and Vapnik, 1995</cite>). We conclude this paper in Section 5.
            </paragraph>
        </section>
        <section imrad="m">
            <title>
                2 Related Work
            </title>
            <paragraph>
                The previous studies regarded the task of identifying section names as a text-classification problem that determines a label (section name) for each sentence. 
                <context>
                    <kw>Various</kw>
                    <tool>classifiers</tool> 
                    <kw>for</kw> 
                    <task>text categorization</task>, <method>Naive Bayesian Model (NBM)</method>  
                    <cite id="13" function="bas" polarity="pos">(Teufel and Moens, Yamamoto and Takagi, 2005</cite>)<kw> were applied</kw>.
                </context>
            </paragraph>
            <paragraph>
                Table 1 summarizes these approaches and performances. All studies target scientific abstracts except for Teufel and <cite id="14">Moens (2002)</cite> who target scientific full papers. Field classes show the set of section names that each study assumes: background (B), objective/aim/purpose (O), method (M), result (R), conclusion (C), and introduction (I) that combines the background and objective. Although we should not compare directly the performances ofthese studies, which use a different set of classification labels and evaluation corpora, SVM classifiers appear to yield better results for this task. The rest of this section elaborates on the previous studies with SVMs.
            </paragraph>
            <paragraph>
                <context>
                    <cite id="15" function="use" polarity="neu">Shimbo et al. (2003)</cite> 
                    <kw>presented an </kw> 
                    <tool>advanced text retrieval system</tool> for Medline <kw>that</kw> 
                    <task>can focus on a specific section in abstracts specified by a user</task>. 
                </context>
                The system classifies sentences in each Medline abstract into four sections, objective, method, results, and conclusion. Each sentence is represented by words, word bigrams, and contextual information of the sentence (e.g., class of the previous sentence, relative location of the current sentence). 
                
                They reported 91.9% accuracy (per-sentence basis) and 51.2% accuracy (per-abstract basis) for the classification with the best feature set for quadratic SVM. 
                <context>
                    <cite id="16" function="use" polarity="neu">Ito et al. (2004)</cite> 
                    <kw>extended the work with</kw> 
                    <method>a semi-supervised learning technique using transductive SVM</method> (TSVM).
                </context>
            </paragraph>
            <paragraph>
                <context>
                    <cite id="17" function="use" polarity="neu">Yamamoto and Takagi (2005)</cite> 
                    <kw>developed a</kw>
                    <tool>system</tool> 
                    <kw>to</kw> 
                    <task>classify abstract sentences into five sections</task>, background, purpose, method, result, and conclusion. 
                </context>
                They trained a linear-SVM classifier with features such as unigram, subject-verb, verb tense, relative sentence location, and sentence score (average TF*IDF score of constituent words). Their method scores for classifying background, purpose, method, result, and conclusion sentences respectively. They also reported the classification performance of introduction sentences, which combines background and purpose sentences, with 91.3% F-score. An abstract is considered correct if all constituent sentences are correctly labeled.
            </paragraph>
       
            <title>
                3 Proposed method
            </title>
            
            
                3.1 Section identification as a sequence labeling problem
           
            <paragraph>
                The previous work saw the task of labeling as a text categorization that determines the class label yi for each sentence xi. Even though some work includes features of the surrounding sentences for xi, e.g. "class label of xi_i sentence," "class label of xi+1sentence," and "unigram in xi_1 sentence," the classifier determines the class label yi for each sentence xi independently. It has been an assumption for text classification tasks to decide a class label independently of other class labels.
            </paragraph>
            <paragraph>
                However, as described in Section 1, scientific abstracts have typical moves of rhetorical roles: it would be very peculiar if result sentences appearing before method sentences were described in an abstract. Moreover, we would like to model the structure of abstract sentences rather than modeling just the section label for each sentence. Thus, the task is more suitably formalized as a sequence labeling problem: given an abstract with sentences x = (x1,...,xn), determine the optimal sequence of section names y = (y1, ...,yn) of all possible sequences.
            </paragraph>
            <paragraph>
                <context>
                    <method>Conditional Random Fields (CRFs)</method> 
                    <kw>have been successfully applied to</kw> 
                    <task>various NLP tasks</task> including part-of-speech tagging (<cite id="18" function="use" polarity="neu">Lafferty et al., 2001</cite>) and shallow parsing (<cite id="19" function="use" polarity="neu">Sha and Pereira, 2003</cite>).
                </context>

                CRFs define a conditional probability distribution p(y\!x) for output and input sequences, y and x, Therein: function F(y, x) denotes a global feature vector for input sequence x and output sequence y, F(y, x)= ]T f (y, x,i), (2) i ranges over the input sequence, function f (y, x, i) is a feature vector for input sequence x and output sequence y at position i (based on state features and transition features), A is a vector where an element Ak represents the weight of feature Fk(y, x), and Z\(x) is a normalization factor, The optimal output sequence y for an input sequence x, is obtained efficiently by the Viterbi algorithm. 
                    
                <context>
                    <kw>The optimal</kw> 
                    <data>set of parameters A</data>
                    <kw>is determined efficiently by</kw>
                    <method>the Generalized Iterative Scaling (GIS)</method> (<cite id="20" function="bas" polarity="pos" >Darroch and Ratcliff, 1972</cite>) <kw>or</kw>
                    <method>Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)</method> (<cite id="21" function="bas" polarity="pos">Nocedal and Wright, 1999</cite>) method.
                </context>
            </paragraph>
            
            
            
                3.2 Features
            
            <paragraph>
                We design three kinds of features to represent each abstract sentence for CRFs. The contributions of these features will be evaluated later in Section 4.
            </paragraph>
            <paragraph>
                Content (n-gram) This feature examines the existence of expressions that characterize a specific section, e.g. "to determine ...," and "aim at ... " for stating the objective of a study. We use features for sentence contents represented by: i) words, ii) word bi-grams, and iii) mixture of words and word bigrams. 
                <context>
                    <data>Words</data> 
                    <kw>are normalized into their base forms by</kw> 
                    <tool>the GENIA tagger</tool> (<cite id="22" function="bas" polarity="neu">Tsuruoka and Tsujii, 2005</cite>), which is a part-of-speech tagger trained for the biomedical domain.
                </context>
                We measure the co-occurrence strength (xvalue) between each feature and section label. If a feature appears selectively in a specific section, the X value is expected to be high. Thus, we extract the top 200,000 features that have high x values to reduce the total number of features. Table 3.2 shows examples of the top five bigrams that have high xvalues. Table 1: Approaches and performances of previous studies on section identification
            </paragraph>
            <paragraph>
                Relative sentence location An abstract is likely to state objective of the study at the beginning and its conclusion at the end. The position of a sentence may be a good clue for determining its section label. Thus, we design five binary features to indicate relative position of sentences in five scales.
            </paragraph>
            <paragraph>
                Features from previous/next w sentences This reproduces features from previous and following w sentences to the current sentence (w = (0,1, 2}), so that a classifier can make use of the content of the surrounding sentences. Duplicated features have prefixes (e.g. PREV_ and NEXT_) to distinguish their origins.
            </paragraph>
            
            
           
                3.3 Section labels
            
            <paragraph>
                It would require much effort and time to prepare a large amount of abstracts annotated with section labels. Fortunately, some Medline abstracts have section labels stated explicitly by its authors. We examined section labels in 7,811,582 abstracts in the whole Medline, using the regular-expression pattern: A sentence is qualified to have a section name if it begins with up to 4 uppercase token(s) followed by a colon This pattern identified 683,207 (ca.9%) abstracts with structured sections. We chose the number of features based on exploratory experiments. The Medline database was up-to-date on March 2006.
            </paragraph>
            <paragraph>
                Table 3 shows typical moves of sections in Med-line abstracts. The majority of sequences in this table consists of four sections compatible with the ANSI standard, purpose, methods, results, and conclusions. Moreover, the most frequent sequence is "OBJECTIVE — METHOD(S) — RESULTS — CONCLUSION(S)," supposing that AIM and PURPOSE are equivalent to OBJECTIVE. Hence, this study assumes four sections, OBJECTIVE, METHOD, RESULTS, and CONCLUSIONS.
            </paragraph>
            <paragraph>
                <context>
                    Meanwhile, it is common for NP chunking tasks to represent a chunk (e.g., NP) with two labels, the begin (e.g., B-NP) and inside (e.g., I-NP) of a chunk (<cite id="23" function="ack" polarity="neu">Ramshaw and Marcus, 1995</cite>). 
                </context>
                Although none of the previous studies employed this representation, attaching B- and I- prefixes to section labels may improve a classifier by associating clue phrases (e.g., "to determine") with the starts of sections (e.g., B-OBJECTIVE). We will compare classification performances on two sets of label representations: namely, we will compare four section labels and eight labels with BI prefixes attached to section names.
            </paragraph>
            
        </section>
        <section>
            <title>
                4 Evaluation
            </title>
            
            
                4.1 Experiment
            
            <paragraph>
                We constructed two sets of corpora ('pure' and 'expanded'), each of which contains 51,000 abstracts sampled from the abstracts with structured sections. The 'pure' corpus consists of abstracts that have the exact four section labels. In other words, this corpus does not include AIM or PURPOSE sentences even though they are equivalent to OBJECTIVE sentences. The 'pure' corpus is useful to compare the performance of this study with the previous work. Table 2: Bigram features with highx2values ('#' stands for a beginning of a sentence). Rank # abstracts In contrast, the 'expanded' corpus includes sentences in equivalent sections: AIM and PURPOSE sentences are mapped to the OBJECTIVE. Table 4 shows the sets of equivalent sections for representative sections. We created this mapping table manually by analyzing the top 100 frequent section labels found in the Medline. The 'expanded' corpus is close to the real situation in which the proposed method annotates unstructured abstracts.
            </paragraph>
            <paragraph>
                We utilized FlexCRFs implementation to build a classifier with linear-chain CRFs. As a baseline method, we also prepared an SVM classifier with the same features. Number of abstracts for training ra urc c a Figure 2: Training curve Flexible Conditional Random Field Toolkit (FlexCRFs): http://flexcrfs.sourceforge.net/ We used SVM light implementation with the linear kernel, which achieved the best accuracy through this experiment: http://svmlight.joachims.org/
            </paragraph>
            
            
            
                4.2 Results
           
            <paragraph>
                Given the number of abstracts for training n, we randomly sampled n abstracts from a corpus for training and 1,000 abtracts for testing. Content (n-gram) features were generated for each trainig set. We measured the classification accuracy of sentences (per-sentence accuracy) and abstracts (per-abstract accuracy). In per-abstract accuracy, an abstract is considered correct if all constituent sentences are correctly labeled. Table 4: Representative section names and their expanded sections
            </paragraph>
            <paragraph>
                Trained with n = 50, 000 abstracts from 'pure' corpus, the proposed method achieved 95.5% per-sentence accuracy and 68.8% per-abstract accuracy. The F-score for each section label was 98.7% (O), 95.8% (M), 95.0% (R), and 94.2% (C). The proposed method performed this task better than the previous studies by a great margin. Figure 2 shows the training curve for the 'pure' corpus with all features presented in this paper. CRF and SVM methods performed better with more abstracts used for training. This training curve demonstrated that, with less than half the number of training corpus, the proposed method could achieve the same accuracy as the baseline method.
            </paragraph>
            <paragraph>
                Tables 5 and 6 report the performance of the proposed and baseline methods on 'pure' and 'expanded' corpora respectively (n = 10, 000). These tables show per-sentence accuracy followed by per-abstract accuracy in parentheses with different configurations of features (row) and label representations (column). For example, the proposed method obtained 94.3% per-sentence accuracy and 62.9% per-abstract accuracy with 10,000 training abstracts from 'pure' corpus, all features, and BI prefixes for class labels.
            </paragraph>
            <paragraph>
                The proposed method outperformed the baseline method in all experimental configurations. This suggests that CRFs are more suitable for modeling moves of rhetorical roles in scientific abstracts.It is noteworthy that the CRF classifier gained higher per-abstract accuracy than the SVM. For example, both the CRF classifier with features from surrounding sentences (w = 1), and SVM classifier with full features, obtained 93.3% per-sentence accuracy in Table 5. Nevertheless, the per-abstract accuracies of the former and latter were 60.4% and 55.5% respectively: the CRF classifier had roughly 5% advantage on per-abstract accuracy over SVM. This analysis reflects the capability of CRFs to determine the optimal sequence of section names.
            </paragraph>
            <paragraph>
                Additional features such as sentence position and surrounding sentences improved the performance by ca. 5-10%.The proposed method achieved the best results with all features. Another interesting discussion arises with regard to the representations of section labels. The BI representation always boosted the per-abstract accuracy of CRF classifiers by ca. 4-14%. In contrast, the SVM classifier could not leverage the BI representation, and in some configurations, even degraded the accuracy. Table 5: Classification performance (accuracy) on 'pure' corpus (n = 10,000) Table 6: Classification performance (accuracy) on 'expanded' corpus (n = 10,000)
            </paragraph>
            
        </section>
        <section>
            <title>
                5 Conclusion
            </title>
            <paragraph>
                This paper presented a novel approach to identifying rhetorical roles in scientific abstracts using CRFs. The proposed method achieved more successful results than any other previous reports. The CRF classifier had roughly 5% advantage on per-abstract accuracy over SVM. The BI representation of section names also boosted the classification accuracy by 5%. In total, the proposed method gained more than 10% improvement on per-abstract accuracy.
            </paragraph>
            <paragraph>
                We have evaluated the proposed method only on medical literatures. In addition to improving the classification performance, a future direction for this study would be to examine the adaptability of the proposed method to include other types of texts. We are planning to construct a summarization system using the proposed method. 
            </paragraph>
        </section>
    </paper>
</annotatedpaper>