<annotatedpaper>
    <paper title="Weakly Supervised Learning for Hedge Classification in Scientific Literature" authors="Ben Medlock, Ted Briscoe" year="">
        <title>Weakly Supervised Learning for Hedge Classification in Scientific Literature</title>
        <section>
            <paragraph>Ben Medlock
                Computer Laboratory
                University of Cambridge
                Cambridge, CB3 OFD
                benmedlock@cantab.net
                Ted Briscoe
                Computer Laboratory
                University of Cambridge
                Cambridge, CB3 OFD
                ejb@cl.cam.ac.uk</paragraph>
        </section>
        <section> 
            <paragraph>We investigate automatic classification of speculative language ('hedging'), in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented. We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research.</paragraph>
        </section>
        <section>
            <title>1 Introduction</title> 
            <paragraph>The automatic processing of scientific papers using NLP and machine learning (ML) techniques is an increasingly important aspect of technical informatics. In the quest for a deeper machine-driven 'understanding' of the mass of scientific literature, a frequently occuring linguistic phenomenon that must be accounted for is the use of hedging to denote propositions of a speculative nature. Consider the following:</paragraph>
            <paragraph>1. Our results prove that XfK89 inhibits Felin-9.</paragraph>
            <paragraph>2. Our results suggest that XfK89 might inhibit Felin-9.</paragraph>
            <paragraph>The second example contains a hedge, signaled by the use of suggest and might, which renders the proposition inhibit{XfK89 ^Felin-9) speculative. Such analysis would be useful in various applications; for instance, consider a system designed to identify and extract interactions between genetic entities in the biomedical domain. Case 1 above provides clear textual evidence of such an interaction and justifies extraction of inhibit{XfK89 ^Felin-9), whereas case 2 provides only weak evidence for such an interaction.</paragraph>
            <paragraph>Hedging occurs across the entire spectrum of scientific literature, though it is particularly common in the experimental natural sciences. In this study we consider the problem of learning to automatically classify sentences containing instances of hedging, given only a very limited amount of annotator-labelled 'seed' data. This falls within the weakly supervised ML framework, for which a range of techniques have been previously explored. The contributions of our work are as follows:</paragraph>
            <paragraph>1.We provide a clear description of the problem of hedge classification and offer an improved and expanded set of annotation guidelines, which as we demonstrate experimentally are sufficient to induce a high level of agreement between independent annotators.</paragraph>
            <paragraph>2.We discuss the specificities of hedge classification as a weakly supervised ML task.</paragraph>
            <paragraph>3. We derive a probabilistic weakly supervised learning model and use it to motivate our approach.</paragraph>
            <paragraph>4. We analyze our learning model experimentally and report promising results for the task on a new publicly-available dataset.</paragraph>
        </section>
        <section>
            <title>2 Related Work</title>
        </section>

            <subtitle>2.1 Hedge Classification</subtitle>
            
            
            <paragraph>
                <context>
                    <kw>While there is</kw> 
                    <posfeature>a certain amount of literature</posfeature> within the linguistics community on the use of hedging in lavailable from www.cl.cam.ac.uk/ bwm2S/ scientific text, eg. (<cite id="1" function="wea" polarity="neg">Hyland, 1994</cite>), <negfeature>there is little of direct relevance</negfeature> to the task of classifying speculative language from an NLP/ML perspective.
                </context>
            </paragraph>
            
            
            <paragraph>
                <context>
                    <posfeature>The most clearly relevant</posfeature> study <kw>is</kw> 
                    <cite id="2" function="ack" polarity="pos">Light et al. (2004)</cite> where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts.
                </context>
                We will draw on this work throughout our presentation of the task.</paragraph>
            <paragraph>
                <context>
                    Hedging is sometimes classed under the umbrella concept of subjectivity, which covers a variety of linguistic phenomena used to express differing forms of authorial opinion (<cite id="3" function="ack" polarity="neu">Wiebe et al., 2004</cite>). 
            
                </context>
                <context>                 
                    <cite id="4" function="wea" polarity="neg">Riloff et al. (2003)</cite> 
                    <action>explore</action> 
                    <method>bootstrapping techniques</method> 
                    <kw>to</kw> 
                    <task>identify subjective nouns</task> and subsequently classify subjective vs. objective sentences in newswire text. Their work bears some relation to ours; <kw>however</kw>, our domains of interest differ (newswire vs. scientific text) and <negfeature>they do not address the problem of hedge classification directly</negfeature>.
                </context>
            
            
            
            </paragraph>

            <subtitle>2.2 Weakly Supervised Learning</subtitle>
            <paragraph>Recent years have witnessed a significant growth of research into weakly supervised ML techniques for NLP applications. Different approaches are often characterised as either multi- or single-view, where the former generate multiple redundant (or semi-redundant) 'views' of a data sample and perform mutual bootstrapping. 
                <context>
                    This idea was formalised by  <cite id="5" function="ack" polarity="neu">Blum and Mitchell (1998)</cite> in their presentation of co-training.
                </context>
                <context>
                    <method>Co-training</method>
                    <kw>has also been used for</kw> 
                    <task>named entity recognition (NER)</task> (<cite id="6" function="use" polarity="neu">Collins and Singer, 1999</cite>), <task>coreference resolution</task> (<cite id="7" function="use" polarity="neu">Ng and Cardie, 2003</cite>), <task>text categorization</task> (<cite id="8" function="use" polarity="neu" >Nigam and Ghani, 2000</cite>) and <task>improving gene name data</task> (<cite id="9" function="use" polarity="neu">Wellner, 2005</cite>).
                </context>
            
            </paragraph>
            <paragraph>
                <context>
                    Conversely, single-view learning models operate without an explicit partition of the feature space. 
                    Perhaps <kw>the most well known of </kw> 
                    <method>such approaches</method> 
                    <kw>is</kw> 
                    <method>expectation maximization (EM)</method>,<kw> used by </kw> 
                    <cite id="10" function="use" polarity="neu">Nigam et al. (2000)</cite> 
                    <task>for text categorization</task> and by  <cite id="11" function="use" polarity="neu">Ng and Cardie (2003)</cite> in combination with a meta-level feature selection procedure. 
                </context>
                
                Self-training is an alternative single-view algorithm in which a labelled pool is incrementally enlarged with unlabelled samples for which the learner is most confident. 
                <context>
                    Early work by <cite id="12" function="ack" polarity="neu">Yarowsky (1995)</cite> falls within this framework. 
                </context>
                <context>
                    <cite id="13" function="use" polarity="neu">Banko and Brill (2001)</cite> 
                    <kw>use</kw> 
                    <method>'bagging' and agreement</method> 
                    <kw>to</kw> 
                    <task>measure confidence</task> on unlabelled samples, and more recently <cite id="14" function="use" polarity="neu">McClosky et al. (2006)</cite> 
                    <kw>use</kw> 
                    <method>self-training</method> 
                    <kw>for</kw> 
                    <task>improving parse reranking</task>.             
                </context>
            
            </paragraph>
            <paragraph>
               
                <context>
                    Other <kw>relevant recent work</kw> 
                    <kw>includes</kw> (<cite id="15" function="use" polarity="neu" >Zhang, 2004</cite>), <kw>in which</kw> 
                    <method>random feature projection</method> and a committee of SVM classifiers <kw>is used</kw> in a hybrid co/self-training strategy for <task>weakly supervised relation classification</task> and (<cite id="16" function="use" polarity="neu">Chen et al., 2006</cite>) <kw>where a</kw> 
                    <method>graph based algorithm</method> called label propagation is employed to perform weakly supervised relation extraction.
                </context>
            
            </paragraph>

        <section>
            <title>3 The Hedge Classification Task</title>
            <paragraph>Given a collection of sentences, S, the task is to label each sentence as either speculative or non-speculative (spec or nspec henceforth). Specifically, S is to be partitioned into two disjoint sets, one representing sentences that contain some form of hedging, and the other representing those that do not. 
                <context>
                    To further elucidate the nature of the task and improve annotation consistency, <author>we</author> 
                    <action>have developed</action> a new set of guidelines, <kw>building on the work of</kw> 
                    <cite id="17" function="bas" polarity="pos">Light et al. (2004)</cite>. 
                </context>
                As noted by Light et al., speculative assertions are to be identified on the basis of judgements about the author's intended meaning, rather than on the presence of certain designated hedge terms. We begin with the hedge definition given by Light et al. (item 1) and introduce a set of further guidelines to help elucidate various 'grey areas' and tighten the task specification. These were developed after initial annotation by the authors, and through discussion with colleagues. Further examples are given in online Appendix A. The following are considered hedge instances:</paragraph>
            <paragraph>1.An assertion r&amp;#60;elating to a result that does not necessarily follow from work presented, but could be extrapolated from it (Light et al.).</paragraph>
            <paragraph>2. Relay of hedge made in previous work. Dl and Ser have been proposed to act redundantly in the sensory bristle lineage.</paragraph>
            <paragraph>3. Statement of knowledge paucity. available from www.cl.cam.ac.uk/~bwm23/ How endocytosis of Dl leads to the activation of N remains to be elucidated.</paragraph>
            <paragraph>4. Speculative question. A second important question is whether the roX genes have the same, overlapping or complementing functions.</paragraph>
            <paragraph>5. Statement of speculative hypothesis. To test whether the reported sea urchin sequences represent a true RAGl-like match, we repeated the BLASTP search against all GenBank proteins.</paragraph>
            <paragraph>6. Anaphoric hedge reference. This hypothesis is supported by our finding that both pu-pariation rate and survival are affected by EL9.</paragraph>
            <paragraph>The following are not considered hedge instances:</paragraph>
            <paragraph>1. Indication of experimentally observed nonuniversal behaviour. proteins with single BIR domains can also have functions in cell cycle regulation and cytokinesis.</paragraph>
            <paragraph>2. Confident assertion based on external work. Two distinct E3 ubiquitin ligases have been shown to regulate Dl signaling in Drosophila melanogaster.</paragraph>
            <paragraph>3. Statement of existence of proposed alternatives. Different models have been proposed to explain how en-docytosis ofthe ligand, which removes the ligand from the cell surface, results in N receptor activation.</paragraph>
            <paragraph>4. Experimentally-supported confirmation of previous speculation. Here we show that the hemocytes are the main regulator ofadenosine in the Drosophila larva, as was speculated previously for mammals.</paragraph>
            <paragraph>5. Negation of previous hedge. Although the adgf-a mutation leads to larval or pupal death, we have shown that this is not due to the adenosine or deoxyadenosine simply blocking cellular proliferation or survival, as the experiments in vitro would suggest.</paragraph>
        </section>
        <section>
            <title>4 Data</title>
            <paragraph>We used an archive of 5579 full-text papers from the functional genomics literature relating to Drosophila melanogaster (the fruit fly). The papers were converted to XML and linguistically processed using the RASP toolkit. We annotated six of the papers to form a test set with a total of 380 spec sentences and 1157 nspec sentences, and randomly selected 300,000 sentences from the remaining papers as training data for the weakly supervised learner. To ensure selection of complete sentences rather than headings, captions etc., unlabelled samples were chosen under the constraints that they must be at least 10 words in length and contain a main verb. www.informatics.susx.ac.uk/research/nlp/rasp</paragraph>
        </section>
        <section>
            <title>5 Annotation and Agreement</title>
            <paragraph>Two separate annotators were commissioned to label the sentences in the test set, firstly one of the authors and secondly a domain expert with no prior input into the guideline development process.The two annotators labelled the data independently using the guidelines outlined in section 3. Relative F1 (Fle1) and Cohen's Kappa (k) were then used to quantify the level of agreement. 
                <context>
                    For brevity we refer the reader to (<cite id="18" function="ack" polarity="neu">Artstein and Poesio, 2005</cite>) and ( <cite id="19" function="ack" polarity="neu">Hripc-sak and Rothschild, 2004</cite>) for formulation and discussion of k and F1e1 respectively.
                </context>
            </paragraph>
            <paragraph>The two metrics are based on different assumptions about the nature of the annotation task. F1e1is founded on the premise that the task is to recognise and label spec sentences from within a background population, and does not explicitly model agreement on nspec instances.It ranges from 0 (no agreement) to 1 (no disagreement). Conversely, k gives explicit credit for agreement on both spec and nspec instances. The observed agreement is then corrected for 'chance agreement', yielding a metric that ranges between —1 and 1. Given our definition of hedge classification and assessing the manner in which the annotation was carried out, we suggest that the founding assumption of F1e1 fits the nature of the task better than that of k.</paragraph>
            <paragraph>Following initial agreement calculation, the instances of disagreement were examined. It turned out that the large majority of cases of disagreement were due to negligence on behalf of one or other of the annotators (i.e. cases of clear hedging that were missed), and that the cases of genuine disagreement were actually quite rare. New labelings were then created with the negligent disagreements corrected, resulting in significantly higher agreement scores. Values for the original and negligence-corrected labelings are reported in Table 1. Table 1: Agreement Scores</paragraph>
            <paragraph>Annotator conferral violates the fundamental assumption of annotator independence, and so the latter agreement scores do not represent the true level of agreement; however, it is reasonable to conclude that the actual agreement is approximately lower bounded by the initial values and upper bounded by the latter values. In fact even the lower bound is well within the range usually accepted as representing 'good' agreement, and thus we are confident in accepting human labeling as a gold-standard for the hedge classification task. For our experiments, we use the labeling of the genetics expert, corrected for negligent instances.</paragraph>
        </section>
        <section>
            <title>6 Discussion</title>
            <paragraph>In this study we use single terms as features, based on the intuition that many hedge cues are single terms (suggest, likely etc.) and due to the success of 'bag of words' representations in many classification tasks to date. Investigating more complex sample representation strategies is an avenue for future research. </paragraph>
            <paragraph>There are a number of factors that make our formulation of hedge classification both interesting and challenging from a weakly supervised learning perspective. Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features.
                <context>
                    This is <kw>in contrast to</kw> much <kw>previous work</kw>
                    on  weakly supervised learning, where for instance in the case of text categorization (<cite id="20" function="con" polarity="neu">Blum and Mitchell, 1998</cite>; <cite id="21" function="con" polarity="neu">Nigam et al., 2000</cite>) almost all content terms
                    are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stop-word removal). 
                </context>
                
                In the same vein, for the case of entity/relation extraction and classification (<cite id="22" function="ack" polarity="neu">Collins and Singer, 1999</cite>; <cite id="23" function="ack" polarity="neu">Zhang, 2004</cite>; <cite id="24" function="ack" polarity="neu">Chen et al., 2006</cite>) the context of the entity or entities in consideration provides a highly relevant feature space.</paragraph>
                
                
            <paragraph>Another interesting factor in our formulation of hedge classification is that the nspec class is defined on the basis of the absence of hedge cues, rendering it hard to model directly. This characteristic is also problematic in terms of selecting a reliable set of nspec seed sentences, as by definition at the beginning of the learning cycle the learner has little knowledge about what a hedge looks like. This problem is addressed in section 10.3.</paragraph>
            <paragraph>In this study we develop a learning model based around the concept of iteratively predicting labels for unlabelled training samples, the basic paradigm for both co-training and self-training. However we generalise by framing the task in terms of the acquisition of labelled training data, from which a supervised classifier can subsequently be learned.</paragraph>
        </section>
        <section>
            <title>7 A Probabilistic Model for Training Data Acquisition</title>
            <paragraph>In this section, we derive a simple probabilistic model for acquiring training data for a given learning task, and use it to motivate our approach to weakly supervised hedge classification.</paragraph>
            <paragraph>• sample space X</paragraph>
            <paragraph>• target function Y : X — Y</paragraph>
            <paragraph>• set of seed samples for each class S1 ■ ■ ■ SNwhere Si C X and Vx e Sj[Y(x) =</paragraph>
            <paragraph>Aim: Infer a set of training samples Ti for each concept class yi such that Vx e Ti[Y (x) = yij Now, it follows that Vx e Ti [Y(x) = yij is satisfied in the case that Vx e Ti[P(yi\!x) = 1], which leads to a model in which Ti is initialised to Si and then iter-atively augmented with the unlabelled sample(s) for which the posterior probability of class membership is maximal. Formally: At each iteration: where j = argmax[P(yi \!xj) j Expansion with Bayes' Rule yields: P(xj \!yi) • P(yi)" P(xj) .</paragraph>
            <paragraph>An interesting observation is the importance of the sample prior P(xj) in the denominator, often ignored for classification purposes because of its invariance to class. We can expand further by marginalising over the classes in the denominator in expression 2, yielding: so we are left with the class priors and class-conditional likelihoods, which can usually be estimated directly from the data, at least under limited dependence assumptions. The class priors can be estimated based on the relative distribution sizes derived from the current training sets: Efc \!rfc\! where \!S\! is the number of samples in training set S. If we assume feature independence, which as we will see for our task is not as gross an approximation as it may at first seem, we can simplify the class-conditional likelihood in the well known manner: P(xj\!yi) = II P(Xjfc\!yi)k and then estimate the likelihood for each feature: where f (x, S) is the number of samples in training set S in which feature x is present, and a is a universal smoothing constant, scaled by the class prior. This scaling is motivated by the principle that without knowledge of the true distribution of a particular feature it makes sense to include knowledge of the class distribution in the smoothing mechanism. Smoothing is particularly important in the early stages of the learning process when the amount of training data is severely limited resulting in unreliable frequency estimates.</paragraph>
        </section>
        <section>
            <title>8 Hedge Classification</title>
            <paragraph>We will now consider how to apply this learning model to the hedge classification task. As discussed earlier, the speculative/non-speculative distinction hinges on the presence or absence of a few hedge cues within the sentence. Working on this premise, all features are ranked according to their probability of 'hedge cue-ness': which can be computed directly using (4) and (6). The m most probable features are then selected from each sentence to compute (5) and the rest are ignored. This has the dual benefit of removing irrelevant features and also reducing dependence between features, as the selected features will often be nonlocal and thus not too tightly correlated. Note that this idea differs from traditional feature selection in two important ways:</paragraph>
            <paragraph>1. Only features indicative of the spec class are retained, or to put it another way, nspec class membership is inferred from the absence of strong spec features.</paragraph>
            <paragraph>2. Feature selection in this context is not a preprocessing step; i.e. there is no re-estimation after selection. This has the potentially detrimental side effect of skewing the posterior estimates in favour of the spec class, but is admissible for the purposes of ranking and classification by posterior thresholding (see next section).</paragraph>
        </section>
        <section>
            <title>9 Classification</title>
            <paragraph>The weakly supervised learner returns a labelled data set for each class, from which a classifier can be trained. We can easily derive a classifier using the estimates from our learning model by: where a is an arbitrary threshold used to control the precision/recall balance.
                <context>
                    For comparison purposes, <author>we</author> <kw>also use </kw> <tool>Joachims' SVM1ight</tool> (<cite id="25" function="bas" polarity="pos">Joachims, 1999</cite>).
                </context>
            </paragraph>
      
        </section>
        <section>
            <title>10 Experimental</title>
        </section>

            <subtitle>Evaluation 10.1 Method</subtitle>
            <paragraph>To examine the practical efficacy of the learning and classification models we have presented, we use the following experimental method: -"raspec</paragraph>
            <paragraph>1.Generate seed training data: S spe c and S</paragraph>
            <paragraph>2.Initialise: T Spec ^ S spec and T /nspec ^ S nspec</paragraph>
            <paragraph>3. Iterate:</paragraph>
            <paragraph>• Order U by P(spec\!xj) (expression 3)</paragraph>
            <paragraph>• Tpec &amp;#60;— most probable batch</paragraph>
            <paragraph>• Tispec — least probable batch</paragraph>
            <paragraph>• Train classifier using Tpec and Tispec</paragraph>
            <paragraph>3. Iterate:</paragraph>
            <paragraph>• Compute spec recall/precision BEP (break-even point) on the test data</paragraph>
            <paragraph>The batch size for each iteration is set to 0.001 * \U\. After each learning iteration, we compute the precision/recall BEP for the spec class using both classifiers trained on the current labelled data. We use BEP because it helps to mitigate against misleading results due to discrepancies in classification threshold placement. Disadvantageously, BEP does not measure a classifier's performance across the whole of the recall/precision spectrum (as can be obtained, for instance, from receiver-operating characteristic (ROC) curves), but for our purposes it provides a clear, abstracted overview of a classifier's accuracy given a particular training set.</paragraph>

            <subtitle>10.2 Parameter Setting</subtitle>
            <paragraph>The training and classification models we have presented require the setting of two parameters: the smoothing parameter a and the number of features per sample m. Analysis of the effect of varying a on feature ranking reveals that when a = 0, low frequency terms with spurious class correlation dominate and as a increases, high frequency terms become increasingly dominant, eventually smoothing away genuine low-to-mid frequency correlations. This effect is illustrated in Table 2, and from this analysis we chose a = 5 as an appropriate level of smoothing. We use m = 5 based on the intuition that five is a rough upper bound on the number of hedge cue features likely to occur in any one sentence. We use the linear kernel for SVM1ight with the default setting for the regularization parameter C. We construct binary valued, L2-normalised (unit length) input vectors to represent each sentence, as this resulted in better performance than using frequency-based weights and concords with our presence/absence feature estimates.</paragraph>

            <subtitle>10.3 Seed Generation</subtitle> 
            <paragraph>The learning model we have presented requires a set of seeds for each class. To generate seeds for the spec class, we extracted all sentences from U containing either (or both) of the terms suggest or likely, as these are very good (though not perfect) hedge cues, yielding 6423 spec seeds. Generating seeds for nspec is much more difficult, as integrity requires the absence of hedge cues, and this cannot be done automatically. Thus, we used the following procedure to obtain a set of nspec seeds:</paragraph>
            <paragraph>1. Create initial Snspec by sampling randomly from U.</paragraph>
            <paragraph>2. Manually remove more 'obvious' speculative sentences using pattern matching</paragraph>
            <paragraph>• Order Snspec by P(spec\xj) using estimates from Sspec and current Snspec</paragraph>
            <paragraph>• Examine most probable sentences and remove speculative instances</paragraph>
            <paragraph>We started with 8830 sentences and after a couple of hours work reduced this down to a (still potentially noisy) nspec seed set of 7541 sentences.¿ Table 2: Features ranked byP(spec\xk) for varyinga Baseline denotes our probabilistic learning model and classifier (§9) denotes probabilistic learning model with SVM classifier denotes committee-based model (§10.4) 
                <context>
                <kw>with</kw> <tool>probabilistic classifier</tool> denotes committee-based model with SVM classifier<kw>denotes substring</kw> <kw>matching classifier of </kw>(<cite id="26" function="bas" polarity="pos">Light et al., 2004</cite>)
                </context>
                Figure 1: Learning curves and significantly better than the SVM committee-based learning model with an SVM classifier, 'SVM (SVM)', according to a binomial sign test (p &amp;#60; 0.01) after 150 iterations. These results suggest that performance may be enhanced when the learning and classification tasks are carried out by different models. This is an interesting possibility, which we intend to explore further. An important issue in incremental learning scenarios is identification of the optimum stopping point. 
                <context>
                <method>Various methods</method> <action>have been investigated</action> to address this problem, <kw>such as </kw> <method>'counter-training'</method> <cite id="27" function="ack" polarity="neu">(Yan-garber, 2003)</cite> and committee agreement (<cite id="28" function="ack" polarity="neu">Zhang, 2004</cite>);
                </context>
                how such ideas can be adapted for this task is one of many avenues for future research.</paragraph>

            <subtitle>10.4 Baselines</subtitle>
            <paragraph>
                <context>
                As a baseline classifier <author>we</author> <kw>use</kw> <method>the substring matching technique</method> of (<cite id="29" function="bas" polarity="pos">Light et al., 2004</cite>), which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part, possibl, further investigation, unlikely, putative, insights, point toward, promise and propose.
                </context>
            
            </paragraph>
            <paragraph>To provide a comparison for our learning model, we implement a more traditional self-training procedure in which at each iteration a committee of five SVMs is trained on randomly generated overlapping subsets of the training data and their cumulative confidence is used to select items for augmenting the labelled training data. 
                <context>
                For similar work see (<cite id="30" function="ack" polarity="neu">Banko and Brill, 2001</cite>; <cite id="31" function="ack" polarity="neu">Zhang, 2004</cite>)
                </context>
                . 10.5 Results Figure 1 plots accuracy as a function of the training iteration. After 150 iterations, all of the weakly supervised learning models are significantly more accurate than the baseline according to a binomial sign test (p &amp;#60; 0.01), though there is clearly still much room for improvement. The baseline classifier achieves a BEP of 0.60 while both classifiers using our learning model reach approximately 0.76 BEP with little to tell between them. Interestingly, the combination of the SVM committee-based learning model with our classifier (denoted by 'SVM (Prob)'), performs competitively with both of the approaches that use our probabilistic learning model&amp;#60;</paragraph>

            <subtitle>10.6 Error Analysis</subtitle>
            <paragraph>Some errors are due to the variety of hedge forms. For example, the learning models were unsuccessful in identifying assertive statements of knowledge paucity, eg: There is no clear evidence for cy-tochrome c release during apoptosis in C elegans or Drosophila. Whether it is possible to learn such examples without additional seed information is an open question. This example also highlights the potential benefit of an enriched sample representation, in this case one which accounts for the negation of the phrase 'clear evidence' which otherwise might suggest a strongly non-speculative assertion.</paragraph>
            <paragraph>In many cases hedge classification is challenging even for a human annotator. For instance, distinguishing between a speculative assertion and one relating to a pattern of observed non-universal behaviour is often difficult. The following example was chosen by the learner as a spec sentence on the 150th training iteration: Each component consists of a set ofsubcomponents that can be localized within a larger distributed neural system. The sentence does not, in fact, contain a hedge but rather a statement of observed non-universal behaviour. However, an almost identical variant with 'could' instead of 'can' would be a strong speculative candidate. This highlights the similarity between many hedge and non-hedge instances, which makes such cases hard to learn in a weakly supervised manner.</paragraph>

        <section>
            <title>11 Conclusions and Future Work</title>
            <paragraph>We have shown that weakly supervised ML is applicable to the problem of hedge classification and that a reasonable level of accuracy can be achieved. The work presented here has application in the wider academic community; in fact a key motivation in this study is to incorporate hedge classification into an interactive system for aiding curators in the construction and population of gene databases. We have presented our initial results on the task using a simple probabilistic model in the hope that this will encourage others to investigate alternative learning models and pursue new techniques for improving accuracy. Our next aim is to explore possibilities of introducing linguistically-motivated knowledge into the sample representation to help the learner identify key hedge-related sentential components, and also to consider hedge classification at the granularity of assertions rather than text sentences.</paragraph>
        </section>
        <section>
            <title>Acknowledgements</title>
            <paragraph>This work was partially supported by the FlySlip project, BBSRC Grant BBS/B/16291, and we thank Nikiforos Karamanis and Ruth Seal for thorough annotation and helpful discussion. The first author is supported by an University of Cambridge Millennium Scholarship. 
            </paragraph>
        </section>
    </paper>
</annotatedpaper>