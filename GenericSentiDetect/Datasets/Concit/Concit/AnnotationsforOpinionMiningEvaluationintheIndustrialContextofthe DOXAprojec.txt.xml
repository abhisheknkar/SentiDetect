<annotatedpaper><paper title="Annotations for Opinion Mining Evaluation in the Industrial Context of the DOXA project" authors="Patrick Paroubek, Alexander Pak, Djamel Mostefa" year="2010"> 
        <section> 
            <title>Annotations for Opinion Mining Evaluation in the Industrial Context of the DOXA project</title> 
            LIMSI-CNRS, BP 133 
            91403 Orsay cedex, France 
            {alexpak, pap}@limsi.fr 
            Evaluation and Language resources Distribution Agency(ELDA) 
            55-57 rue Brillat-Savarin, 75013 Paris, Francia 
            mostefa@alda.org  
        
            <title>Abstract</title> 
            <paragraph> 
                After presenting opinion and sentiment analysis state of the art and the DOXA project, we review the few evaluation campaigns that have dealt in the past with opinion mining. 
                Then we present the two level opinion and sentiment model that we will use for evaluation in the DOXA project and the annotation interface we use for hand annotating a reference corpus. 
                We then present the corpus which will be used on DOXA and report on the hand-annotation task on a corpus of comments on video games and the solution adopted to obtain a sufficient level of inter-annotator agreement. 
            </paragraph> 
        </section> 
        <section imrad="i"> 
            <title>1. Introduction</title> 
            <paragraph> 
                <context>Along with an interest for incorporating emotions in technological devices, the <kw>recent years have seen</kw> the emergence of <method>automatic opinion</method> and <method>sentiment analysis methods</method>  
                <cite id="1" function="use" polarity="pos">(B.Pang and L.Lee, 2008)</cite> particularly in the <field>image management</field> and <field>survey business</field>
                </context>. Opinions are carried over various media, the press, web sites, radio, television etc. 
                They are a spontaneous source of information, which is updated daily and provides the means to draw quickly an image of the perception that the public entertain with respect to some service, product or major actor of the entertainment or political scene. 
                Survey and analysis of these information sources provide a company with a better knowledge of its customers. 
                They give the means to anticipate new demands, to ensure their fidelity and to reduce attrition risks. 
            </paragraph> 
            <paragraph> 
                The DOXA project aims at specifying and developing components, resources and services which will allow to : 
            </paragraph> 
            <paragraph> 
                Automatically detect topics addressed in large volumes of texts in French and in English, 
                Automatically detect feelings and opinions expressed within large volumes of texts in French and in English, 
                Automatically detect relations between feelings and opinions expressed and the topics concerned by these feelings and opinions, 
                Transform extracted information from texts into structured information to combine this new information with structured information, associated with texts and their authors, to deduct synthetized and exploitable knowledge, by using techniques of data analysis, 
            </paragraph> 
            <paragraph> 
                DOXA is a project (DGE n o 08-2-93-0888) supported by the numeric competitiveness center CAP DIGITAL of Ile-de-France region which aims among other things at defining and implementing an OSA semantic model for opinion mining in an industrial context. 
                See http://www.projet-doxa.fr 
            </paragraph> 
            <paragraph> 
                Integrate the components of texts and data analysis into a new version of the INFOM@GIC's platform (services oriented) to build three applications dedicated to "opinion watch" , "consumers and citizens intelligence" , "customer loyalty and churn" for the end-users of the project : OpinionWay, EDF and Meetic. 
            </paragraph> 
            <paragraph> 
                The applications developed for end-users will help to survey in dynamic, quantitative and qualitative ways: 
            </paragraph> 
            <paragraph> 
                the positioning of consumers, customers and users, 
                the relationships they maintain with the universes about which they express themselves, 
                the trends or evolutions of these universes. 
            </paragraph> 
            <paragraph> 
                They will help to improve both decision-making (On-Line Analytic Processing, segmentation, scoring, etc.) and operational processes (profiling), by integrating enriched knowledge into these processes. 
            </paragraph> 
            <paragraph> 
                In the next section, we will make a rapid survey of the various models we have found in the literature in relation with opinion analysis, and we will draw a picture of their relative positions based on the information dimensions that they consider, as far as it is possible to provide an integrated view based on their widely varying characteristics. 
               <context>This <action>will serve</action> <author>us</author> to <action>locate </action>in the landscape the model of 
                <cite id="2" function="bas" polarity="pos">(Y.Yannik-Mathieu, 1991)</cite> which <kw>was used as</kw> 
                <kw>starting point</kw> for our <data>opinion model</data> in DOXA</context>. 
                Then we will have a second state of the art section, but this time devoted to a rendering of the evaluation activities for opinion mining. 
                Once the background picture has been set we will see how both previous topics are addressed in the context of DOXA with first a presentation of opinion model that will be used for annotating the evaluation corpus and second, a presentation of the annotation guidelines and toolkit. 
            </paragraph> 
        </section>
 
         <section imrad="m">
            <title>2. Opinion Mining and Sentiment Analysis (OSA) Models</title> 
            <paragraph> 
                OSA models vary greatly in their orientation. 
                They may be either oriented toward discovering expression of opinion based on more or less rational considerations, judgments or appreciations, either oriented toward the modeling and representation of the expression of the sentiment/emotions that one entertains about an object or an issue. 
                They vary also greatly in the number of dimensions that they use to represent opinion or sentiments and in the granularity of their semantics. 
            </paragraph> 
            <paragraph> 
                <context>
                    <kw>According to</kw>  
                    <cite id="3" function="ack" polarity="neu">(A.Esuli and F.Sebastiani, 2006</cite>), opinion mining
                   consists both in 
                   searching for the opinions or sentiments expressed in a document and in  acquiring  new methods to automatically perform such analysis</context>. 
                The authors mentioned three main activities of the field: 
            </paragraph> 
            <paragraph> 
                 A1 developing language resources for opinion mining, e.g. building a lexicon of subjective terms; 
                 A2 classifying text according to the expressions of opinion contained; 
                <context>A3 extracting from text opinion expressions, taking into account the relationship that links the expression of opinion (the words expressing the opinion) to the source (the author of the opinion statement) or to the target of the expression of opinion (the object the opinion is about)  <cite id="4" function="ack" polarity="neu">(S.-M.Kim and E.Hovy, 2006</cite>)</context>. 
            </paragraph> 
            <paragraph> 
                To build our synthetic view of the various models we will make use a set of general "features", each one broadly associated with a particular information dimension. 
                The previous definition of the activities associated with opinion mining, provides us with the four main features that we will use in our description of the various models, namely: 
            </paragraph> 
            <paragraph> 
                1. the opinion marker, i.e. the language items expressing an opinion (A1 &amp; A3), 
                2. the opinion polarity, the more or less positive impression felt when one reads the opinion expression (A2), 
                3. the source, the (possibly indirect) reference to the beholder of the opinion (A3), 
                4. and the target, the reference to the object/issue/person about which an opinion is expressed (A3). 
            </paragraph> 
            <paragraph> 
                Among the other features that we will use to organize our presentation of the various models for opinion mining, we have: 
            </paragraph> 
            <paragraph> 
                 the intensity, i.e. the relative strength of an expression, 
                 the theme/topic, whether the models makes use of a representation of the topic addressed, in the document containing an expression of opinion, 
                 the information, the more or less factual aspect of the opinion expression, 
                 the engagement, the relative implication that the opinion holder is supposed to have in supporting his opinion expression. 
            </paragraph> 
            <paragraph> 
                Listing the features sets of all the models we have encountered and putting them into relation yielded a graph that is too complex to be easily displayed because of the numerous links. 
                So we decided to sort our presentation features according to an arbitrary order based on the intuitive importance one would accord to a given feature if it were missing from an opinion statement. 
                In our mind, an opinion statement which would mention only the intensity of an opinion without giving any indication of its polarity should be considered less informative for opinion mining. 
                As a result, we put polarity before intensity in our arbitrary ordering and following the same train of thought, we have afterward: the target, the information, the engagement and and the source. 
                Putting the source last may seem strange, but very often the source is not explicitly mentioned in a document, since the source is the author. 
                Then we sorted the different models of opinion, first according to the number of "features" they display and second according to the relative position of their features in our arbitrary ordering. 
                For instance a model having only the attribute polarity would be judged more generic than a model which would have both polarity and target. 
                With this considerations in mind, the twenty different models organize themselves into a quasi linear sort. 
                From the most generic to the most specific model, we have identified six levels in the hierarchy of models in Figure 1. 
                The first level of our hierarchy lists authors who have not proposed any attribute in particular, but have addressed the subject of opinion and sentiment in language. 
                They are associated in our representation to the most generic (top) attribute OSA model. 
                Level 2 shows authors who do not have any polarity in their model and level 3 those who did not address Intensity, and so on. 
                Then we have used the same methodology at each level to refine our hierarchy. 
                <context>At level 1, <author>we</author> 
                    <action>find</action> 
                    <concept>the models of</concept> 
                    <cite id="5" function="bas" polarity="pos">(R.Quirk et al., 1985)</cite>, <cite id="6" function="bas" polarity="pos">(J.Kamps et al., 2004)</cite> and (S.<cite id="7" function="bas" polarity="pos">Berthard et al., 2004</cite>). 
                    
                    <kw>They have defined</kw> other attributes of opinion expression, like polarity, intensity, target, information etc.</context> 
                    <context> 
                        <cite id="8" function="ack" polarity="neu">(R.Quirk et al., 1985)</cite> have <action>introduced</action> the notion of <concept>private state</concept> which regroups all the expressions of subjectivity like emotions, opinions, attitudes, evaluations etc. </context> 
                   <context> <concept>This notion</concept> 
                <kw>is also present</kw> 
                <kw>in the model of</kw> 
                <cite id="9" function="ack" polarity="neu">(J.Wiebe et al., 2005)</cite>, <cite id="10" function="ack" polarity="neu">(B.Pang V.Hatzivassiloglou, 2003)</cite> were considered more specific than those of level 2 because they stressed the importance of target and source for opinion mining. </context>
                <context> The work of <cite id="11" function="ack" polarity="neu">(Y.Yannik-Mathieu, 1991)</cite> is characterized by a categorization of verbs expressing feelings . The model of <cite id="12" function="ack" polarity="neu">(J.R.Martin and P.R.R.White, 2005)</cite> >deals with evaluative aspects. </context>
                The authors have mentioned three subtypes of evaluation, characterized by their respective attributes which are: attitude, engagement and graduation. 
                Attitude refers to values returned by judgement from one or more sources and can be associated to emotional responses. 
                Its three subtypes are: judgement, affect and appreciation. 
                Engagement explicits the position, the implication of the source with respect to its expression of opinion. 
                It is one of the main characteristics of subjectivity. 
               <context> <cite id="13" function="bas" polarity="pos">(J.R.Martin and P.R.R.White, 2005</cite>) <kw>have introduced</kw> the concept of <concept>graduation</concept> which is further declined using force and focus. 
                It expresses the strength of the opinion expression, so <author>we</author> 
                <action>merged this concept</action> 
                <kw>with intensity</kw> in <author>our</author> 
                <concept>representation</concept>
               </context> . 
               <context> Models of  <cite id="14" function="con" polarity="neu">(Y.Choi et al., 2005</cite>) and <cite id="15" function="con" polarity="neu">(E.Riloff et al., 2003</cite>) <kw>are also</kw> 
                   <kw>at the same level  as </kw> 
                   <cite id="16" function="con" polarity="neu">(E.Riloff et al., 2006</cite>),  <cite id="17" function="con" polarity="neu">(P.Turney and M.Littman, 2003</cite>) and <cite id="18" function="con" polarity="neu">(Y.Yannik-Mathieu, 1991)</cite>. </context>
                   <context> At last,  <cite id="19" function="ack" polarity="pos">(B.Pang and L.Lee, 2008)</cite> and <cite id="20" function="ack" polarity="pos">(J.Wiebe et al., 2005)</cite> 
                       <action>propose the most complete</action> 
                       <method>models</method> which regroup together all our presentation features.</context> 
                Figure 1: Hierarchical OSA models sorting from the most generic (top) to the most specific (bottom), based on the attribute ordering shown in the box, with Opinion being considered the most important and Source the least. 
                When the same author has proposed different models, they are distinguished by publication year. 
            </paragraph> 
            <paragraph> 
                The characteristics that we retain from this survey as important for an OSA model are the three part representation with: the opinion/sentiment expression, its source and its target, and the distinction between sentiments, related to emotions from opinions which retain more a flavor of rationality. 
                We will now survey the evaluation campaigns that proposed an evaluative framework to compare algorithms for OSA. 
            </paragraph> 
        </section> 
        <section imrad="m"> 
            <title>3. Opinion Mining Evaluation Campaigns</title> 
            <paragraph> 
          From 2006 up to 2008, TREC (Text Retrieval Conference) has proposed the Blog Track for searching a corpus of blogs.
                The task consisted of: 
            </paragraph> 
            <paragraph> 
                1. distinguishing whether a post was subjective or objective (no opinion expressed in the document), 
                http://ir.dcs.gla.ac.uk/wiki/TREC-BLOG/ 
                2. separating positive posts from negative ones, 
                3. sorting them by decreasing order of positiveness. 
            </paragraph> 
            <paragraph> 
                Performances observed during the campaign were measured in terms of Mean Average Precision (MAP), and the value measured ranged from 0,17 to 0,45. 
            </paragraph> 
            <paragraph> 
                Since 2006, the NTCIR-MOAT campaign (Multilingual 
                Opinion Analysis Task) proposes to tag newspaper articles. 
                Each sentence must be tagged depending on its relative subjectivity/objectivity and its relevance with respects to an a priori topic. 
                At the clause level, the participants must identify the source and target of opinion, as well as the polarity expressed on a 3 value scale (positive, neuter, negative). 
                In this campaign, identifying subjectivity and relevance have yielded better performance results (F-scores between 0.41 and 0.92) than detecting the source, the target and the polarity of an opinion expression (F-scores between 0 and 0.75). 
                In 2007, SemEval campaign (Semantic Evaluations) has proposed the task Affective Text to explore the connections between emotion and semantic lexicons. 
                The participant had to annotate news headlines along two dimensions, first, according to the emotion expressed, chosen among a list of 6 basic emotions: anger, disgust, fear, joy, sadness, surprise; and second, according to the polarity (positive/negative) of the opinion expressed. 
                The best recognized emotions were: sadness and fear with respective maximum F-scores of 0.30 and 0.20. 
                Anger, joy and surprise have obtained a maximum F-score of 0.15, while disgust was the emotion the most badly recognized with a null F-score. 
            </paragraph> 
            <paragraph> 
                Finally in 2008, TAC campaign (Text Analysis Conference) had a Question Answering task with opinion questions. 
                There were two types of questions: a first batch of questions about the opinion bearer (Who supports what?) and a second one about the opinion itself (What are the critics about... 
                Why do people like... ) 
                Here the F-scores obtained for this two types of questions have ranged from 0.01 to 0.17. 
                In TAC 2008, there was also a task for automatics opinion abstract generation taking as input the answers proposed for a set of questions. 
            </paragraph> 
            <paragraph> 
                In France two evaluation campaigns took place respectively of the series of text mining challenges (DEFT). 
                <context> The first campaign
                    dealt with the assignment of a polarity value to a text bearing an opinion (review of books or of video games for instance), taken from a three value range (bad, average, good)<cite id="21" function="ack" polarity="neu">(J.-B.Berthelin et al., 2008)</cite>
                </context>. 
                The second campaign asked the participants to determine whether a text was subjective or objective as a whole and locally to identify the subjective portions of a text taken from a newspaper corpus made of both factual articles and more opinion oriented ones. 
                For both campaigns, the reference data have been built automatically from the metadata present in each corpus. 
                Results from the 2007 campaign were different depending on the corpus processed. 
                The best results were obtained with the video game reviews (with F-scores values ranging from 0.46 to 0.78) while the corpus of scientific article review was more problematic for the participants (F-scores between 0.40 and 0.57). 
                http://deft09.limsi.fr 
            </paragraph> 
            <paragraph> 
                In 2009, the participants had slightly better success at the global analysis task (with F-scores between 0.66 and 0.92 for the French corpus) than at the task of subjective text spans identification (F-score ranging from 0.65 and 0.91). 
                Note that for the later, the reference data had been produced also by combining the data produced by the participants with a majority vote algorithm. 
            </paragraph> 
        
            <title>4.DOXA Opinion models</title>  
           
                <subtitle>4.1. DOXA full opinion model</subtitle> 
                <paragraph> 
                    <context><author>In DOXA</author>, <concept>the model</concept> that was selected to <kw>serve as an initial basis</kw> for <task>opinion mining</task> is the one <kw>proposed by</kw> 
                    <cite id="22" function="bas" polarity="pos">(Y.Yannik-Mathieu, 1991)</cite>, which relies on a study of the verbs used in French to express sentiments. </context>
                    <context>The <kw>linguistic theory</kw> 
                        <kw>underpinning</kw> its model is the <method>Lexique-Grammaire</method> <cite id="23" function="use" polarity="pos">(M.Gross, 1968)</cite>. </context>
                    In Yannick-Mathieu's work, the verbs expressing sentiment were divided in homogeneous semantic classes (for instance in one class you could find all the verbs expressing joy) further refined into classes based on common linguistic characteristics, mainly sub-categorization frames. 
                    Since some of the classes of the initial model did not fit well the purpose of DOXA and the number of classes was deemed to big to be manageable in the considered applicative context, a new OSA model was designed especially for DOXA jointly by Thales, IGM, ARISEM, LIMSI and LIP6. 
                    Taking into account the need of the end-users of the DOXA project, three levels were defined in the DOXA OSA model (see Figure 2): 
                </paragraph> 
                <paragraph> 
                     macro, which corresponds to the document level, 
                     meso, for the paragraph level, defined as a fixed-size text span in order to avoid any variability because of the type of text processed, 
                     micro, for the sentence level annotations, with a classical definition of sentence based on syntactic and typographic analysis. 
                </paragraph> 
                <paragraph> 
                    The informations for the macro and meso level will be synthesized from the information of the micro level. 
                    Note that both for the macro and meso levels, several categories must be given when the polarity has value "mixed". 
                    There are 17 semantic categories of opinion, split into three broad classes regrouping respectively : the ones associated to affect, the ones associated to intellectual appreciation and the one pertaining to both previous classes. 
                    DOXA model 
                </paragraph> 
            
                <subtitle>4.2. DOXA evaluation opinion model</subtitle> 
                <paragraph> 
                    DOXA evaluation will address only the macro and meso levels because of cost and complexity reasons. 
                    The attributes and their values used in the evaluation opinion model are given in Table 2. 
                    In the model for evaluation we have a three value scale and an alternative value "neutral" for the the documents considered not to express any particular opinion, combined with a two value scale for intensity, which distinguishes weak-medium expression of opinion from strong ones. 
                    This representation, which is completely equivalent to the "neutral" plus five value scale of the full DOXA model, was preferred because deemed more intuitive for the human annotators. 
                    When "neutral" is used the polarity field is to be left undefined. 
                    Note also that the source of an opinion is not annotated since we assume that the source is the author of the document. 
                    As a consequence, reported opinions are ignored. 
                </paragraph> 
                <paragraph> 
                    The idea behind the design of the DOXA evaluation opinion model is to have a model that is as much as possible similar to the DOXA full model and when not, to have a model that can be considered as generalization of the latter, so mapping from the full model to the evaluation model will be straightforward. 
                    Having two different models allows more freedom in the management of the project since the work on the evaluation procedure can start before the DOXA full model for analysis is completely finalized, and thus the deployment of the evaluation can happen sufficiently early in the project lifetime to be able to provide useful feedbackon the technology developed. 
                    The justification feature of the DOXA evaluation model is used by the annotators to identify the text span of the document (or paragraph depending whether they annotate at the meso or macro leve) that represents best the opinion annotations (see example in Table 6. the justification and text information). 
                    This information will not be for performance scoring, but for improving the consistency checking during manual annotation and for enriching the annotation guidelines by building lists of opinion related expressions. 
                    Table 1: The 17 semantic categories of opinion of the full 
                    Figure 2: DOXA opinion annotation full model. 
                </paragraph> 
           
        </section> 
 
        <section imrad="r"> 
            <title>5. Evaluation metrics</title> 
            <paragraph> 
                For evaluation, three metrics will be considered separatly: one for opinion annotations, one for topic annotation and one for the links between topic and opinion. 
                In Table 3 we present the different metrics used. 
                <context> For topic annotation, <author>we</author> 
                    <action>will use</action> the <concept>conceptual distance</concept> 
                    <kw>proposed by</kw> Wu et Palmer  <cite id="24" function="bas" polarity="pos">(Z.Wu and M.Palmer, 1994)</cite>
                </context>. 
                If we call d(x, y) the topological conceptual similarity between two concepts of an ontology, i.e. the number of concepts encountered along the shortest path linking x to y, the conceptual similarity between two concepts C1 and C2, whose least common superconcept is the concept Cca in a hierarchie of root R, is given by the formula : Sim(C1,C2) = 
                d{Cl,ce0)+dS(,Cc"a')+2 xd{cca,R) The performance measure 
                Table 3: Evaluation metrics 
                of a particular link annotation will be given by the product of the performance measure of the opinion annotation (0 or 1 value) by the performance measure of the topic (value obtained with Wu &amp; Palmer conceptual similarity measure). 
                The problem of how to compute the performance measure for the link information when the hypothesis data or the reference data contain several links will be solved by simple combinatorics, computing all possible performance values and keeping the highest performance value (minimal penalty for the system). 
            </paragraph> 
        
            <title>6. Hand annotation tasks</title> 
            <paragraph> 
                The corpus is made of video games reviews, news and posts about video games collected from 8 dedicated web sites. 
                There are 8,000 documents of an average size of 4500 words, split into fixed size paragraphs of 200 words (around 1000 characters). 
                Right now there are around 2,000 documents which have been hand annotated with the DOXA OSA evaluation model (see details in Table 6). 
            </paragraph> 
            <paragraph> 
                <context> As <tool>annotation software</tool>, <author>we</author> 
                    <action>use</action> the Knowtator <cite id="25" function="bas" polarity="neu">(P.Ogren, 2006)</cite> plugin for Protege since the resulting software combination provides an annotation graphic interface coupled to an ontology browser for annotating opinion and sentiment in corpora (see Figure 3)</context>. 
            </paragraph> 
            <paragraph> 
                The annotation speed measured during this period, measured per day per annotator, ranged from 26 paragraphs (3 documents) to 41 paragraphs (5 documents). 
            </paragraph> 
            <paragraph> 
                The first return from the hand annotation task that started in october 2009 with two annotators are the low kappa values, in particular for documents which are only slighty positive or negative in nature and for the topic annotation. 
                Table 6. displays the two measurements of kappa computed considering two different equality functions for topics and semantic category lists : 
            </paragraph> 
            <paragraph> 
                This poor k values can be partially explained by the fact that annotation guidelines were being finalized during this 
                http ://knowtator. sourceforge.net/ 
            </paragraph> 
            <paragraph> 
                Table 2: DOXA macro/meso annotation of opinion. 
            </paragraph> 
            <paragraph> 
                Table 4: Annotation example from the video game corpus with the DOXA evaluation opinion model. 
                Figure 3: Knowtator annotation interface for DOXA. 
                The left hand side window displays the opinion annotation ontology, the middle contains window the text currently annotated with the selected annotation highlighted and the right hand side window shows the current annotation attribute values. 
                trial phase of annotation and that the annotators were new to the task. 
                The topic annotation will have to be reconsidered, maybe by automatic generalization going up in the conceptual hierarchy of several levels and improving the annotation guidelines. 
                An example of annotation guideline is provide in table 6.. 
            </paragraph> 
            <paragraph> 
                Table 5: Two measurements of kappa computed considering two different equality functions for topics and semantic category lists. 
            </paragraph> 
            <paragraph> 
                Table 6: Characteristics of the reference corpus for video games. 
            </paragraph> 
            <paragraph> 
                Table 7: Example of annotation guideline, for a positive polarity with a strong intensity. 
            </paragraph> 
        </section> 
 
        <section imrad="d"> 
            <title>7. conclusion</title> 
            <paragraph> 
                We have presented the context of the project DOXA which aims at developing an opinion mining industrial plateform. 
                After reviewing the state of the art in terms of opinion mining modeling and opinion mining evaluation, we have presented the two opinion mining models developed in DOXA. 
                One model will be used for fine grained opinion mining analysis and the other one, more general, will be used for evaluation. 
                The coexistence of the two models within the same project finds its justification in project management considerations. 
                Having two models enables to start early in the project lifetime the evaluation activities and thus we will be able to benefit from the evaluation feedback within the timeframe of the project. 
                Then we have presented the video game reviews corpus we will use for evaluation along with the annotation toolkit and guidelines, as well a preliminary assessment of the inter-annotator agreement estimated during the initial start up phase of hand annotation. 
                The preliminary results for the kappa are relatively low, in particular for documents which are only slighty positive or negative in nature and for the topic annotation. 
                To improve our kappa for topic annotation, we will consider an automatic generalization going up in the conceptual hierarchy of several levels either during the manual annotation by providing feedback inside the annotation interface, or as post-processing of the annotated data. 
            </paragraph> 
        </section> 
       
            <title>8. Acknwoledgements</title> 
            <paragraph> 
                Thanks to Catherine Gouttas from THALES Communications (TCF) for helping us present the DOXA project. 
            </paragraph> 
        
    </paper>
</annotatedpaper>