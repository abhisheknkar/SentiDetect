<annotatedpaper>
    <paper title="Coherent Citation-Based Summarization of Scientific Papers" authors="Amjad Abu-Jbara, Dragomir Radev" year="2011"> 
        <section> 
            <title>Coherent Citation-Based Summarization of Scientific Papers</title> 
            Amjad Abu-Jbara 
            EECS Department 
            University of Michigan 
            Ann Arbor, MI, USA 
            amjbara@umich.edu 
            Dragomir Radev 
            EECS Department and 
            School of Information 
            University of Michigan 
            Ann Arbor, MI, USA 
            radev@umich.edu  
        </section> 
        <section> 
            <title>Abstract</title> 
            <paragraph> 
                In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper.Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper).Meanwhile, the fluency of the produced summaries has been mostly ignored.For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered.This resulted in noisy and confusing summaries.In this work, we present an approach for producing readable and cohesive citation-based summaries.Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency. 
            </paragraph> 
        </section> 
        <section imrad="i"> 
            <title>1 Introduction</title> 
            <paragraph> 
                Scientific research is a cumulative activity.The work of downstream researchers depends on access to upstream discoveries.The footnotes, end notes, or reference lists within research articles make this accumulation possible.When a reference appears in a scientific paper, it is often accompanied by a span of text describing the work being cited.  
            </paragraph> 
            <paragraph> 
                We name the sentence that contains an explicit reference to another paper citation sentence.Citation sentences usually highlight the most important aspects of the cited paper such as the research problem it addresses, the method it proposes, the good results it reports, and even its drawbacks and limitations.  
            </paragraph> 
            <paragraph> 
                By aggregating all the citation sentences that cite a paper, we have a rich source of information about it.This information is valuable because human experts have put their efforts to read the paper and summarize its important contributions. 
            </paragraph> 
            <paragraph> 
                One way to make use of these sentences is creating a summary of the target paper.This summary is different from the abstract or a summary generated from the paper itself.While the abstract represents the author's point of view, the citation summary is the summation of multiple scholars' viewpoints.The task of summarizing a scientific paper using its set of citation sentences is called citation-based summarization. 
            </paragraph> 
            <paragraph> 
                <context>
                There has been previous work done on citation-based summarization (<cite id="1" function="ack" polarity="neu">Nanba et al., 2000</cite>; <cite id="2" function="ack" polarity="neu">Elkiss et al., 2008</cite>; <cite id="3" function="ack" polarity="neu">Qazvinian and Radev, 2008</cite>; <cite id="4" function="ack" polarity="neu">Mei and Zhai, 2008</cite>; <cite id="5" function="ack" polarity="neu">Mohammad et al., 2009</cite>)
                </context>
                .Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper.The cohesion and the readability of the produced summaries have been mostly ignored.This resulted in noisy and confusing summaries. 
            </paragraph> 
            <paragraph> 
                In this work, we focus on the coherence and readability aspects of the problem.Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing.Our experiments show that our approach produces better summaries than several baseline summarization systems. 
            </paragraph> 
            <paragraph> 
                The rest of this paper is organized as follows. After we examine previous work in Section 2, we outline the motivation of our approach in Section 3. Section 4 describes the three stages of our summarization system. The evaluation and the results are presented in Section 5. Section 6 concludes the paper.  
            </paragraph> 
        </section> 
        <section imrad="m"> 
            <title>2 Related Work</title> 
            <paragraph> 
                The idea of analyzing and utilizing citation information is far from new. 
                <context>
                   The motivation for using information latent in citations has been explored tens of years back (<cite id="6" function="ack" polarity="neu">Garfield et al., 1984</cite>; <cite id="7" function="ack" polarity="neu">Hodges, 1972</cite>).   
                </context>               
                Since then, there has been a large body of research done on citations.  
            </paragraph> 
            <paragraph> 
                <context>
                 <cite id="8" function="ack" polarity="neu">Nanba and Okumura (2000)</cite> analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrase-based rules.
                </context>
                <context>
                <person>They</person> <kw>also used</kw> <method>citation categorization</method> <kw>to support a</kw> <tool>system</tool> for writing surveys (<cite id="9" function="use" polarity="neu">Nanba and Okumura, 1999</cite>).          </context>
                <context>
                <cite id="10" function="ack" polarity="neu">Newman (2001)</cite> analyzed the structure of the citation networks. <cite id="11" function="use" polarity="neu">Teufel et al. (2006)</cite> 
                </context>
                addressed the problem of classifying citations based on their function.  
            </paragraph> 
            <paragraph> 
                <context>
                <cite id="12" function="use" polarity="neu"> Siddharthan and Teufel (2007)</cite> <kw>proposed</kw> a <method>method</method> <kw> for</kw> <task>determining the scientific attribution of an article</task> by analyzing citation sentences. 
                </context>
                <context>
                  <cite id="13" function="use" polarity="neu">Teufel (2007)</cite> <kw>described a</kw> <method>rhetorical classification</method> task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work.    
                </context>
                
            </paragraph> 
            <paragraph> 
                <context>
                    <cite id="14" function="use" polarity="neu">Elkiss et al. (2008)</cite> <kw>performed</kw> a <experiment>study</experiment> on <task>citation summaries</task> and their importance.
                </context>
                <context>
                They concluded that citation summaries are more focused and contain more information than abstracts. <cite id="15" function="use" polarity="neu">Mohammad et al. (2009)</cite> suggested using citation information to generate surveys of scientific paradigms.  
                </context>
            </paragraph> 
            <paragraph> 
                <context>
                 <cite id="16" function="use" polarity="neu">Qazvinian and Radev (2008)</cite> <kw>proposed</kw> a <method>method</method> <kw>for</kw> <task>summarizing scientific</task> articles by building a similarity network of the citation sentences that cite the target paper,
                </context>
                
                 and then applying network analysis techniques to find a set of sentences that covers as much of the summarized paper facts as possible.
                <context>
                     <author>We</author> <kw>use</kw> <method>this method</method> as one of the baselines when we evaluate our approach. <cite id="17" function="bas" polarity="pos">Qazvinian et al. (2010)</cite> 
                     </context>
                     proposed a citation-based summarization method that first extracts a number of important keyphrases from the set of citation sentences, and then finds the best subset of sentences that covers as many keyphrases as possible.
                
                

                     <context>
                         <cite id="18" function="ack" polarity="neu">Qazvinian and Radev (2010)</cite> addressed the problem of identifying the non-explicit citing sentences to aid citation-based summarization.
                     </context>               
                       
            </paragraph> 
       
            <title>3 Motivation</title> 
            <paragraph> 
                The coherence and readability of citation-based summaries are impeded by several factors. First, many citation sentences cite multiple papers besides the target. For example, the following is a citation sentence that appeared in the NLP literature and talked about Resnik's (1999) work.  
            </paragraph> 
            <paragraph> 
                
                <cite id="19" function="con" polarity="neu">(1) Grefenstette and  Nioche (2000)</cite> and  <cite id="20" function="con" polarity="neu">Jones and Ghani (2000)</cite> <kw>use</kw> <tool>the web</tool> to generate corpora for languages where electronic resources are scarce, <kw>while</kw> <cite id="21" function="con" polarity="neu">Resnik (1999)</cite> describes a method for mining the web for bilingual texts. 
            </paragraph> 
            <paragraph> 
                The first fragment of this sentence describes different work other than Resnik's. The contribution of Resnik is mentioned in the underlined fragment. Including the irrelevant fragments in the summary causes several problems. First, the aim of the summarization task is to summarize the contribution of the target paper using minimal text. These fragments take space in the summary while being irrelevant and less important. Second, including these fragments in the summary breaks the context and, hence, degrades the readability and confuses the reader.Third, the existence of irrelevant fragments in a sentence makes the ranking algorithm assign a low weight to it although the relevant fragment may cover an aspect of the paper that no other sentence covers. 
            </paragraph> 
            <paragraph> 
                <context>
                    A second factor has to do with the ordering of the sentences included in the summary.For example, the following are two other citation sentences for <cite id="22" function="ack" polarity="neu">Resnik (1999)</cite>. 
                </context>
                 
            </paragraph> 
            <paragraph> 
                <context>
                    (2) Mining the Web for bilingual text (<cite id="23" function="ack" polarity="neu">Resnik, 1999</cite>) is not likely to provide sufficient quantities of high quality data. 
                </context>
                
            </paragraph> 
            <paragraph> 
                <context>
                   (3) <cite id="24" function="ack" polarity="neu">Resnik (1999)</cite> addressed the issue of language identification for finding Web pages in the languages of interest. 
                </context>
                 
            </paragraph> 
            <paragraph> 
                If these two sentences are to be included in the summary, the reasonable ordering would be to put the second sentence first. 
            </paragraph> 
            <paragraph> 
                Thirdly, in some instances of citation sentences, the reference is not a syntactic constituent in the sentence. It is added just to indicate the existence of citation. For example, in sentence (2) above, the reference could be safely removed from the sentence without hurting its grammaticality.  
            </paragraph> 
            <paragraph> 
                In other instances (e.g. sentence (3) above), the reference is a syntactic constituent of the sentence and removing it makes the sentence ungrammatical. However, in certain cases, the reference could be replaced with a suitable pronoun (i.e. he, she or they). This helps avoid the redundancy that results from repeating the author name(s) in every sentence. 
            </paragraph> 
            <paragraph> 
                <context>
                Finally, a significant number of citation sentences are not suitable for summarization (<cite id="25" function="ack" polarity="neu">Teufel et al., 2006</cite>) and should be filtered out. The following sentences are two examples. 
                </context> 
            </paragraph> 
            <paragraph> 
                <context>
                    (4) The <method>two algorithms</method> <author>we</author> <kw>employed</kw> in our dependency parsing model <kw>are</kw> the <method>Eisner parsing</method> (<cite id="26" function="bas" polarity="pos">Eisner, 1996</cite>) and <method>Chu-Lius algorithm</method> (<cite id="27" function="bas" polarity="pos">Chu and Liu, 1965</cite>). 
                </context>
            </paragraph> 
            <paragraph> 
                <context>
                   (5) This type of model has been used by, among others, <cite id="28" function="ack" polarity="neu">Eisner (1996)</cite>. 
                </context>
                 
            </paragraph> 
            <paragraph> 
                Sentence (4) appeared in a paper by Nguyen et al (2007). It does not describe any aspect of Eisner's work, rather it informs the reader that Nguyen et al. used Eisner's algorithm in their model. There is no value in adding this sentence to the summary of Eisner's paper. 
                <context>
                 <cite id="29" function="ack" polarity="neu">Teufel (2007)</cite> reported that a significant number of citation sentences (67% of the sentences in her dataset) were of this type.    
                </context>
                 
            </paragraph> 
            <paragraph> 
                Likewise, the comprehension of sentence (5) depends on knowing its context (i.e. its surrounding sentences). This sentence alone does not provide any valuable information about Eisner's paper and should not be added to the summary unless its context is extracted and included in the summary as well.  
            </paragraph> 
            <paragraph> 
                In our approach, we address these issues to achieve the goal of improving the coherence and the readability of citation-based summaries. 
            </paragraph> 
      
            <title>4 Approach</title> 
            <paragraph> 
                In this section we describe a system that takes a scientific paper and a set of citation sentences that cite it as input, and outputs a citation summary of the paper. Our system produces the summaries in three stages. In the first stage, the citation sentences are preprocessed to rule out the unsuitable sentences and the irrelevant fragments of sentences. In the second stage, a number of citation sentences that cover the various aspects of the paper are selected. In the last stage, the selected sentences are post-processed to enhance the readability of the summary. We describe the stages in the following three subsections.  
            </paragraph> 
           
                <subtitle>4.1 Preprocessing</subtitle> 
                <paragraph> 
                    The aim of this stage is to determine which pieces of text (sentences or fragments of sentences) should be considered for selection in the next stage and which ones should be excluded. This stage involves three tasks: reference tagging, reference scope identification, and sentence filtering.  
                </paragraph> 
               
                    <title>4.1.1 Reference Tagging</title> 
                    <paragraph> 
                        A citation sentence contains one or more references. At least one of these references corresponds to the target paper. When writing scientific articles, authors usually use standard patterns to include pointers to their references within the text. We use pattern matching to tag such references. The reference to the target is given a different tag than the references to other papers.  
                    </paragraph> 
                    <paragraph> 
                        The following example shows a citation sentence with all the references tagged and the target reference given a different tag. 
                    </paragraph> 
                    <paragraph> 
                        <context>
                    In <cite id="30" function="ack" polarity="neu">Resnik (1999)</cite>, <cite id="31" function="ack" polarity="neu">Nie, Simard, and Foster (2001)</cite>,  <cite id="32" function="ack" polarity="neu">Ma and Liberman (1999)</cite>, and  <cite id="33" function="ack" polarity="neu">Resnik and Smith (2002)</cite>, the Web is harvested in search of pages that are available in two languages.         
                        </context> 
                    </paragraph> 
                    <title>4.1.2 Identifying the Reference Scope</title> 
                    <paragraph> 
                        In the previous section, we showed the importance of identifying the scope of the target reference; i.e. the fragment of the citation sentence that corresponds to the target paper. We define the scope of a reference as the shortest fragment of the citation sentence that contains the reference and could form a grammatical sentence if the rest of the sentence was removed.  
                    </paragraph> 
                    <paragraph> 
                        To find such a fragment, we use a simple yet adequate heuristic. 
                        <context>
                         <author>We</author> <kw>start by</kw> parsing the sentence <kw>using the</kw> <tool>link grammar parser</tool> (<cite id="34" function="bas" polarity="neu">Sleator and Temperley, 1991</cite>).    
                        </context>
                       
                        Since the parser is not trained on citation sentences, we replace the references with placeholders before passing the sentence to the parser. Figure 1 shows a portion of the parse tree for Sentence (1) (from Section 1). Figure 1: An example showing the scope of a target reference  
                    </paragraph> 
                    <paragraph> 
                        We extract the scope of the reference from the parse tree as follows. We find the smallest subtree rooted at an S node (sentence clause node) and contains the target reference node. We extract the text that corresponds to this subtree if it is grammatical. Otherwise, we find the second smallest subtree rooted at an S node and so on. For example, the parse tree shown in Figure 1 suggests that the scope of the reference is:  
                    </paragraph> 
                    <paragraph> 
                        <context>
                          <cite id="35" function="use" polarity="neu">Resnik (1999)</cite> <kw>describes</kw> a <method>method</method> <kw>for</kw> <task>mining the web</task> for bilingual texts.   
                        </context>
                        
                    </paragraph> 
                    <title>4.1.3 Sentence Filtering</title> 
                    <paragraph> 
                        The task in this step is to detect and filter out unsuitable sentences; i.e., sentences that depend on their context (e.g. Sentence (5) above) or describe the own work of their authors, not the contribution of the target paper (e.g Sentence (4) above). Formally, we classify the citation sentences into two classes: suitable and unsuitable sentences. We use a machine learning technique for this purpose. We extract a number of features from each sentence and train a classification model using these features. The trained model is then used to classify the sentences. We use Support Vector Machines (SVM) with linear kernel as our classifier. The features that we use in this step and their descriptions are shown in Table 1.  
                    </paragraph> 
               

            
                <title>4.2 Extraction</title> 
                <paragraph> 
                    In the first stage, the sentences and sentence fragments that are not useful for our summarization task are ruled out. The input to this stage is a set of citation sentences that are believed to be suitable for the summary. From these sentences, we need to select a representative subset. The sentences are selected based on these three main properties:  
                </paragraph> 
                <paragraph> 
                    First, they should cover diverse aspects of the paper. Second, the sentences that cover the same aspect should not contain redundant information. For example, if two sentences talk about the drawbacks of the target paper, one sentence can mention the computation inefficiency, while the other criticize the assumptions the paper makes. Third, the sentences should cover as many important facts about the target paper as possible using minimal text.  
                </paragraph> 
                <paragraph> 
                    In this stage, the summary sentences are selected in three steps.In the first step, the sentences are classified into five functional categories: Background, Problem Statement, Method, Results, and Limitations. In the second step, we cluster the sentences within each category into clusters of similar sentences. In the third step, 
                    <context>
                        <author>we</author> <action>compute</action> <concept>the LexRank</concept> (<cite id="36" function="bas" polarity="pos">Erkan and Radev, 2004</cite>) values for the sentences within each cluster
                    </context>
                    
                    . The summary sentences are selected based on the classification, the clustering, and the LexRank values.  
                </paragraph> 
                <title>4.2.1 Functional Category Classification</title> 
                <paragraph> 
                    We classify the citation sentences into the five categories mentioned above using a machine learning technique. A classification model is trained on a number of features (Table 2) extracted from a labeled set of citation sentences. We use SVM with linear kernel as our classifier.  
                </paragraph> 
                <title>4.2.2 Sentence Clustering</title> 
                <paragraph> 
                    In the previous step we determined the category of each citation sentence. It is very likely that sentences from the same category contain similar or overlapping information. For example, Sentences (6), (7), and (8) below appear in the set of citation Feature Description sentences that cite Goldwater and Griffiths' (2007). These sentences belong to the same category (i.e Method). 
                    <context>
                    Both Sentences (6) and (7) convey the same information about <cite id="37" function="ack" polarity="neu"> Goldwater and Griffiths (2007)</cite> contribution. 
                    </context>
                    Sentence (8), however, describes a different aspect of the paper methodology.  
                </paragraph> 
                <paragraph> 
                    (6)  <cite id="38" function="use" polarity="neu">Goldwater and Griffiths (2007)</cite> <kw>proposed</kw> an <method>information-theoretic measure</method> known as the Variation of Information (VI) 
                </paragraph> 
                <paragraph> 
                    <context>
                        
                    (7)  <cite id="39" function="ack" polarity="neu">Goldwater and Griffiths (2007)</cite> propose using the Variation of Information (VI) metric 
                    </context>
                </paragraph> 
                <paragraph> 
                    <context>
                         (8) <method>A fully-Bayesian approach</method> <kw>to</kw> <task>unsupervised POS tagging </task> <kw>has been developed by</kw>  <cite id="40" function="use" polarity="neu">Goldwater and Griffiths (2007)</cite> as a viable alternative to the traditional maximum likelihood-based HMM approach. 
                    </context>
                   
                </paragraph> 
                <paragraph> 
                    Clustering divides the sentences of each category into groups of similar sentences. 
                    <context>
                      <kw>Following</kw> <cite id="41" function="bas" polarity="pos">Qazvinian and Radev (2008)</cite>, <author>we</author> <action>build</action> a <data>cosine similarity graph</data> out of the sentences of each category.   
                    </context>
                   

                    This is an undirected graph in which nodes are sentences and edges represent similarity relations. Each edge is weighted by the value of the cosine similarity (using TF-IDF vectors) between the two sentences the edge connects. Once we have the similarity network constructed, we partition it into clusters using a community finding technique.
                     <context>
                    <author>We</author> <kw>use</kw> <method>the Clauset algorithm</method> (<cite id="42" function="bas" polarity="pos">Clauset et al., 2004</cite>), a hierarchical ag-glomerative community finding algorithm that runs in linear time.  
                </context>
                </paragraph>
                
                <title>4.2.3 Ranking</title> 
                <paragraph> 
                    Although the sentences that belong to the same cluster are similar, they are not necessarily equally important.
                    <context>
                    <author>We</author> <kw>rank the sentences</kw> within each cluster <kw>by computing their</kw> <concept>LexRank</concept> (<cite id="43" function="bas" polarity="neu">Erkan and Radev, 2004</cite>). Sentences with higher rank are more important. 
                    </context>
                </paragraph> 
       
                <title>4.2.4 Sentence Selection</title> 
                <paragraph> 
                    At this point we have determined (Figure 2), for each sentence, its category, its cluster, and its relative importance. Sentences are added to the summary in order based on their category, the size of their clusters, then their LexRank values. The categories are ordered as Background, Problem, Method, Results, then Limitations. Clusters within each category are ordered by the number of sentences in them whereas the sentences of each cluster are ordered by their LexRank values. Table 1: The features used for sentence filtering Table 2: The features used for sentence classification Figure 2: Example illustrating sentence selection  
                </paragraph> 
                <paragraph> 
                    In the example shown in Figure 2, we have three categories. Each category contains several clusters. Each cluster contains several sentences with different LexRank values (illustrated by the sizes of the dots in the figure.) If the desired length of the summary is 3 sentences, the selected sentences will be in order S1, S12, then S18. If the desired length is 5, the selected sentences will be S1, S5, S12, S15, then S18.  
                </paragraph> 
     
            <title>4.3 Postprocessing</title> 
            <paragraph> 
                In this stage, we refine the sentences that we extracted in the previous stage. Each citation sentence will have the target reference (the author's names and the publication year) mentioned at least once. The reference could be either syntactically and se-mantically part of the sentence (e.g. Sentence (3) above) or not (e.g. Sentence (2)). The aim of this refinement step is to avoid repeating the author's names and the publication year in every sentence. We keep the author's names and the publication year only in the first sentence of the summary. In the following sentences, we either replace the reference with a suitable personal pronoun or remove it. The reference is replaced with a pronoun if it is part of the sentence and this replacement does not make the sentence ungrammatical. The reference is removed if it is not part of the sentence. If the sentence contains references for other papers, they are removed if this doesn't hurt the grammaticality of the sentence.  
            </paragraph> 
            <paragraph> 
                To determine whether a reference is part of the sentence or not, we again use a machine learning approach. We train a model on a set of labeled sentences. The features used in this step are listed in Table 3. The trained model is then used to classify the references that appear in a sentence into three classes: keep, remove, replace. If a reference is to be replaced, and the paper has one author, we use "he/she" (we do not know if the author is male or female). If the paper has two or more authors, we use "they".  
            </paragraph> 
        </section> 
               
                <section imrad="r"> 
                    <title>5 Evaluation</title> 
                    <paragraph> 
                        We provide three levels of evaluation. First, we evaluate each of the components in our system separately. Then we evaluate the summaries that our system generate in terms of extraction quality. Finally, we evaluate the coherence and readability of the summaries.  
                    </paragraph> 
                    <subsection> 
                        <title>5.1 Data</title> 
                        <paragraph> 
                            <context>
                             <author>We</author> <kw>use the</kw> <data>ACL Anthology Network (AAN)</data> (<cite id="44" function="bas" polarity="pos">Radev et al., 2009</cite>) in our evaluation.   
                            </context>
                            

                            AAN is a collection of more than 16000 papers from the Computational Linguistics journal, and the proceedings of the ACL conferences and workshops. AAN provides all citation information from within the network including the citation network, the citation sentences, and the citation context for each paper.  
                        </paragraph> 
                        <paragraph> 
                            We used 55 papers from AAN as our data. The papers have a variable number of citation sentences, ranging from 15 to 348. The total number of citation sentences in the dataset is 4,335. We split the data randomly into two different sets; one for evaluating the components of the system, and the other for evaluating the extraction quality and the readability of the generated summaries. The first set (dataset1, henceforth) contained 2,284 sentences coming from 25 papers. We asked humans with good background in NLP (the area of the annotated papers) to provide two annotations for each sentence in this set: 1) label the sentence as Background, Problem, Method, Result, Limitation, or Unsuitable, 2) for each reference in the sentence, determine whether it could be replaced with a pronoun, removed, or should be kept. Each sentence was given to 3 different annotators. We used the majority vote labels.  
                        </paragraph> 
                        <paragraph> 
                            
                            <author>We</author> <kw>use</kw> <concept>Kappa coefficient</concept> (<cite id="45" function="bas" polarity="pos">Krippendorff, 2003</cite>) to measure the inter-annotator agreement. 
                            
                            Kappa coefficient is defined as: where P(A) is the relative observed agreement among raters and P (E) is the hypothetical probability of chance agreement.  
                        </paragraph> 
                        <paragraph> 
                            The agreement among the three annotators on distinguishing the unsuitable sentences from the other five categories is 0.85. On Landis and Kochs(1977) scale, this value indicates an almost perfect agreement. The agreement on classifying the sentences into the five functional categories is 0.68. On the same scale this value indicates substantial agreement.  
                        </paragraph> 
                        <paragraph> 
                            The second set (dataset2, henceforth) contained 30 papers (2051 sentences). We asked humans with a good background in NLP (the papers topic) to generate a readable, coherent summary for each paper in the set using its citation sentences as the source text. We asked them to fix the length of the summaries to 5 sentences. Each paper was assigned to two humans to summarize.  
                        </paragraph> 
                        <title>5.2 Component Evaluation</title> 
                        <paragraph> 
                            Reference Tagging and Reference Scope Identification Evaluation: We ran our reference tagging and scope identification components on the 2,284 sentences in dataset1. Then, we went through the tagged sentences and the extracted scopes, and counted the number of correctly/incorrectly tagged (extracted)/missed references (scopes). Our tagging - Bkgrnd Prob Method Results Limit. Table 4: Precision and recall results achieved by our citation sentence classifier component achieved 98.2% precision and 94.4% recall. The reference to the target paper was tagged correctly in all the sentences.  
                        </paragraph> 
                        <paragraph> 
                            Our scope identification component extracted the scope of target references with good precision (86.4%) but low recall (35.2%). In fact, extracting a useful scope for a reference requires more than just finding a grammatical substring. In future work, we plan to employ text regeneration techniques to improve the recall by generating grammatical sentences from ungrammatical fragments.  
                        </paragraph> 
                        <paragraph> 
                            Sentence Filtering Evaluation: We used Support Vector Machines (SVM) with linear kernel as our classifier. We performed 10-fold cross validation on the labeled sentences (unsuitable vs all other categories) in dataset1. Our classifier achieved 80.3% accuracy.  
                        </paragraph> 
                        <paragraph> 
                            Sentence Classification Evaluation: We used SVM in this step as well. We also performed 10fold cross validation on the labeled sentences (the five functional categories). This classifier achieved 70.1% accuracy. The precision and recall for each category are given in Table 4 Author Name Replacement Evaluation: The classifier used in this task is also SVM. We performed 10-fold cross validation on the labeled sentences of dataset1. Our classifier achieved 77.41% accuracy. Table 3: The features used for author name replacement Produced using our system There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g. constraint-based techniques and transformation-based techniques. A thorough removal of ambiguity requires a syntactic process. 
                            <context>
                            A rule-based tagger described in <cite id="46" function="ack" polarity="neu">Voutilainen (1995)</cite> was equipped with a set of guessing rules that had been hand-crafted using knowledge of English morphology and intuitions. The precision of rule-based taggers may exceed that of the probabilistic ones. 
                            </context>
                            The construction of a linguistic rule-based tagger, however, has been considered a difficult and time-consuming task. 
                            <context>
                            <action>Produced</action> <kw>using</kw>  <cite id="47" function="bas" polarity="pos">Qazvinian and Radev (2008)</cite> <tool>system</tool>
                            </context>
                            <context>
                            <kw>Another</kw> <method>approach</method> is the rule-based or constraint-based approach, <kw>recently most prominently exemplified by</kw> the <experiment>Constraint Grammar work</experiment>  <cite id="48" function="use" polarity="pos">(Karlsson et al. , 1995; Voutilainen, 1995</cite>b;<cite id="49" function="use" polarity="pos">Voutilainen et al. , 1992;</cite> <cite id="50" function="use" polarity="pos">Voutilainen and Tapanainen, 1993</cite>),
                            </context>
                             where a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context. 
                            <context>
                            Some systems even perform the POS tagging as part of a syntactic analysis process (<cite id="51" function="ack" polarity="neu">Voutilainen, 1995</cite>). 
                            </context>
                            <context>
                            A <tool>rule-based tagger</tool> <kw>described in</kw> (<cite id="52" function="use" polarity="neu">Voutilainen, 1995</cite>) <kw>is equipped</kw> <data>with a set of guessing rules </data>which has been hand-crafted using knowledge of English morphology and intuition. 
                            </context>
                            Older versions of EngCG (using about 1,150 constraints) are reported <cite id="53" function="ack" polarity="neu">( butilainen et al. 1992 </cite>; <cite id="54" function="ack" polarity="neu">Voutilainen and HeikkiUi 1994;</cite> <cite id="55" function="ack" polarity="neu">Tapanainen and Voutilainen 1994</cite>;<cite id="56" function="ack" polarity="neu"> Voutilainen 1995</cite>) to assign a correct analysis to about 99.7% of all words while each word in the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remait unresolved. 
                            <context>
                            <author>We</author> <action>evaluate</action> the resulting disambiguated <data>text</data> <kw>by a number of metrics defined as</kw> follows (<cite id="57" function="bas" polarity="pos">Voutilainen, 1995</cite>a). Table 5: Sample Output  
                            </context>
                        </paragraph> 
                        <title>5.3 Extraction Evaluation</title> 
                        <paragraph> 
                            To evaluate the extraction quality, we use dataset2 (that has never been used for training or tuning any of the system components). We use our system to generate summaries for each of the 30 papers in dataset2. We also generate summaries for the papers using a number of baseline systems (described in Section 5.3.1). All the generated summaries were 5 sentences long. We use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) based on the longest common substrings (ROUGE-L) as our evaluation metric.  
                        </paragraph> 
                    </subsection> 
                    <title>5.3.1 Baselines</title> 
                    <paragraph> 
                        
                        <context>
                        <author>We</author> <kw>evaluate</kw> <feature>the extraction quality of our system</feature> (FL) against 7 different baselines. In the first baseline, the sentences are selected randomly from the set of citation sentences and added to the summary. The second baseline is the <tool>MEAD summarizer</tool> (<cite id="58" function="bas" polarity="pos">Radev et al., 2004</cite>) with all its settings set to default. The third baseline is <tool>LexRank</tool> (<cite id="59" function="bas" polarity="pos">Erkan and Radev, 2004</cite>) run on the entire set of citation sentences of the target paper. The forth baseline is  <cite id="60" function="bas" polarity="pos">Qazvinian and Radev (2008)</cite> citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are ranked using LexRank.
                        </context>
                        The remaining baselines are variations of our system produced by removing one component from the pipeline at a time. In one variation (FL-1), we remove the sentence filtering component. In another variation (FL-2), we remove the sentence classification component; so, all the sentences are assumed to come from one category in the subsequent components. In a third variation (FL-3), the clustering component is removed. To make the comparison of the extraction quality to those baselines fair, we remove the author name replacement component from our system and all its variations.  
                    </paragraph> 
                    <title>5.3.2 Results</title> 
                    <paragraph> 
                        Table 6 shows the average ROUGE-L scores (with 95% confidence interval) for the summaries of the 30 papers in dataset2 generated using our system and the different baselines. The two human summaries were used as models for comparison. The Human score reported in the table is the result of comparing the two human summaries to each others. Statistical significance was tested using a 2-tailed paired t-test. The results are statistically significant at the 0.05 level.  
                    </paragraph> 
                    
                    <paragraph> 
                        The results show that our approach outperforms all the baseline techniques. It achieves higher ROUGE-L score for most of the papers in our testing set. Comparing the score of FL-1 to the score of FL shows that sentence filtering has a significant impact on the results. It also shows that the classification and clustering components both improve the extraction quality.  
                    </paragraph> 
             
                <title>5.4 Coherence and Readability Evaluation</title> 
                <paragraph> 
                    We asked human judges (not including the authors) to rate the coherence and readability of a number of summaries for each of dataset2 papers.
                    <context>
                    For each paper we evaluated 3 summaries. The summary that our system produced, the human summary, and a summary produced by <cite id="61" function="ack" polarity="neu"> Qazvinian and Radev (2008)</cite> summarizer (the best baseline - after our system and its variations - in terms of extraction quality as shown in the previous subsection. )
                    </context>
                    The summaries were randomized and given to the judges without telling them how each summary was produced. The judges were not given access to the source text. They were asked to use a five point-scale to rate how coherent and readable the summaries are, where 1 means that the summary is totally incoherent and needs significant modifications to improve its readability, and 5 means that the summary is coherent and no modifications are needed to improve its readability. We gave each summary to 5 different judges and took the average of their ratings for each summary. 
                    <context>
                    <author>We</author> <kw>used</kw> <method>Weighted Kappa</method> with linear weights (<cite id="62" function="bas" polarity="pos">Cohen, 1968</cite>) to measure the interrater agreement. The Weighted Kappa measure between the five groups of ratings was 0.72. 
                    </context> 
                </paragraph> 
                <paragraph> 
                    Table 7 shows the number of summaries in each rating range. The results show that our approach significantly improves the coherence of citation-based summarization. 
                    <context>
                    Table 5 shows two sample summaries (each 5 sentences long) for the <cite id="63" function="ack" polarity="neu">Voutilainen (1995)</cite> paper. 
                    </context>
                    <context>
                    One summary was produced using our system and the <data>other</data> <kw>was produced</kw> <kw>using</kw>  <cite id="64" function="ack" polarity="neu">Qazvinian andRadev (2008)</cite> <tool>system</tool>.  
                    </context>
                    
                </paragraph> 
          
                </section> 
                <section imrad="d"> 
                    <title>6 Conclusions</title> 
                    <paragraph> 
                        In this paper, we presented a new approach for citation-based summarization of scientific papers that produces readable summaries. Our approach involves three stages. The first stage preprocesses the set of citation sentences to filter out the irrelevant sentences or fragments of sentences. In the second stage, a representative set of sentences are extracted and added to the summary in a reasonable order. In the last stage, the summary sentences are refined to improve their readability. The results of our experiments confirmed that our system outperforms several baseline systems.  
                    </paragraph> 
                </section> 
                <section> 
                    <title>Acknowledgments</title> 
                    <paragraph> 
                        This work is in part supported by the National Science Foundation grant "iOPENER: A Flexible Framework to Support Rapid Learning in Unfamiliar Research Domains", jointly awarded to University of Michigan and University of Maryland as DA021519 to the National Center for Integrative Biomedical Informatics. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the supporters. Table 6: Extraction Evaluation Table 7: Coherence Evaluation  
                    </paragraph> 
                </section> 
    </paper> 
 
 
</annotatedpaper>