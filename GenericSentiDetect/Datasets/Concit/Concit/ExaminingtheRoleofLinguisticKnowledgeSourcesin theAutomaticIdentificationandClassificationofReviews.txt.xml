<annotatedpaper><paper title="Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews"  
authors="Ng,Vicent, Dasgupta, Sajib, Arifin, S.M. Niaz" year="2006"> 
 
<section> 
<title>Abstract</title> 
<paragraph> 
This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative. 
We first demonstrate that review identification can be performed with high accuracy using only unigrams as features. 
We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system. 
</paragraph> 
</section> 
 
<section imrad="i"> 
<title>1 Introduction </title> 
<paragraph> 
Sentiment analysis involves the identification of positive and negative opinions from a text segment. 
<context>The <task>task</task> <kw>has recently received</kw> <posfeature>a lot of attention</posfeature>, <kw>with applications ranging from</kw> multi-perspective <task>question-answering</task> (e.g., <cite id="1" function="use" polarity="pos">Cardie et al. (2004)</cite>) <kw>to</kw> opinion-oriented <task>information extraction</task> (e.g., <cite id="2" function="use" polarity="pos">Riloff et al. (2005)</cite>) and <task>summarization</task> (e.g., <cite id="3" function="use" polarity="pos">Hu and Liu (2004)</cite>).</context> 
Research in sentiment analysis has generally proceeded at three levels, aiming to identify and classify opinions from documents, sentences, and phrases. 
This paper examines two problems in document-level sentiment analysis, focusing on analyzing a particular type of opinionated documents: reviews. 
The first problem, polarity classification, has the goal of determining a review's polarity — positive ("thumbs up") or negative ("thumbs down"). 
Recent work has expanded the polarity classification task to additionally handle documents expressing a neutral sentiment. 
Although studied fairly extensively, polarity classification remains a challenge to natural language processing systems. 
We will focus on an important linguistic aspect of polarity classification: examining the role of a variety of simple, yet under-investigated, linguistic knowledge sources in a learning-based polarity classification system. 
Specifically, we will show how to build a high-performing polarity classifier by exploiting information provided by (1) high order n-grams, (2) a lexicon composed of adjectives manually annotated with their polarity information (e.g., happy is annotated as positive and terrible as negative), (3) dependency relations derived from dependency parses, and (4) objective terms and phrases extracted from neutral documents. 
As mentioned above, the majority of work on document-level sentiment analysis to date has focused on polarity classification, assuming as input a set of reviews to be classified. 
A relevant question is: what if we don't know that an input document is a review in the first place? 
The second task we will examine in this paper — review identification — attempts to address this question. 
Specifically, review identification seeks to determine whether a given document is a review or not. 
We view both review identification and polarity classification as a classification task. 
For review identification, we train a classifier to distinguish movie reviews and movie-related nonreviews (e.g., movie ads, plot summaries) using only unigrams as features, obtaining an accuracy of over 99% via 10-fold cross-validation. 
Similar experiments using documents from the book domain also yield an accuracy as high as 97%. 
An analysis of the results reveals that the high accuracy can be attributed to the difference in the vocabulary employed in reviews and non-reviews: while reviews can be composed of a mixture of subjective and objective language, our non-review documents rarely contain subjective expressions. 
<context>Next, <author>we</author> <kw>learn</kw> our <task>polarity classifier</task> using positive and negative reviews taken <kw>from</kw> two movie review datasets, one assembled by <cite id="4" function="bas" polarity="pos">Pang and Lee (2004)</cite> and the other by ourselves.</context> 
The resulting classifier, when trained on a feature set derived from the four types of linguistic knowledge sources mentioned above, achieves a 10-fold cross-validation accuracy of 90.5% and 86.1% on Pang et al.'s dataset and ours, respectively. 
To our knowledge, our result on Pang et al.'s dataset is one of the best reported to date. 
Perhaps more importantly, an analysis of these results show that the various types of features interact in an interesting manner, allowing us to draw conclusions that provide new insights into polarity classification. 
</paragraph> 
</section> 
 
<section imrad="m"> 
<title> 2 Related Work</title> 
<subsection> 
<subtitle> 2.1 Review Identification</subtitle> 
<paragraph> 
As noted in the introduction, while a review can contain both subjective and objective phrases, our non-reviews are essentially factual documents in which subjective expressions can rarely be found. 
Hence, review identification can be viewed as an instance of the broader task of classifying whether a document is mostly factual/objective or mostly opinionated/subjective. 
<context><kw>There have been attempts on</kw> tackling this so-called document-level subjectivity classification task, <kw>with</kw> <posfeature>very encouraging results</posfeature> (see <cite id="5" function="ack" polarity="pos">Yu and Hatzivassiloglou (2003)</cite> and <cite id="6" function="ack" polarity="pos">Wiebe et al. (2004)</cite> for details).</context> 
</paragraph> 
<subtitle> 2.2 Polarity Classification</subtitle> 
<paragraph> 
(2005) ). 
Below we will center our discussion of related work around the four types of features we will explore for polarity classification. 
Higher-order n-grams. 
<context><kw>While</kw> n-grams offer a simple way of capturing context, previous work has rarely explored the use of n-grams as features in a polarity classification system beyond un-igrams. 
Two <kw>notable exceptions are</kw> the work of <cite id="7" function="ack" polarity="neu">Dave et al. (2003)</cite> and <cite id="8" function="ack" polarity="neu">Pang et al. (2002)</cite>.</context> 
Interestingly, while Dave et al. report good performance on classifying reviews using bigrams or trigrams alone, Pang et al. show that bigrams are not useful features for the task, whether they are used in isolation or in conjunction with unigrams. 
This motivates us to take a closer look at the utility of higher-order n-grams in polarity classification. 
Manually-tagged term polarity. 
<context>Much work has been performed on learning to identify and classify polarity terms (i.e., terms expressing a positive sentiment (e.g., happy) or a negative sentiment (e.g., terrible)) and exploiting them to do polarity classification (e.g., <cite id="9" function="ack" polarity="neu">Hatzivassiloglou and McKeown (1997)</cite>, <cite id="10" function="ack" polarity="neu">Turney (2002)</cite>, <cite id="11" function="ack" polarity="neu">Kim and Hovy (2004)</cite>, <cite id="12" function="ack" polarity="neu">Whitelaw et al. (2005)</cite>, <cite id="13" function="ack" polarity="neu">Esuli and Se-bastiani (2005))</cite>.</context> 
Though reasonably successful, these (semi-)automatic techniques often yield lexicons that have either high coverage/low precision or low coverage/high precision. 
While manually constructed positive and negative word lists exist (e.g., General Inquirer), they too suffer from the problem of having low coverage. 
This prompts us to manually construct our own polarity word listsand study their use in polarity classification. 
Dependency relations. 
<context>There have been several attempts at extracting features for polarity classification from dependency parses, <kw>but most focus on</kw> extracting specific types of information such as adjective-noun relations (e.g., <cite id="14" function="wea" polarity="neg">Dave et al. (2003)</cite>, <cite id="15" function="wea" polarity="neg">Yi et al. (2003)</cite>) or nouns that enjoy a dependency relation with a polarity term (e.g., <cite id="16" function="wea" polarity="neg">Popescu and Et-zioni (2005)</cite>).</context> 
<context><cite id="17" function="wea" polarity="neg">Wilson et al. (2005)</cite> extract a larger variety of features from dependency parses, <kw>but unlike us</kw>, their goal is to determine the polarity of a phrase, <kw>not a</kw> document.</context> 
In comparison to previous work, we investigate the use of a larger set of dependency relations for classifying reviews. 
Objective information. 
The objective portions of a review do not contain the author's opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task. 
<context><kw>Indeed</kw>, <posfeature>recent work</posfeature> <kw>has shown that</kw> benefits can be made by first separating facts from opinions in a document (e.g, <cite id="18" function="ack" polarity="pos">Yu and Hatzivassiloglou (2003)</cite>) and classifying the polarity based solely on the subjective portions of the document (e.g., <cite id="19" function="ack" polarity="pos">Pang and Lee (2004)</cite>).</context> 
<context><kw>Motivated by</kw> the <paper>work of</paper> <cite id="20" function="bas" polarity="pos">Koppel and Schler (2005)</cite>, <author>we</author> <kw>identify and extract</kw> <posfeature>objective material</posfeature> <kw>from</kw> nonreviews and show how to exploit such information in polarity classification.</context> 
http://www.wjh.harvard.edu/~inquirer/ spreadsheet_guid. htm 
<context><cite id="21" function="wea" polarity="neg">Wilson et al. (2005)</cite> have also manually tagged a list of terms with their polarity, <kw>but this</kw> list <negfeature>is not publicly available</negfeature>.</context> 
Finally, previous work has also investigated features that do not fall into any of the above categories. 
<context>For instance, <kw>instead of</kw> representing the polarity of a term using a binary value, <cite id="22" function="con" polarity="neu">Mullen and Collier (2004)</cite> use <cite id="23" function="con" polarity="neu">Turney's (2002)</cite> method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration.</context> 
</paragraph> 
</subsection> 
</section> 
 
<section imrad="m"> 
<title>3 Review Identification</title> 
<paragraph> 
Recall that the goal of review identification is to determine whether a given document is a review or not. 
Given this definition, two immediate questions come to mind. 
First, should this problem be addressed in a domain-specific or domain-independent manner? 
In other words, should a review identification system take as input documents coming from the same domain or not? 
Apparently this is a design question with no definite answer, but our decision is to perform domain-specific review identification. 
The reason is that the primary motivation of review identification is the need to identify reviews for further analysis by a polarity classification system. 
Since polarity classification has almost exclusively been addressed in a domain-specific fashion, it seems natural that its immediate upstream component — review identification — should also assume domain specificity. 
Note, however, that assuming domain specificity is not a self-imposed limitation. 
In fact, we envision that the review identification system will have as its upstream component a text classification system, which will classify documents by topic and pass to the review identifier only those documents that fall within its domain. 
Given our choice of domain specificity, the next question is: which documents are non-reviews? 
Here, we adopt a simple and natural definition: a non-review is any document that belongs to the given domain but is not a review. 
Dataset. 
Now, recall from the introduction that we cast review identification as a classification task. 
To train and test our review identifier, we use 2000 reviews and 2000 non-reviews from the movie domain. 
The 2000 reviews are taken from Pang et al.'s polarity dataset (version 2.0), which consists of an equal number of positive and negative reviews. 
We collect the non-reviews for the movie domain from the Internet Movie Database website, randomly selecting any documents from this site that are on the movie topic but are not reviews themselves. 
With this criterion in mind, the 2000 non-review documents we end up with are either movie ads or plot summaries. 
Available from http://www.cs.cornell.edu/ people/pabo/movie-review-data. 
Training and testing the review identifier. 
<context><author>We</author> <kw>perform</kw> 10-fold cross-validation (CV) <experiment>experiments on</experiment> the above dataset, <kw>using</kw> <cite id="24" function="bas" polarity="pos">Joachims' (1999)</cite> SVMHgh* package to train an SVM classifier <kw>for</kw> distinguishing reviews and non-reviews.</context> 
All learning parameters are set to their default values. 
Each document is first tokenized and downcased, and then represented as a vector of unigrams with length normalization. 
<context><kw>Following</kw> <cite id="25" function="bas" polarity="pos">Pang et al. (2002)</cite>, <author>we</author> <kw>use</kw> frequency as presence.</context> 
In other words, the ith element of the document vector is 1 if the corresponding unigram is present in the document and 0 otherwise. 
The resulting classifier achieves an accuracy of 99.8%. 
Classifying neutral reviews and non-reviews. 
<context><posfeature>Admittedly</posfeature>, the <posfeature>high accuracy achieved</posfeature> <kw>using such a</kw> simple set of features <posfeature>is somewhat surprising</posfeature>, <kw>although</kw> it is consistent with previous results on document-level subjectivity classification in which accuracies of 94-97% were obtained (<cite id="26" function="ack" polarity="pos">Yu and Hatzivassiloglou, 2003</cite>; <cite id="27" function="ack" polarity="pos">Wiebe et al., 2004</cite>).</context> 
Before concluding that review classification is an easy task, we conduct an additional experiment: we train a review identifier on a new dataset where we keep the same 2000 non-reviews but replace the positive/negative reviews with 2000 neutral reviews (i.e., reviews with a mediocre rating). 
Intuitively, a neutral review contains fewer terms with strong polarity than a positive/negative review. 
Hence, this additional experiment would allow us to investigate whether the lack of strong polarized terms in neutral reviews would increase the difficulty of the learning task. 
Our neutral reviews are randomly chosen from Pang et al.'s pool of 27886 unprocessed movie reviews that have either a rating of 2 (on a 4-point scale) or 2.5 (on a 5-point scale). 
Each review then undergoes a semi-automatic preprocessing stage where (1) HTML tags and any header and trailer information (such as date and author identity) are removed; (2) the document is tokenized and down-cased; (3) the rating information extracted by regular expressions is removed; and (4) the document is manually checked to ensure that the rating information is successfully removed. 
When trained on this new dataset, the review identifier also achieves an accuracy of 99.8%, suggesting that this learning task isn't any harder in comparison to the previous one. 
See http://www.imdb.com . 
Available from svmlight.joachims.org . 
We tried polynomial and RBF kernels, but none yields better performance than the default linear kernel. 
We observed that not performing length normalization hurts performance slightly. 
Also available from Pang's website. 
See Footnote 3. 
Discussion. 
We hypothesized that the high accuracies are attributable to the different vocabulary used in reviews and non-reviews. 
As part of our verification of this hypothesis, we plot the learning curve for each of the above experiments. 
We observe that a 99% accuracy was achieved in all cases even when only 200 training instances are used to acquire the review identifier. 
The ability to separate the two classes with such a small amount of training data seems to imply that features strongly indicative of one or both classes are present. 
To test this hypothesis, we examine the "informative" features for both classes. 
To get these informative features, we rank the features by their weighted log-likelihood ratio (WLLR): 
[Wt]C]) og p(wthCjy 
where wt and Cj denote the tth word in the vocabulary and the jth class, respectively. 
Informally, a feature (in our case a unigram) w will have a high rank with respect to a class c if it appears frequently in C and infrequently in other classes. 
This correlates reasonably well with what we think an informative feature should be. 
A closer examination of the feature lists sorted by WLLR confirms our hypothesis that each of the two classes has its own set of distinguishing features. 
Experiments with the book domain. 
To understand whether these good review identification results only hold true for the movie domain, we conduct similar experiments with book reviews and non-reviews. 
Specifically, we collect 1000 book reviews (consisting of a mixture of positive, negative, and neutral reviews) from the Barnes and Noble website, and 1000 non-reviews that are on the book topic (mostly book summaries) from Amazon. 
We then perform 10-fold CV experiments using these 2000 documents as before, achieving a high accuracy of 96.8%. 
These results seem to suggest that automatic review identification can be achieved with high accuracy. 
The curves are not shown due to space limitations. 
<context><cite id="28" function="use" polarity="pos">Nigam et al. (2000)</cite> <kw>show that</kw> this <method>metric</method> <posfeature>is effective at selecting good</posfeature> features <kw>for</kw> <task>text classification</task>.</context>
<context>Other commonly-used feature selection metrics are discussed in Yang and <cite id="29" function="ack" polarity="neu">Pedersen (1997)</cite>.</context> 
</paragraph> 
</section> 
 
<section imrad="r"> 
<title>4 Polarity Classification</title> 
<paragraph>Compared to review identification, polarity classification appears to be a much harder task. 
This section examines the role of various linguistic knowledge sources in our learning-based polarity classification system. 
</paragraph> 
<subsection> 
<subtitle>4.1 Experimental Setup</subtitle> 
<paragraph> 
<context><kw>Like</kw> <posfeature>several previous work</posfeature> (e.g., <cite id="30" function="bas" polarity="pos">Mullen and Collier (2004)</cite>, <cite id="31" function="bas" polarity="pos">Pang and Lee (2004)</cite>, <cite id="32" function="bas" polarity="pos">Whitelaw et al. (2005)</cite>), <author>we</author> <kw>view</kw> <task>polarity classification</task> <kw>as</kw> a supervised learning task.</context> 
As in review identification, we use SVM1ight with default parameter settings to train polarity classifiers, reporting all results as 10-fold CV accuracy. 
We evaluate our polarity classifiers on two movie review datasets, each of which consists of 1000 positive reviews and 1000 negative reviews. 
The first one, which we will refer to as Dataset A, is the Pang et al. polarity dataset (version 2.0). 
The second one (Dataset B) was created by us, with the sole purpose of providing additional experimental results. 
Reviews in Dataset B were randomly chosen from Pang et al.'s pool of 27886 unprocessed movie reviews (see Section 3) that have either a positive or a negative rating. 
We followed exactly Pang et al.'s guideline when determining whether a review is positive or negative. 
Also, we took care to ensure that reviews included in Dataset B do not appear in Dataset A. We applied to these reviews the same four pre-processing steps that we did to the neutral reviews in the previous section. 
</paragraph> 
</subsection> 
<subsection> 
<subtitle>4.2 Results</subtitle> 
<paragraph> 
The baseline classifier. 
We can now train our baseline polarity classifier on each of the two 
www.barnesandnoble.com www.amazon.com 
We also experimented with polynomial and RBF kernels when training polarity classifiers, but neither yields better results than linear kernels. 
The guidelines come with their polarity dataset. 
Briefly, a positive review has a rating of > 3.5 (out of 5) or > 3 (out of 4), whereas a negative review has a rating of  2 (out of 5) or  1.5 (out of 4). 
datasets. 
Our baseline classifier employs as features the k highest-ranking unigrams according to WLLR, with k/2 features selected from each class. 
Results with k = 10000 are shown in row 1 of Table 1. 
As we can see, the baseline achieves an accuracy of 87.1% and 82.7% on Datasets A and B, respectively. 
<context>Note that <author>our result on</author> Dataset A <posfeature>is as strong as</posfeature> that <kw>obtained by</kw> <cite id="33" function="bas" polarity="pos">Pang and Lee (2004)</cite> <kw>via</kw> their subjectivity summarization algorithm, which retains only the subjective portions of a document.</context> 
<context><posfeature>As a sanity check</posfeature>, <author>we</author> <kw>duplicated</kw> <cite id="34" function="bas" polarity="pos" >Pang et al.'s (2002)</cite> baseline in which all unigrams that appear four or more times in the training documents are used as features.</context> 
The resulting classifier achieves an accuracy of 87.2% and 82.7% for Datasets A and B, respectively. 
Neither of these results are significantly different from our baseline results. 
Adding higher-order n-grams. 
<context>The <result>negative results</result> that <cite id="35" function="ack" polarity="neg">Pang et al. (2002)</cite> obtained when using bigrams as features for their polarity classifier <kw>seem to suggest that</kw> high-order n-grams <negfeature>are not useful for</negfeature> polarity classification.</context> 
<context>However, <posfeature>recent research in</posfeature> the related (but arguably simpler) task of <task>text classification</task> <kw>shows that</kw> a <tool>bigram-based text classifier</tool> <posfeature>outperforms</posfeature> its unigram-based counterpart (<cite id="36" function="use" polarity="pos">Peng et al., 2003</cite>).</context> 
This prompts us to re-examine the utility of high-order n-grams in polarity classification. 
In our experiments we consider adding bigrams and trigrams to our baseline feature set. 
However, since these higher-order n-grams significantly outnumber the unigrams, adding all of them to the feature set will dramatically increase the dimensionality of the feature space and may undermine the impact of the unigrams in the resulting classifier. 
To avoid this potential problem, we keep the number of unigrams and higher-order n-grams equal. 
Specifically, we augment the baseline feature set (consisting of 10000 unigrams) with 5000 bigrams and 5000 trigrams. 
The bigrams and tri-grams are selected based on their WLLR computed over the positive reviews and negative reviews in the training set for each CV run. 
We experimented with several values of k and obtained the best result with k = 10000. 
We use two-tailed paired t-tests when performing significance testing, with p set to 0.05 unless otherwise stated. 
Results using this augmented feature set are shown in row 2 of Table 1. 
We see that accuracy rises significantly from 87.1% to 89.2% for Dataset A and from 82.7% to 84.7% for Dataset B. This provides evidence that polarity classification can indeed benefit from higher-order n-grams. 
Adding dependency relations. 
While bigrams and trigrams are good at capturing local dependencies, dependency relations can be used to capture non-local dependencies among the constituents of a sentence. 
Hence, we hypothesized that our n-gram-based polarity classifier would benefit from the addition of dependency-based features. 
<context><kw>Unlike most previous</kw> <paper>work on</paper> polarity classification, <kw>which has</kw> <kw>largely focused on</kw> exploiting adjective-noun (AN) relations (e.g., <cite id="37" function="ack" polarity="neu">Dave et al. (2003)</cite>, <cite id="38" function="ack" polarity="neu">Popescu and Etzioni (2005)</cite>), we hypothesized that subject-verb (SV) and verb-object (VO) relations <kw>would also be useful for</kw> the task.</context> 
The following (one-sentence) review illustrates why. 
While I really like the actors, the plot is rather uninteresting. 
A unigram-based polarity classifier could be confused by the simultaneous presence of the positive term like and the negative term uninteresting when classifying this review. 
However, incorporating the VO relation (like, actors) as a feature may allow the learner to learn that the author likes the actors and not necessarily the movie. 
<context><author>In our</author> <experiment>experiments</experiment>, the SV, VO and AN <feature>relations</feature> <kw>are extracted from</kw> <data>each document</data> <kw>by</kw> the MINIPAR dependency <tool>parser</tool> (<cite id="39" function="bas" polarity="pos">Lin, 1998</cite>).</context> 
As with n-grams, instead of using all the SV, VO and AN relations as features, we select among them the best 5000 according to their WLLR and retrain the polarity classifier with our n-gram-based feature set augmented by these 5000 dependency-based features. 
Results in row 3 of Table 1 are somewhat surprising: the addition of dependency-based features does not offer any improvements over the simple n-gram-based classifier. 
Table 1: Polarity classification accuracies. 
Incorporating manually tagged term polarity. 
Next, we consider incorporating a set of features that are computed based on the polarity of adjectives. 
As noted before, we desire a high-precision, high-coverage lexicon. 
So, instead of exploiting a learned lexicon, we manually develop one. 
To construct the lexicon, we take Pang et al.'s pool of unprocessed documents (see Section 3), remove those that appear in either Dataset A or Dataset B, and compile a list of adjectives from the remaining documents. 
Then, based on heuristics proposed in psycholinguistics, we hand-annotate each adjective with its prior polarity (i.e., polarity in the absence of context). 
Out of the 45592 adjectives we collected, 3599 were labeled as positive, 3204 as negative, and 38789 as neutral. 
A closer look at these adjectives reveals that they are by no means domain-dependent despite the fact that they were taken from movie reviews. 
Now let us consider a simple procedure P for deriving a feature set that incorporates information from our lexicon: (1) collect all the bigrams from the training set; (2) for each bigram that contains at least one adjective labeled as positive or negative according to our lexicon, create a new feature that is identical to the bigram except that each adjective is replaced with its polarity label; (3) merge the list of newly generated features with the list of bigrams and select the top 5000 features from the merged list according to their WLLR. 
We then repeat procedure P for the trigrams and also the dependency features, resulting in a total of 15000 features. 
Our new feature set comprises these 15000 features as well as the 10000 unigrams we used in the previous experiments. 
Results of the polarity classifier that incorporates term polarity information are encouraging (see row 4 of Table 1). 
In comparison to the classifier that uses only n-grams and dependency-based features (row 3), accuracy increases significantly (p = . 
1) from 89.2% to 90.4% for Dataset A, and from 84.7% to 86.2% for Dataset B. These results suggest that the classifier has benefited from the use of features that are less sparse than n-grams. 
We treat the test documents as unseen data that should not be accessed for any purpose during system development. 
http://www.sci.sdsu.edu/CAL/wordlist 
Neutral adjectives are not replaced. 
<context>A newly generated feature <negfeature>could be misleading for</negfeature> the learner if the contextual polarity (i.e., polarity in the presence of context) of the adjective involved <kw>differs from its prior</kw> polarity (see <cite id="40" function="ack" polarity="neg">Wilson et al. (2005)</cite>).</context> 
The motivation behind merging with the bigrams is to create a feature set that is more robust in the face of potentially misleading generalizations. 
Using objective information. 
Some of the 25000 features we generated above correspond to n-grams or dependency relations that do not contain subjective information. 
We hypothesized that not employing these "objective" features in the feature set would improve system performance. 
More specifically, our goal is to use procedure P again to generate 25000 "subjective" features by ensuring that the objective ones are not selected for incorporation into our feature set. 
To achieve this goal, we first use the following rote-learning procedure to identify objective material: (1) extract all unigrams that appear in objective documents, which in our case are the 2000 non-reviews used in review identification [see Section 3]; (2) from these "objective" unigrams, we take the best 20000 according to their WLLR computed over the non-reviews and the reviews in the training set for each CV run; (3) repeat steps 1 and 2 separately for bigrams, trigrams and dependency relations; (4) merge these four lists to create our 80000-element list of objective material. 
Now, we can employ procedure P to get a list of 25000 "subjective" features by ensuring that those that appear in our 80000-element list are not selected for incorporation into our feature set. 
Results of our classifier trained using these subjective features are shown in row 5 of Table 1. 
Somewhat surprisingly, in comparison to row 4, we see that our method for filtering objective features does not help improve performance on the two datasets. 
We will examine the reasons in the following subsection. 
</paragraph> 
</subsection> 
<subsection> 
<subtitle>4.3 Discussion and Further Analysis</subtitle> 
<paragraph> 
Using the four types of knowledge sources previously described, our polarity classifier significantly outperforms a unigram-based baseline classifier. 
In this subsection, we analyze some of these results and conduct additional experiments in an attempt to gain further insight into the polarity classification task. 
Due to space limitations, we will simply present results on Dataset A below, and show results on Dataset B only in cases where a different trend is observed. 
The role of feature selection. 
In all of our experiments we used the best k features obtained via WLLR. 
An interesting question is: how will these results change if we do not perform feature selection? 
To investigate this question, we conduct two experiments. 
First, we train a polarity classifier using all unigrams from the training set. 
Second, we train another polarity classifier using all unigrams, bigrams, and trigrams. 
We obtain an accuracy of 87.2% and 79.5% for the first and second experiments, respectively. 
In comparison to our baseline classifier, which achieves an accuracy of 87.1%, we can see that using all unigrams does not hurt performance, but performance drops abruptly with the addition of all bigrams and trigrams. 
These results suggest that feature selection is critical when bigrams and trigrams are used in conjunction with unigrams for training a polarity classifier. 
The role of bigrams and trigrams. 
So far we have seen that training a polarity classifier using only unigrams gives us reasonably good, though not outstanding, results. 
Our question, then, is: would bigrams alone do a better job at capturing the sentiment of a document than unigrams? 
To answer this question, we train a classifier using all bigrams (without feature selection) and obtain an accuracy of 83.6%, which is significantly worse than that of a unigram-only classifier. 
<context><kw>Similar</kw> <result>results</result> <kw>were also obtained by</kw> <cite id="41" function="ack" polarity="neu">Pang et al. (2002)</cite>.</context> 
It is possible that the worse result is due to the presence of a large number of irrelevant bigrams. 
To test this hypothesis, we repeat the above experiment except that we only use the best 10000 bi-grams selected according to WLLR. 
Interestingly, the resulting classifier gives us a lower accuracy of 82.3%, suggesting that the poor accuracy is not due to the presence of irrelevant bigrams. 
To understand why using bigrams alone does not yield a good classification model, we examine a number of test documents and find that the feature vectors corresponding to some of these documents (particularly the short ones) have all zeroes in them. 
In other words, none ofthe bigrams from the training set appears in these reviews. 
This suggests that the main problem with the bigram model is likely to be data sparseness. 
Additional experiments show that the trigram-only classifier yields even worse results than the bigram-only classifier, probably because of the same reason. 
Nevertheless, these higher-order n-grams play a non-trivial role in polarity classification: we have shown that the addition of bigrams and trigrams selected via WLLR to a unigram-based classifier significantly improves its performance. 
The role of dependency relations. 
In the previous subsection we see that dependency relations do not contribute to overall performance on top of bigrams and trigrams. 
There are two plausible reasons. 
First, dependency relations are simply not useful for polarity classification. 
Second, the higher-order n-grams and the dependency-based features capture essentially the same information and so using either of them would be sufficient. 
To test the first hypothesis, we train a classifier using only 10000 unigrams and 10000 dependency-based features (both selected according to WLLR). 
For Dataset A, the classifier achieves an accuracy of 87.1%, which is statistically indistinguishable from our baseline result. 
On the other hand, the accuracy for Dataset B is 83.5%, which is significantly better than the corresponding baseline (82.7%) at the p = . 
1 level. 
These results indicate that dependency information is somewhat useful for the task when bigrams and trigrams are not used. 
So the first hypothesis is not entirely true. 
So, it seems to be the case that the dependency relations do not provide useful knowledge for polarity classification only in the presence ofbigrams and trigrams. 
This is somewhat surprising, since these n-grams do not capture the non-local dependencies (such as those that may be present in certain SV or VO relations) that should intuitively be useful for polarity classification. 
To better understand this issue, we again examine a number of test documents. 
Our initial investigation suggests that the problem might have stemmed from the fact that MINIPAR returns dependency relations in which all the verb inflections are removed. 
For instance, given the sentence My cousin Paul really likes this long movie, MINIPAR will return the VO relation (like, movie). 
To see why this can be a problem, consider another sentence I like this long movie. 
From this sentence, MINIPAR will also extract the VO relation (like, movie). 
Hence, this same VO relation is capturing two different situations, one in which the author himself likes the movie, and in the other, the author's cousin likes the movie. 
The over-generalization resulting from these "stemmed" relations renders dependency information not useful for polarity classification. 
Additional experiments are needed to determine the role of dependency relations when stemming in MINIPAR is disabled. 
The role of objective information. 
Results from the previous subsection suggest that our method for extracting objective materials and removing them from the reviews is not effective in terms of improving performance. 
To determine the reason, we examine the n-grams and the dependency relations that are extracted from the nonreviews. 
We find that only in a few cases do these extracted objective materials appear in our set of 25000 features obtained in Section 4.2. 
This explains why our method is not as effective as we originally thought. 
<context><author>We</author> <kw>conjecture that</kw> more sophisticated methods <kw>would be needed in order to</kw> take advantage of objective information in polarity classification (e.g., Koppel and <cite id="42" function="bas" polarity="pos">Schler (2005)</cite>)</context>. 
</paragraph> 
</subsection> 
</section> 
 
<section imrad="d"> 
<title>5 Conclusions</title> 
<paragraph> 
We have examined two problems in document-level sentiment analysis, namely, review identification and polarity classification. 
We first found that review identification can be achieved with very high accuracies (97-99%) simply by training an SVM classifier using unigrams as features. 
We then examined the role of several linguistic knowledge sources in polarity classification. 
Our results suggested that bigrams and trigrams selected according to the weighted log-likelihood ratio as well as manually tagged term polarity information are very useful features for the task. 
On the other hand, no further performance gains are obtained by incorporating dependency-based information or filtering objective materials from the reviews using our proposed method. 
Nevertheless, the resulting polarity classifier compares favorably to state-of-the-art sentiment classification systems. 
</paragraph> 
</section> 
 
</paper></annotatedpaper>