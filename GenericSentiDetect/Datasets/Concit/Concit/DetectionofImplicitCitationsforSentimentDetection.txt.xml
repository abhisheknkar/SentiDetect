<annotatedpaper><paper title="Detection of Implicit Citations for Sentiment Detection" authors="Awais Athar, Simone Teufel" year="2012"> 
<section> 
<title>Detection of Implicit Citations for Sentiment Detection</title> 
Awais Athar Simone Teufel 
Computer Laboratory, University of Cambridge 
15 JJ Thomson Avenue, Cambridge CB3 0FD, UK 
{awais.athar,simone.teufel}@cl.cam.ac.uk 
</section> 
<section> 
<title>Abstract</title> 
<paragraph> 
Sentiment analysis of citations in scientific papers is a new and interesting problem which can open up many exciting new applications in bibliometrics. 
Current research assumes that using just the citation sentence is enough for detecting sentiment. 
In this paper, we show that this approach misses much of the existing sentiment. 
We present a new corpus in which all mentions of a cited paper have been annotated. 
We explore methods to automatically identify these mentions and show that the inclusion of implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment. 
</paragraph> 
</section> 
<section imrad="i"> 
<title>1 Introduction</title> 
<paragraph> 
The idea of using citations as a source of information has been explored extensively in the field of biblio-metrics, and more recently in the field of computational linguistics. 
<context><posfeature>State-of-the-art</posfeature> citations identification <tool>mechanisms</tool> <kw>focus either on</kw> detecting explicit citations i.e. those that consist of either the author names and the year of publication or bracketed numbers only, or include a small sentence window around the explicit citation as input text (<cite id="1" function="use" polarity="pos">Coun-cill et al., 2008</cite>; <cite id="2" function="use" polarity="pos">Radev et al., 2009</cite>; <cite id="3" function="use" polarity="pos">Ritchie et al., 2008</cite>).</context> 
The assumption behind this approach is that all related mentions of the paper would be concentrated in the immediate vicinity of the anchor text. 
However,<context> <kw>this</kw> assumption <negfeature>does not generally hold true</negfeature> (<cite id="4" function="ack" polarity="neg">Teufel, 2010</cite>; <cite id="5" function="ack" polarity="neg">Sugiyama et al., 2010</cite>).</context> 
<context>The phenomenon of trying to <task>determine a citations's  context</task> 
    <posfeature>has a long tradition in</posfeature> library sciences (<cite id="6" function="use" polarity="pos">O'Connor, 1982</cite>), and its connection with coreference <posfeature>has been duely noted</posfeature> (<cite id="7" function="use" polarity="pos">Kim et al., 2006</cite>; <cite id="8" function="use" polarity="pos">Kaplan et al., 2009</cite>).</context>
Consider Figure 1, which illustrates a typical case. 
Figure 1: Example of the use of anaphora 
</paragraph> 
<paragraph> 
While the first sentence cites the target paper explicitly using the name of the primary author along with the year of publication of the paper, the remaining sentences mentioning the same paper appear after a gap and contain an indirect and implicit reference to that paper. 
These mentions occur two sentences after the formal citation in the form of anaphoric it and the lexical hook METEOR. 
<context>Most current with the exception of <cite id="9" function="ack" polarity="pos">Qazvinian and Radev (2010)</cite>, are not <posfeature>able to detect</posfeature> linguistic mentions of citations in such forms.</context> 
Ignoring such mentions and examining only the sentences containgrams. 
<context>
    <kw>In order to improve</kw> 
    <result>sentence-level evaluation performance</result>, <kw>several</kw> metrics <kw>have been proposed</kw>, including ROUGE-W, ROUGE-S (<cite id="10" function="use" polarity="neu">Lin and Och. 2004</cite>) and METEOR (<cite id="11" function="use" polarity="neu">Banerjee and Lavie, 2005</cite>).</context> 
ROUGE-W differs from BLEU and NIST 
METEOR is essentially a unigram based metric, which prefers the monotonie word alignment between MT output and the references by penalizing crossing word alignments. 
There are two problems with METEOR. 
First, it doesn't consider gaps in the aligned words, which is an important feature for evaluating the sentence fluency; second, it cannot use multiple references simultaneously. 
ROUGE and METEOR both use WordNet ing an explicit citation results in loss of information about the cited paper. 
<context>While this <result>phenomenon</result> <negfeature>is problematic for</negfeature> applications like scientific summarisation (<cite id="12" function="wea" polarity="neg">Abu-Jbara and Radev, 2011</cite>)</context>, <context>it <posfeature>has a particular relevance for</posfeature> <task>citation sentiment detection</task> (<cite id="13" function="ack" polarity="pos">Athar, 2011</cite>).</context> 
</paragraph> 
<paragraph> 
Citation sentiment detection is an attractive task. 
Availability of citation polarity information can help researchers in understanding the evolution of a field on the basis of research papers and their critiques. 
<context>It <posfeature>can also help</posfeature> expert researchers who are in the process of preparing opinion based summaries <kw>for</kw> survey papers <kw>by providing</kw> them with motivations behind as well as positive and negative comments about different <method>approaches</method> (<cite id="14" function="use" polarity="pos">Qazvinian and Radev, 2008</cite>).</context> 
</paragraph> 
<paragraph> 
<context>
<posfeature>Current work on</posfeature> citation sentiment detection <kw>works under the assumption</kw> that the sentiment present in the citation sentence <posfeature>represents the true</posfeature> sentiment of the author towards the cited paper (<cite id="15" function="hed" polarity="neg">Athar, 2011</cite>; <cite id="16" function="hed" polarity="neg">Piao et al., 2007</cite>; <cite id="17" function="hed" polarity="neg">Pham and Hoffmann, 2004</cite>). 
This assumption is so dominant because current citation identification methods (Councill et can readily identify the citation sentence, whereas it is much harder to determine the relevant context. 
<kw>However</kw>, this assumption most certainly <negfeature>does not hold true</negfeature> when the citation context spans more than one sentence.</context> 
</paragraph> 
<paragraph> 
Concerning the sentiment aspect of the citation context from Figure 1, we see that the citation sentence does not contain any sentiment towards the cited paper, whereas the following sentences act as a critique and list its shortcomings. 
It is clear that criticism is the intended sentiment, but if the gold standard is defined by looking at the citation sentence in isolation, a significant amount of sentiment expressed in the text is lost. 
<context>Given that overall most citations in a text are neutral with respect to sentiment (<cite id="18" function="ack" polarity="neu">Spiegel-Rosing, 1977</cite>; <cite id="19" function="ack" polarity="neu">Teufel et al., 2006</cite>), this makes it even more important to recover what explicit sentiment there is in the article, wherever it is to be found.</context> 
</paragraph> 
<paragraph> 
In this paper, we examine methods to extract all opinionated sentences from research papers which mention a given paper in as many forms as we can identify, not just as explicit citations. 
We present a new corpus in which all mentions of a cited paper have been manually annotated, and show that our annotation treatment increases citation sentiment coverage, particularly for negative sentiment. 
We then explore methods to automatically identify all mentions of a paper in a supervised manner. 
In particular, we consider the recognition of named approaches and acronyms. 
Our overall system then classifies explicit and implicit mentions according to sentiment. 
The results support the claim that including implicit citations in citation sentiment analysis improves the quality of the overall sentiment assignment. 
</paragraph> 
</section> 
<section imrad="m"> 
<title>2 Corpus Construction</title> 
<paragraph> 
<context><author>We</author> <kw>use</kw> the dataset <kw>from</kw> <cite id="20" function="bas" polarity="neu">Athar (2011)</cite> <kw>as our</kw> <kw>starting point</kw>, which consists of 8,736 citations in the ACL Anthology (<cite id="21" function="bas" polarity="neu">Bird et al., 2008</cite>) that cite a target set of 310 <data>ACL Anthology papers</data>. 
The citation summary data from the ACL Anthology Network (<cite id="22" function="bas" polarity="neu">Radev et al., 2009</cite>) is used.</context>
This dataset is rather large, and since manual annotation of context for each citation is a time consuming task, a subset of 20 target papers (i.e., all citations to these) has been selected for annotation. 
These 20 papers correspond to approximately 20% of incoming citations in the original dataset. 
They contain a total of 1,555 citations from 854 citing papers. 
</paragraph> 
<subsection> 
<title>2.1 Annotation</title> 
<paragraph> 
We use a four-class scheme for annotation. 
Every sentence which does not contain any direct or indirect mention of the citation is labelled as being excluded (x) from the context. 
The rest of the sentences are marked either positive (p), negative (n) or objective/neutral (o). 
To speed up the annotation process, we developed a customised annotation tool. 
</paragraph> 
<paragraph> 
A total of 203,803 sentences have been annotated from 1,034 paper-reference pairs. 
<context>Although this annotation been performed by the first author only, <author>we</author> <kw>know from</kw> previous work that similar styles of annotation <posfeature>can achieve acceptable</posfeature> inter-annotator agreement (<cite id="23" function="bas" polarity="pos">Teufel et al., 2006</cite>).</context> 
An example annotation is given in Figure 2, <context>where the first column shows the line number and the second one shows the class label for the citation to <cite id="24" function="ack" polarity="neu">Smadja (1993)</cite>.</context> 
It should be noted that since annotation is always performed for a specific citation only, sentences such as the one at line 32, which carry sentiment but refer to a different citation, are marked as excluded from the context. 
http://www.aclweb.org 
</paragraph> 
<paragraph> 
If there are multiple sentiments in the same sentence, the sentence has been labelled with the class of the last sentiment mentioned. 
In this way, a total of 3,760 citation sentences have been found in the whole corpus, i.e. sentences belonging to class o, n or p, and the rest have been labelled as x. 
Table 1 compares the number of sentences with only the explicit citations with all explicit and implicit mentions of those citations. 
We can see that including the citation context increases the subjective sentiment by almost 185%. 
The resulting negative sentiment also increases by more than 325%. 
<context>This may be attributed to the <kw>strategic behaviour of</kw> the authors of 'sweetening' the criticism <kw>in order to</kw> soften its effects among their peers (<cite id="25" function="use" polarity="neu">Hornsey et al., 2008</cite>).</context> 
Figure 2: Example annotation of a citation context. 
</paragraph> 
<paragraph> 
Another view of the annotated data is available in Figure 3a. 
This is in the form of interactive HTML where each HTML page represents all the incoming links to a paper. 
Each row represents the citing paper and each column square represents a sentence. 
The rows are sorted by increasing publication date. 
Black squares are citations with the author name and year of publication mentioned in the text. 
The red, green and gray squares show negative, positive and neutral sentiment respectively. 
Pointing the mouse cursor at any square gives the text content of the corresponding sentence, as shown in the Figure 3a. 
</paragraph> 
<paragraph> 
The ACL Id, paper title and authors' names are also given at the top of the page. 
Similar data for the corresponding citing paper is made available when the mouse cursor is positioned on one of the orange squares at the start of each row, as shown in the Figure 3b. 
Clicking on the checkboxes at the top hides or shows the corresponding type of squares. 
There is also an option to hide/show a grid so that the squares are separated and rows are easier to trace. 
For example, Figure 3b shows the grid with the neutral or objective citations hidden. 
</paragraph> 
<paragraph> 
In the next section, we describe the features set we use to detect implicit citations from this annotated corpus and discuss the results. 
</paragraph> 
</subsection> 
</section> 
<section imrad="r"> 
<title>3 Experiments and Results</title>
<paragraph> 
For the task of detecting all mentions of a citation, we merge the class labels of sentences mentioning a citation in any form (o_n_p). 
To make sure that the easily detectable explicit citations do not influence the results, we change the class label of all those sentences to x which contain the first author's name within a 4-word window of the year of publication. 
</paragraph> 
<paragraph> 
Our dataset is skewed as there are many more objective sentences than subjective ones. 
In such scenarios, average micro-F scores tend to be slightly higher as they are a weighted measure. 
To avoid this bias, we also report the macro-F scores. 
<context><kw>Furthermore</kw>, <kw>to ensure</kw> there is enough data <kw>for</kw> training each class, <author>we</author> <kw>use</kw> 10-fold cross-validation (<cite id="26" function="bas" polarity="pos">Lewis, 1991</cite>) in all our <experiment>experiments</experiment>.</context> 
</paragraph> 
<paragraph> 
<context><author>We</author> <kw>represent</kw> each citation <kw>as</kw> a <feature>feature</feature> set in a Support Vector Machine (SVM) (<cite id="27" function="bas" polarity="pos">Cortes and Vapnik, 1995</cite>) framework.</context> 
<context>The <data>corpus</data> <kw>is processed using</kw> WEKA (<cite id="28" function="use" polarity="pos">Hall et al., 2008</cite>) and the Weka LibSVM library (<cite id="29" function="use" polarity="pos">EL-Manzalawy and Honavar, 2005</cite>; <cite id="30" function="use" polarity="pos">Chang and Lin, 2001</cite>).</context> 
For each ith sentence Si, we use the following binary features. 
</paragraph> 
<paragraph> 
• Si-i contains the last name of the primary author, followed by the year of publication within a four-word window. 
Table 1: Distribution of classes. 
</paragraph> 
<paragraph> 
• Si contains the last name of the primary author followed by the year of publication within a four-word window. 
(a) Sentence Text (b) Paper metadata 
Figure 3: Different views of an annotated paper. 
This feature is meant to capture the fact that the sentence immediately after an explicit citation is more likely to continue talking about the same work. 
This feature should help in identifying sentences containing explicit citations. 
Since such sentences are easier to identify, including them in the evaluation metric would result in a false boost in the final score. 
We have thus excluded all such sentences in our annotation and this feature should indicate a negative instance to the classifier. 
</paragraph> 
<paragraph> 
• Si contains the last name ofthe primary author. 
This feature captures sentences which contain a reference to tools and algorithms which have been named after their inventors, such as, "One possible direction for future work is to compare the search-based approach of Collins and Roark with our DP-based approach." 
It should also capture the mentions of methods and techniques used in the cited paper e.g., "We show that our approach outperforms Tur-ney's approach." 
</paragraph> 
<paragraph> 
• Si contains an acronym used in an explicit citation. 
Acronyms are taken to be capitalised words which are extracted from the vicinity of the cited author's last name using regular expressions. 
For example, METEOR in Figure 1 is an acronym which is used in place of a formal citation to refer to the original paper in the rest of the citing paper. 
</paragraph> 
<paragraph> 
• Si contains a determiner followed by a work noun. 
We use the following determiners D = {the, this, that, those, these, his, her, their, such, previous, other}. 
<context>The list of work nouns (technique, method, etc.) has been taken from <cite id="31" function="ack" polarity="neu">Teufel (2010)</cite>.</context> 
<context>This <feature>feature</feature> <kw>extracts</kw> a pattern <kw>which has been found to</kw> be <posfeature>useful</posfeature> <kw>for</kw> extracting citations in previous work (<cite id="32" function="use" polarity="pos">Qazvinian and Radev, 2010</cite>).</context> 
Such phrases usually signal a continuation of the topics related to citations in earlier sentences. 
For example: 
<context>"<cite id="33" function="wea" polarity="neg">Church et al.(1989)</cite>, <cite id="34" function="wea" polarity="neg">Wettler &amp; Rapp (1989)</cite> and <cite id="35" function="wea" polarity="neg">Church &amp; Hanks (1990)</cite> describe algorithms which do this. 
<kw>However</kw>, the validity of these algorithms <negfeature>has not been tested by</negfeature> systematic comparisons with associations ofhuman subjects."</context> 
</paragraph> 
<paragraph> 
• Si starts with a third person pronoun. 
The feature also tries to capture the topic continuation after a citation. 
Sentences starting with a pronoun (e.g. they, their, he, she, etc.) are more likely to describe the subject citation of the previous sentence in detail. 
For example: 
"<context>Because <cite id="36" function="wea" polarity="neg">Daume III (2007)</cite> views the adaptation <negfeature>as merely augmenting</negfeature> the feature space, each of his features has the same prior mean and variance, <negfeature>regardless of</negfeature> whether it is domain specific or independent.</context>
He could have set these parameters differently, but he did not." 
</paragraph> 
<paragraph> 
• Si starts with a connector. 
This feature also focuses on detecting the topic continuity. 
<context>Connectors <kw>have been shown to be</kw> <posfeature>effective</posfeature> <kw>in</kw> other context related <paper>works</paper> <kw>as well</kw> (<cite id="37" function="use" polarity="pos">Hatzivassiloglou and McKeown, 1997</cite>; <cite id="38" function="use" polarity="pos">Polanyi and Zaenen, 2006</cite>).</context> 

A list of 23 connectors (e.g. however, although, moreover, etc.) has been compiled by examining the high frequency connectors from a separate set ofpapers from the same domain. 
An example is: 
"<context>An additional consistent edge of a linear-chain conditional random field (CRF) explicitly models the dependencies between distant occurrences of similar words (<cite id="39" function="ack" polarity="neu">Sutton and McCal-lum, 2004</cite>; <cite id="40" function="ack" polarity="neu">Finkel et al. , 2005</cite>)</context>. 
However, this approach requires additional time complexity in inference/learning time and it is only suitable for representing constraints by enforcing label consistency." 
</paragraph> 
<paragraph> 
• Si starts with a (sub)section heading. 
</paragraph> 
<paragraph> 
• Si-i starts with a (sub)section heading. 
</paragraph> 
<paragraph> 
• Si+1 starts with a (sub)section heading. 
The three features above are a consequence of missing information about the paragraph and section boundaries in the used corpus. 
Since the text extraction has been done automatically, the section headings are usually found to be merged with the text of the succeeding sentence. 
For example, the text below merges the heading of section 4.2 with the next sentence. 
"4.2 METEOR vs. SIA SIA is designed to take the advantage ofloose sequence-based metrics without losing word-level information." 
Start and end of such section boundaries can give us important information about the scope of a citation. 
In order to exploit this information, we use regular expressions to detect if the sentences under review contains these merged section titles and headings. 
</paragraph> 
<paragraph> 
• Si contains a citation other than the one under review. 
It is more probable for the context of a citation to end when other citations are mentioned in a sentence, which is the motivation behind using this feature, which might contribute to the discriminating power of the classifier in conjunction with the presence of a citation in the previous sentence. 
For example, in the extract below, the scope of the first citation is limited to the first sentence only. 
"<context><cite id="41" function="use" polarity="neu">Blitzer et al.(2006)</cite> <kw>proposed</kw> a structural correspondence learning <method>method</method> <kw>for</kw> domain adaptation and <kw>applied it to</kw> part-of-speech tagging.</context><context> <cite id="42" function="use" polarity="neu">Daume III (2007)</cite> <kw>proposed</kw> a simple feature augmentation <method>method</method> <kw>to achieve</kw> domain adaptation.</context>" 
</paragraph> 
<paragraph> 
• Si contains a lexical hook. 
The lexical hooks feature identifies lexical substitutes for the citations. 
We obtain these hooks by examining all explicit citation sentences to the cited paper and selecting the most frequent capitalized phrase in the vicinity of the author's last name. 
The explicit citations come from all citing papers and not just the paper for which the features are being determined. 
<context>For example, the sentences below have been taken from two different papers and cite the same target paper (<cite id="43" function="ack" polarity="neu">Cutting et al., 1992</cite>).</context> 
While the acronym HMM will be captured by the feature stated earlier, the word Xerox will be missed. 
E95-1014: "This text was part-of-speech tagged using the Xerox HMM tagger (Cutting J97-3003: "The Xerox tagger (Cutting et al. 1992) comes with a set ofrules that assign an unknown word a set ofpossible pos-tags (i.e. , POS-class) on the basis ofits ending segment." 
This 'domain level' feature makes it possible to extract the commonly used name for a technique which may have been missed by the acronym feature due to long term dependencies. 
We also extrapolate the acronym for such phrases, e.g., in the example below, SCL would also be checked along with Structural Correspondence Learning. 
"<context>The paper <kw>compares</kw> Structural Correspondence Learning (<cite id="44" function="con" polarity="neu">Blitzer et al., 2006</cite>) with (various instances of) self-training (<cite id="45" function="con" polarity="neu">Abney, 2007</cite>; <cite id="46" function="con" polarity="neu">McClosky et al., 2006</cite>) <kw>for</kw> the adaptation of a parse selection model to Wikipedia domains</context>" 
We also add n-grams of length 1 to 3 to this lexical feature set and compare the results obtained with an n-gram only baseline in Table 2. 
<context>
    <concept>N-grams</concept> 
    <kw>have been shown to</kw> 
    <kw>perform</kw> 
    <posfeature>consistently well in</posfeature> 
    <task>various NLP tasks</task> (<cite id="47" function="use" polarity="pos">Bergsma et al., 2010</cite>).</context> 
</paragraph> 
<paragraph> 
By adding the new features listed above, the performance of our system increases by almost 8% over the n-gram baseline for the task of detecting citation mentions. 
Using the pairwise Wilcoxon rank-sum test at 0.05 significance level, we found that the difference between the baseline and our system is statistically significant. 
While the micro-F score obtained is quite high, the individual class scores show that the task is hard and a better solution may require a deeper analysis of the context. 
</paragraph> 
</section> 
<section imrad="r"> 
<title>4 Impact on Citation Sentiment Detection</title> 
<paragraph> 
We explore the effect of this context on citation sentiment detection. 
<context>For a baseline, <author>we</author> <kw>use</kw> <feature>features of</feature> the state-of-the-art system proposed in <kw>our</kw> earlier <paper>work</paper> (<cite id="48" function="bas" polarity="pos">Athar, 2011</cite>).</context> 
While there we used n-gram and dependency feature on sentences containing explicit citations only, our annotation is not restricted to such citations and we may have more than one sentiment per each explicit citation. 
For example, in Figure 2, our 2011 system will be restricted to analysing sentence 33 only. 
However, it is clear from our annotation that there is more sentiment present in the succeeding sentences which belongs to this explicit citation. 
While sentence 34 in Figure 2 is positive towards the cited paper, the next sentence criticises it. 
Thus for this explicit citation, there are three sentences with sentiment and all of them are related to the same explicit citation. 
Treating these sentences separately will result in an artificial increase in the amount of data because they participate in the same discourse. 
It would also make it impossible to compare the sentiment annotated in the previous work with our annotation. 
While this test may not be adequate as the data is highly skewed, we are reporting the results since there is no obvious alternative for discrete skewed data. 
In future, we plan to use the continuous probability estimates produced by the classifier for testing significance. 
</paragraph> 
<paragraph> 
<context>To make sure the annotations are comparable, <author>we</author> <posfeature>mark the true</posfeature> citation sentiment to be the last sentiment mentioned in a 4-sentence context window, as this <posfeature>is pragmatically most likely to</posfeature> be <posfeature>the real intention</posfeature> (<cite id="49" function="bas" polarity="pos">MacRoberts and MacRoberts, 1984</cite>).</context> 
<context>The <concept>window length</concept> <kw>is motivated by</kw> <posfeature>recent research</posfeature> (<cite id="50" function="use" polarity="pos">Qazvinian and Radev, 2010</cite>) <posfeature>which favours</posfeature> a four-sentence boundary <kw>for</kw> detecting non-explicit citations.</context> 
Analysis of our data shows that more than 60% of the subjective citations lie in this window. 
We include the implicit citations predicted by the method described in the previous section in the context. 
</paragraph> 
<paragraph> 
The results of the single-sentence baseline system are compared with this context enhanced system in Table 3. 
Table 3: F-scores for citation sentiment detection. 
The results show that our system outperforms the baseline in all evaluation criteria. 
Performing the pairwise Wilcoxon rank-sum testat 0.05 significance level, we found that the improvement is statistically significant. 
The baseline system does not use any context and thus misses out on all the sentiment information contained within. 
While this window-based representation does not capture all the sentiment towards a citation perfectly, it is closer to the truth than a system based on single sentence analysis and is able to detect more sentiment. 
Table 2: Comparison of F-scores for non-explicit citation detection. 
</paragraph> 
</section> 
<section imrad="d"> 
<title>5 Related Work</title> 
<paragraph> 
    <context>While different <method>schemes</method> 
        <kw>have been proposed for</kw> 
        <task>annotating citations according to</task> their function (<cite id="51" function="con" polarity="neg">Spiegel-Rosing, 1977</cite>; <cite id="52" function="ack" polarity="neu">Nanba and Okumura, 1999</cite>; <cite id="53" function="ack" polarity="neu">Garzone and Mercer, 2000</cite>)</context>,<context> <posfeature>the only recent work on</posfeature> citation sentiment detection using a relatively large corpus is by <cite id="54" function="hed" polarity="neg">Athar (2011)</cite>.
        <kw>However</kw>, this work <negfeature>does not handle citation context.</negfeature> 
        </context> 
<context>
    <kw>Other approaches to</kw> 
    <task>citation classification</task> 
    include work by <cite id="55" function="use" polarity="neu">Wilbur et al. (2006)</cite>, who annotated a <data>101 sentence corpus on focus</data>, polarity, certainty, evidence and directionality.</context> 
<context><cite id="56" function="use" polarity="neu">Piao et al. (2007)</cite> <kw>proposed</kw> a <tool>system</tool> <kw>to attach</kw> sentiment information to the citation links between biomedical papers <kw>by using</kw> existing semantic lexical resources and NLP <tool>tools</tool>.</context> 
</paragraph> 
<paragraph> 
<context><posfeature>A common approach</posfeature> for sentiment detection <kw>is to use</kw> a labelled lexicon to score sentences (Hatzivas-siloglou and <cite id="57" function="hed" polarity="neg">McKeown, 1997</cite>; <cite id="58" function="hed" polarity="neg">Turney, 2002</cite>; <cite id="59" function="hed" polarity="neg">Yu and Hatzivassiloglou, 2003</cite>).
<kw>However</kw>, such <method>approaches</method> <kw>have been found to</kw> be  <negfeature>highly topic dependent</negfeature> (<cite id="60" function="hed" polarity="neg">Engstrom, 2004</cite>; <cite id="61" function="hed" polarity="neg">Gamon and Aue, 2005</cite>; <cite id="62" function="hed" polarity="neg">Blitzer et al., 2007</cite>), <kw>which makes</kw> the creation of a general sentiment classifier <negfeature>a difficult task</negfeature>.</context> 
</paragraph> 
<paragraph> 
<context><cite id="63" function="wea" polarity="neg">Teufel et al. (2006)</cite> worked on a 2,829 sentence citation corpus using a 12-class classification scheme. 
<kw>While</kw> the <person>authors</person> did <kw>make use of</kw> the context in their annotation, their focus was on the task of determining the author's reason for citing a given paper. 
This task <negfeature>differs from</negfeature> citation sentiment detection, <kw>which is in a sense a</kw> "lower level" of analysis.</context> 
</paragraph> 
<paragraph> 
<context>Some other recent work has focused on the problem of implicit citation extraction (<cite id="64" function="wea" polarity="neg">Kaplan et al., 2009</cite>; <cite id="65" function="wea" polarity="neg">Qazvinian and Radev, 2010</cite>). 
<cite id="66" function="wea" polarity="neg">Kaplan et al. (2009)</cite> explore co-reference chains for citation extraction using a combination of co-reference resolution techniques (<cite id="67" function="wea" polarity="neg">Soon et al., 2001</cite>; <cite id="68" function="wea" polarity="neg">Ng and Cardie, 2002</cite>). 
<kw>However</kw>, the <data>corpus</data> that they use <negfeature>consists of only</negfeature> 94 citations to 4 papers and is likely to be too small to be representative.</context> 
</paragraph> 
<paragraph> 
<context>For citation extraction, the <posfeature>most relevant work is by </posfeature> <cite id="69" function="hed" polarity="neg">Qazvinian and Radev (2010)</cite> who proposed a framework of Markov Random Fields to extract only the non-explicit citations <kw>for</kw> a given paper. 
They model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes weighted by lexical similarity between nodes. 
<kw>However</kw>, their dataset <negfeature>consists of only</negfeature> 569 citations from 10 papers and their annotation scheme deals with neither acronyms nor sentiment.</context> 
</paragraph> 
</section> 
<section imrad="d"> 
<title>6 Discussion</title> 
<paragraph> 
What is the role of citation contexts in the overall structure of scientific context? 
<context><author>We</author> <kw>assume</kw> a hierarchical, rhetorical structure not unlike RST (<cite id="70" function="bas" polarity="pos">Mann and Thompson, 1987</cite>), but much flatter, where the atomic units are textual blocks which carry a certain <posfeature>functional role in</posfeature> the overall scientific argument <kw>for</kw> publication (<cite id="71" function="bas" polarity="pos">Teufel, 2010</cite>; <cite id="72" function="bas" polarity="pos">Hyland, 2000</cite>).</context> 
Under such a general model, citation blocks are certainly a functional unit, and their recognition is a rewarding task in their own right. 
If citation blocks can be recognised along with their sentiment, this is even more useful, as it restricts the possibilities for which rhetorical function the segment plays. 
For instance, in the motivation section of a paper, before the paper contribution is introduced, we often find negative sentiment assigned to citations, as any indication can serve as a justification for the current paper. 
In contrast, positive sentiment is more likely to be restricted to the description of an approach which the authors include in their solution, or further develop. 
</paragraph> 
<paragraph> 
Another aspect concerns which features might help in detecting coherent citation blocks. 
We have here addressed coherence of citation contexts via certain referring expressions, lexical hooks and also coherence-indicating conjunctions (amongst others). 
The reintroduction of citation contexts was addressed via lexical hooks. 
Much more could be done to explore this very interesting question. 
<context>A more <concept>fine-grained model</concept> of coherence <kw>might include</kw> proper anaphora resolution (<cite id="73" function="use" polarity="neu">Lee et al., 2011</cite>), which is still an unsolved <task>task</task> <kw>for</kw> scientific texts, and also include models of lexical coherence such as lexical chains (<cite id="74" function="use" polarity="neu">Barzilay and Elhadad, 1997</cite>) and entity coherence (<cite id="75" function="use" polarity="neu">Barzilay and Lapata, 2008</cite></context>). 
</paragraph> 
</section> 
<section imrad="d"> 
<title>7 Conclusion</title> 
<paragraph>
In this paper, we focus on automatic detection of citation sentiment using citation context. 
We annotate a new large corpus and show that ignoring the citation context would result in loss of a lot of sentiment, specially criticism. 
We also report the results of the state-of-the-art citation sentiment detection systems on this corpus and when using this context-enhanced gold standard definition. 
</paragraph> 
</section> 
 
 
 
</paper> 
</annotatedpaper>