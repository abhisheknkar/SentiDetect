<annotatedpaper>
    <paper title=" Identifying Non-Explicit Citing Sentences for Citation-Based Summarization." authors="Vahed Qazvinian and Dragomir R. Radev " year="2010 "> 
        <section> 
            <title> Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.  </title> 
            Vahed Qazvinian  
            Department of EECS 
            University of Michigan 
            Ann Arbor, MI 
            vahed@umich.edu 
 
            Dragomir R. Radev 
 
            Department of EECS and 
            School of Information 
            University of Michigan 
            Ann Arbor, MI 
            radev@umich.edu 
        </section> 
        <section> 
            <title>Abstract</title> 
            <paragraph> 
                Identifying background (context) information in scientific articles can help scholars understand major contributions in their research area more easily.In this paper, we propose a general framework based on probabilistic inference to extract such context information from scientific papers. We model the sentences in an article and their lexical similarities as a Markov Random Field tuned to detect the patterns that context data create, and employ a Belief Propagation mechanism to detect likely context sentences. We also address the problem of generating surveys of scientific papers.Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone. 
            </paragraph> 
        </section> 
        <section imrad="i"> 
            <title>1 Introduction</title> 
            <paragraph> 
                In scientific literature, scholars use citations to refer to external sources.These secondary sources are essential in comprehending the new research. <context>
                    <kw>Previous work</kw> has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information ( <cite id="1" function="ack" polarity="neu">Sid-dharthan and Teufel, 2007</cite>; <cite id="2" function="ack" polarity="neu">Elkiss et al., 2008</cite>; <cite id="3" function="ack" polarity="neu">Qazvinian and Radev, 2008</cite>; <cite id="4" function="ack" polarity="neu">Mohammad et al., 2009</cite>; <cite id="5" function="ack" polarity="neu">Mei and Zhai, 2008</cite>)</context>. 
            </paragraph> 
            <paragraph> 
                A citation to a paper in a scientific article may contain explicit information about the cited research.The following example is an excerpt from a CoNLL paper that contains information about Eisner's work on bottom-up parsers and the notion of span in parsing:<context>"Another use of bottom-up is due to <cite id="6" function="ack" polarity="neu">Eisner (1996)</cite>, who introduced the notion of a span."</context>. However, the citation to a paper may not always include explicit information about the cited paper:<context>"This approach is one of those described in <cite id="7" function="ack" polarity="neu">Eisner (1996)</cite>" </context>
            </paragraph> 
            <paragraph> 
                Although this sentence alone does not provide any information about the cited paper, it suggests that its surrounding sentences describe the proposed approach in Eisner's paper: 
            </paragraph> 
            <paragraph> 
                We refer to such implicit citations that contain information about a specific secondary source but do not explicitly cite it, as sentences with context information or context sentences for short.We look at the patterns that such sentences create and observe that context sentences occur with-ing a small neighborhood of explicit citations.We also discuss the problem of extracting context sentences for a source-reference article pair.We propose a general framework that looks at each sentence as a random variable whose value determines its state about the target paper.In summary, our proposed model is based on the probabilistic inference of these random variables using graphical models.Finally we give evidence on how such sentences can help us produce better surveys of research areas.The rest of this paper is organized as follows.Preceded by a review of prior work in Section 2, we explain the data collection and our annotation process in Section 3.Section 4 explains our methodology and is followed by experimental setup in Section 5."...In an all pairs approach, every possible pair of two tokens in a sentence is considered and some score is assigned to the possibility of this pair having a (directed) dependency relation. Using that information as building blocks, the parser then searches for the best parse for the sentence. <context>This approach is one of those described in <cite id="8" function="ack" polarity="neu">Eisner (1996)</cite></context>."Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publication year, number of references (in AAN) and number of sentences.Papers marked with * are used to calculate inter-judge agreement. 
            </paragraph> 
        </section> 
        <section> 
            <title>2 Prior Work</title> 
            <paragraph> 
 
                Analyzing the structure of scientific articles and their relations has received a lot of attention recently.<context>The structure of citation and collaboration networks has been studied in (<cite id="9" function="ack" polarity="neu">Teufel et al., 2006</cite>; <cite id="10" function="ack" polarity="neu">Newman, 2001</cite>), and summarization of scientific documents is discussed in (<cite id="11" function="ack" polarity="neu">Teufel and Moens, 2002</cite>).In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (<cite id="12" function="ack" polarity="neu">Elkiss et al., 2008</cite>) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts.</context> 
                <context>In other work, Nanba et al, (<cite id="13" function="use" polarity="neu">Nanba and Okumura, 1999</cite>; <cite id="14" function="use" polarity="neu">Nanba et al., 2004</cite>b; <cite id="15" function="use" polarity="neu">Nanba et al., 2004</cite>a) <action>analyze citation sentences</action> and automatically categorize them in order <kw>to build a</kw> 
                    <tool>tool for</tool> survey generation</context>. 
            </paragraph> 
            <paragraph> 
                <context>The text of scientific citations has been used in previous research. Bradshaw (<cite id="16" function="ack" polarity="neu">Bradshaw, 2002</cite>; <cite id="17" function="ack" polarity="neu">Bradshaw, 2003</cite>) uses citations to determine the content of articles</context>.<context>Similarly, the text of citation sentences has been directly used to produce summaries of scientific papers in (<cite id="18" function="ack" polarity="neu">Qazvinian and et al., 2009</cite>).Determining the scientific attribution of an article has also been studied before. Siddharthan and Teufel (<cite id="19" function="ack" polarity="neu">Siddharthan and Teufel, 2007</cite>; <cite id="20" function="ack" polarity="neu">Teufel, 2005</cite>) categorize sentences according to their role in the author's argument into predefined classes: Own, Other, Background, Textual, Aim, Basis, Contrast</context>. 
            </paragraph> 
            <paragraph> 
                <context>Little work has been done on automatic citation extraction from research papers.Kaplan et al, (<cite id="21" function="ack" polarity="neu">Kaplan et al., 2009</cite>) introduces "citation-site" as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citations from research papers and evaluates the result using 4 annotated articles</context>. 
            </paragraph> 
            <paragraph> 
                In our work we use graphical models to extract context sentences. Graphical models have a number of properties and corresponding techniques and have been used before on Information Retrieval tasks. <context>Romanello et al, (<cite id="22" function="ack" polarity="neu">Romanello et al., 2009</cite>) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts.Similar work include term dependency extraction (<cite id="23" function="ack" polarity="neu">Metzler and Croft, 2005</cite>), query expansion (<cite id="24" function="ack" polarity="neu">Metzler and Croft, 2007</cite>), and automatic feature selection (<cite id="25" function="ack" polarity="neu">Metzler, 2007</cite>)</context>. 
            </paragraph> 
        </section> 
        <section imarad="m"> 
            <title>3 Data</title> 
            <paragraph> 
                <context>The ACL Anthology Network (AAN) is a collection of papers from the ACL Anthology published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and <kw>includes more than 14, 000 papers</kw> over a period of four decades (<cite id="26" function="bas" polarity="pos">Radev et al., 2009</cite>).AAN includes the citation network of the papers in the ACL Anthology.The papers in AAN are publicly available in text format retrieved by an OCR process from the original pdf files, and are segmented into sentences. 
                    To build a corpus for <author>our experiments</author> we picked 10 recently published papers from various areas in NLP, each of which had references for a total of 203 candidate paper-reference pairs</context>.Table 1 lists these papers together with their authors, titles, publication year, number of references, number of references within AAN, Regardless of data selection, the methodology in this work is applicable to any ofthe papers in AAN.L&amp;PS&amp;al Sentence <cite id="27" function="ack" polarity="neu">Jacquemin (1999)</cite> and Barzilay and <cite id="28" function="ack" polarity="neu">McKeown (2001)</cite> identify phrase level paraphrases, while Lin and <cite id="29" function="ack" polarity="neu">Pantel (2001)</cite> and <cite id="30" function="ack" polarity="neu">Shinyama et al. (2002)</cite> acquire structural paraphrases encoded as templates.These latter are the most closely related to the sentence-level paraphrases we desire, and so we focus in this section on template-induction approaches.Lin and <cite id="31" function="ack" polarity="neu">Pantel (2001)</cite> extract inference rules, which are related to paraphrases (for example, X wrote Y implies X is the author of Y), to improve question answering.They assume that paths in dependency trees that take similar arguments (leaves) are close in meaning.1 I 0 I However, only two-argument templates are considered.0 \! C \! <cite id="32" function="ack" polarity="neu">Shinyama et al. (2002)</cite> also use dependency-tree information to extract templates of a limited form (in their case, determined by the underlying information extraction application).Like us (and unlike Lin and Pantel, who employ a single large corpus), they use articles written about the same event in different newspapers as data.Our approach shares two characteristics with the two methods just described: pattern comparison by analysis of the patterns respective arguments, and use ofnonparallel corpora as a data source.However, extraction methods are not easily extended to generation methods.One problem is that their templates often only match small fragments ofa sentence.While this is appropriate for other applications, deciding whether to use a given template to generate a paraphrase requires information about the surrounding context provided by the entire sentence. 
            </paragraph> 
            <paragraph> 
                Table 2: <context>Part of the annotation for N03-1003 with respect to two of its references "Lin and Pan-tel (2001)" (the first column) "<cite id="33" function="ack" polarity="neu">Shinyama et al. (2002)</cite>" (the second column).C s indicate explicit citations, 1s indicate implicit citations and 0s are none</context>. 
            </paragraph> 
            <subsection> 
                <subtitle>3.1 Annotation Process</subtitle> 
                <paragraph> 
 
                    We annotated the sentences in each paper from Table 1.Each annotation instance in our setting corresponds to a paper-reference pair, and is a vector in which each dimension corresponds to a sentence and is marked with a C if it explicitly cites the reference, and with a 1 if it implicitly talks about it.All other sentences are marked with 0s.Table 2 shows a portion of two separate annotation instances of N03-1003 corresponding to two of its references.Our annotation has resulted in 203 annotation instances each corresponding to one paper-reference pair.The goal of this work is to automatically identify all context sentences, which are marked as "1". 
                </paragraph> 
 
                <subsection> 
                    <subtitle>3.1.1 Inter-judge Agreement</subtitle> 
                    <paragraph> 
 
                        We also asked a neutral annotator to annotate two of our datasets that are marked with * in Table 1.For each paper-reference pair, the annotator was provided with a vector in which explicit citations were already marked with Cs.The annotation guidelines instructed the annotator to look at each explicit citation sentence, and read up to 15 sentences before and after, then mark context sentences around that sentence with 1s.Next, the 29 annotation instances done by the external annota-tor were compared with the corresponding annotations that we did, and the Kappa coefficient (k) was calculated.The k statistic is formulated as Table 3: Averagekcoefficient as inter-judge agreement for annotations of two sets. Someone not involved with the paper but an expert in NLP.where Pr(a) is the relative observed agreement among raters, and Pr(e) is the probability that an-notators agree by chance if each annotator is randomly assigning categories.To calculate k, we ignored all explicit citations (since they were provided to the external annotator) and used the binary categories (i.e., 1 for context sentences, and 0 otherwise) for all other sentences.Table 3 shows the annotation vector size (i.e., number of sentences), number of annotation instances (i.e., number of references), and average k for each set.The average k is above 0.85 in both cases, suggesting that the annotation process has a low degree of subjectivity and can be considered reliable. 
                    </paragraph> 
                </subsection> 
            </subsection> 
            <subsection> 
                <subtitle>3.2 Analysis</subtitle> 
                <paragraph> 
 
                    In this section we describe our analysis.First, we look at the number of explicit citations each reference has received in a paper.Figure 1 (a) shows the histogram corresponding to this distribution.It indicates that the majority of references get cited in only 1 sentence in a scientific article, while the maximum being 9 in our collected dataset with only 1 instance (i.e., there is only 1 reference that gets cited 9 times in a paper).Moreover, the data exhibits a highly positive-skewed distribution.This is illustrated on a log-log scale in Figure 1 (b).This highly skewed distribution indicates that the majority of references get cited only once in a citing paper.The very small number of citing sentences can not make a full inventory of the contributions of the cited paper, and therefore, extracting explicit citations alone without context sentences may result in information loss about the contributions of the cited paper.Figure 1: (a) Histogram of the number of different citations to each reference in a paper.(b) The distribution observed for the number of different citations on a log-log scale. 
                </paragraph> 
                <paragraph> 
                    Next, we investigate the distance between context sentences and the closest citations.For each context sentence, we find its distance to the closets context sentence or explicit citation.Formally, we define the gap to be the number of sentences between a context sentence (marked with 1) and the closest context sentence or explicit citation (marked with either C or 1) to it.For example, <context>the second column of Table 2 shows that there is a gap of size 1 in the 9th sentence in the set of context and citation sentences about <cite id="34" function="ack" polarity="neu">Shinyama et al. (2002)</cite></context>.Table 4 shows the distribution of gap sizes in the annotated data.This observation suggests that the majority of context sentences directly occur after or before a citation or another context sentence.However, it shows that gaps between sentences describing a cited paper actually exist, and a proposed method should have the capability to capture them. 
                </paragraph> 
            </subsection> 
        </section> 
        <section> 
            <title>4 Proposed Method</title> 
            <paragraph> 
 
                In this section we propose our methodology that enables us to identify the context information of a cited paper.Particularly, the task is to assign a binary label XC to each sentence Si from a paper S, where XC = 1 shows a context sentence related to a given cited paper, C. To solve this problem we propose a systematic way to model the network level relationship between consecutive sentences.In summary, each sentence is represented with a node and is given two scores (context, non-context), and we update these scores to be in harmony with the neighbors' scores.A particular class of graphical models known as Markov Random Fields (MRFs) are suited for solving inference problems with uncertainty in observed data.The data is modeled as an undirected graph with two types of nodes: hidden and observed.Observed nodes represent values that are known from the data.Each hidden node xu, corresponding to an observed node yu, represents the true state underlying the observed value.The state of a hidden node is related to the value of its corresponding observed node as well as the states of its neighboring hidden nodes. 
            </paragraph> 
            <paragraph> 
                The local Markov property of an MRF indicates that a variable is conditionally independent on all other variables given its neighbors: xv _L -L xV\d(v)\xne(v), where ne(v) is the set of neighbors of v, and cl(v) = {v} U ne(v) is the closed neighborhood ofv.Thus, the state ofa node is assumed to statistically depend only upon its hidden node and each of its neighbors, and independent of any other node in the graph given its neighbors. 
            </paragraph> 
            <paragraph> 
                Dependencies in an MRF are represented using two functions: Compatibility function (ip) and Potential function ((f)). ipuv(xc,xd) shows the edge potential of an edge between two nodes u, v of classes xc and xd.Large values of ipuv would indicate a strong association between xc and xd at nodes u, v.The Potential function, ( i(xc, yc), shows the statistical dependency between xc and yc at each node i assumed by the MRF model. 
            </paragraph> 
            <paragraph> 
                <context>In order to find the marginal probabilities of xis in a MRF <author>we</author> 
                    <kw>can use</kw> 
                    <method>BeliefPropagation (BP)</method> (<cite id="35" function="bas" polarity="pos">Yedidia et al., 2003</cite>)</context>.If we assume the yis are fixed and show fi(xi ,yi) by fi(xi), we can find the joint probability distribution for unknown variables xi as 
            </paragraph> 
            <paragraph> 
                In the BP algorithm a set of new variables m is introduced where mij (xj ) is the message passed from i to j about what state xj should be in.Each message, mij (xj ), is a vector with the same dimensionality of xj in which each dimension shows i's opinion about j being in the corresponding class.Therefore each message could be considered as a probability distribution and its components should sum up to 1.The final belief at a 
            </paragraph> 
            <paragraph> 
                Table 4: The distribution of gaps in the annotated data. Figure 2: The illustration of the message updating rule.Elements that make up the message from a node i to another node j : messages from i's neighbors, local evidence at i, and propagation function between i, j summed over all possible states of node i.node i, in the BP algorithm, is also a vector with the same dimensionality of messages, and is proportional to the local evidence as well as all messages from the node's neighbors:where k is the normalization factor of the beliefs about different classes.The message passed from i to j is proportional to the propagation function between i, j, the local evidence at i, and all messages sent to i from its neighbors except j:mij (xj ) — Yl fi (xi (xi ,xj ) n mki (xi ) 
            </paragraph> 
            <paragraph> 
                Figure 2 illustrates the message update rule.Convergence can be determined based on a variety of criteria.It can occur when the maximum change of any message between iteration steps is less than some threshold.<context>Convergence is guaranteed for trees but not for general graphs.However, it typically occurs in practice (<cite id="36" function="ack" polarity="neu">McGlohon et al., 2009</cite>)</context>.Upon convergence, belief scores are determined by Equation 1. 
            </paragraph> 
            <subsection> 
                <subtitle>4.1 MRF construction</subtitle> 
                <paragraph> 
 
                    To find the sentences from a paper that form the context information of a given cited paper, we build an MRF in which a hidden node xi and an observed node yi correspond to each sentence.The structure of the graph associated with the MRF is dependent upon the validity of a basic assumption.This assumption indicates that the generation of a sentence (in form of its words) only. Figure 3: The structure of the MRF constructed based on the independence of non-adjacent sentences; (a) left, each sentence is independent on all other sentences given its immediate neighbors.(b) right, sentences have dependency relationship with each other regardless of their position. 
                </paragraph> 
                <paragraph> 
                    depends on its surrounding sentences.Said differently, each sentence is written independently of all other sentences given a number of its neighbors.This local dependence assumption can result in a number of different MRFs, each built assuming a dependency between a sentence and all sentences within a particular distance.Figure 3 shows the structure of the two MRFs at either extreme of the local dependence assumption.In Figure 3 a, each sentence only depends on one following and one preceding sentence, while Figure 3 b shows an MRF in which sentences are dependent on each other regardless of their position.We refer to the (2f)ormer by BPi, and to the latter by BPn.Generally, we use BPi to denote an MRF in which each sentence is connected to i sentences before and after. 
                </paragraph> 
                <paragraph> 
                    Table 5: The compatibility function p between any two nodes in the MRFs from the sentences in scientific papers 
                </paragraph> 
            </subsection> 
            <subsection> 
                <subtitle>4.2 Compatibility Function</subtitle> 
                <paragraph> 
 
                    The compatibility function of an MRF represents the association between the hidden node classes.A node's belief to be in class 1 is its probability to be included in the context.The belief of a node i, about its neighbor j to be in either classes is assumed to be 0.5 if i is in class 0.In other words, if a node is not part of the context itself, we assume it has no effect on its neighbors' classes.In contrast, if i is in class 1 its belief about its neighbor j is determined by their mutual lexical similarity.Ifthis similarity is close to 1 it indicates a stronger tie between i, j.However, if i, j are not similar, i's probability of being in class 1, should not affect that of j's.To formalize this assumption we use the sigmoid ofthe cosine similarity oftwo sentences to build p.More formally, we define S to 
                </paragraph> 
                <paragraph> 
                    The sigmoid function obtains a value of 0.5 for a cosine of 0 indicating that there is no bias in the association ofthe two sentences.The matrixin Table 5 shows the compatibility function built based on the above arguments. 
                </paragraph> 
            </subsection> 
            <subsection> 
                <subtitle>4.3 Potential Function</subtitle> 
                <paragraph> 
 
                    The node potential function of an MRF can incorporate some other features observable from data.Here, the goal is to find all sentences that are about a specific cited paper, without having explicit citations.To build the node potential function of the observed nodes, we use some sentence level features.First, we use the explicit citation as an important feature of a sentence.This feature can affect the belief of the corresponding hidden node, which can in turn affect its neighbors' beliefs.For a given paper-reference pair, we flag (with a 1) each sentence that has an explicit citation to the reference. 
                </paragraph> 
                <paragraph> 
                    The second set of features that we are interested in are discourse-based features.In particular we match each sentence with specific patterns and flag those that match.The first pattern is a bigram in which the first term matches any of "this; that; those; these; his; her; their; such; previous", and the second term matches any of "work; approach; system; method; technique; result; example".The second pattern includes all sentences that start with "this; such". 
                </paragraph> 
                <paragraph> 
                    Finally, the similarity of each sentence to the reference is observable from the data and can be used as a sentence-level feature.Intuitively, if a sentence has higher similarity with the reference paper, it should have a higher potential of being in class 1 or C. The flag of each sentence here is a value between 0 and 1 and is determined by its cosine similarity to the reference.Once the flags for each sentence, Si are determined, we calculate normalized fi as the unweighted linear combination of individual features.Based on fis, we compute the potential function, ( , as shown in Table 6. 
                </paragraph> 
                <paragraph> 
                    Table 6: The node potential function ( for each node in the MRFs from the sentences in scientific papers is built using the sentences' flags computed using sentence level features. 
                </paragraph> 
            </subsection> 
        </section> 
        <section> 
            <title>5 Experiments</title> 
            <paragraph> 
 
                The intrinsic evaluation of our methodology means to directly compare the output of our method with the gold standards obtained from the annotated data.Our methodology finds the sentences that cite a reference implicitly.Therefore the output of the inference method is a vector, u, of 1's and 0's, whereby a 1 at element i means that sentence i in the source document is a context sentence about the reference while a 0 means an explicit citation or neither.The gold standard for each paper-reference pair, w (obtained from the annotated vectors in Section 3.1 by changing all Cs to 0s), is also a vector of the same format and dimensionality.Precision, recall, and Fß for this task can be defined as where 1 is a vector of 1's with the same dimensionality and ß is a non-negative real number. 
            </paragraph> 
            <subsection> 
                <subtitle>5.1 Baseline Methods</subtitle> 
                <paragraph> 
 
                    The first baseline that we use is an IR-based method.This baseline, Bi, takes explicit citations as an input but use them to find context sentences.Given a paper-reference pair, for each explicit citation sentence, marked with C, Bi picks its preceding and following sentences if their similarities to that sentence is greater than a cutoff (the median of all such similarities), and repeats this for neighboring sentences of newly marked sentences.Intuitively, B1 tries to find the best chain (window) around citing sentences. 
                </paragraph> 
                <paragraph> 
                    As the second baseline, we use the hand-crafted discourse based features used in MRF's potential function.Particularly, this baseline, B2, marks Table 7: Average Fß=3 for similarity based baseline (B1), discourse-based baseline (B2), a supervised method (SVM) and three MRF-based methods.each sentence that is within a particular distance (4 in our experiments) of an explicit citation and matches one of the two patterns mentioned in Section 4.3.After marking all such sentences, B2 also marks all sentences between them and the closest explicit citation, which is no farther than 4 sentences away.This baseline helps us understand how effectively this sentence level feature can work in the absence of other features and the network structure. 
                </paragraph> 
                <paragraph> 
                    Finally, we use a supervised method, SVM, to classify sentences as context/non-context.We use 4 features to train the SVM model.These 4 features comprise the 3 sentence level features used in MRF's potential function (i.e., similarity to reference, explicit citation, matching certain regular-expressions) and a network level feature: distance to the closes explicit citation.For each source paper, P, we use all other source papers and their source-reference annotation instances to train a model.We then use this model to classify all instances in P. Although the number of references and thus source-reference pairs are different for different papers, this can be considered similar to a 10-fold cross validation scheme, since for each source paper the model is built using all source-reference pairs of all other 9 papers. 
                </paragraph> 
                <paragraph> 
                    We compare these baselines with 3 MRF-based systems each with a different assumption about independence of sentences.BPi denotes an MRF in which each sentence is only connected to 1 sentence before and after.In BP4 locality is more relaxed and each sentence is connected to 4 sentences on each sides.BPn denotes an MRF in which all sentences are connected to each other regardless of their position in the paper. 
                </paragraph> 
                <paragraph> 
                    Table 7 shows Fß=3 for our experiments and shows how BP4 outperforms the other methods on average.The value 4 may suggest the fact that although sentences might be independent of distant sentences, they depend on more than one sentence on each side. 
                </paragraph> 
                <paragraph> 
                    The final experiment we do to intrinsically evaluate the MRF-base method is to compare different sentence-level features.The first feature used to build the potential function is explicit citations.This feature does not directly affect context sentences (i.e., it affects the marginal probability of context sentences through the MRF network connections).Therefore, we do not alter this feature in comparing different features.However, we look at the effect of the second and the third features: hand-crafted regular expression-based features and similarity to the reference.For each paper, we use BP4 to perform 3 experiments: two in absence of each feature and one including all features.Figure 4 shows the average Fß=3 for each experiment.This plot shows that the features lead to better results when used together. 
                </paragraph> 
            </subsection> 
        </section> 
        <section immrad="d"> 
            <title>6 Impact on Survey Generation</title> 
            <paragraph> 
 
                We also performed an extrinsic evaluation of our context extraction methodology. Here we show how context sentences add important survey-worthy information to explicit citations. Previous work that generate surveys of scientific topics use the text of citation sentences alone (Mohammad we show how the surveys generated using citations and their context sentences are better than those generated using citation sentences alone. <context>
                    <author>We</author> 
                    <kw>use the data from</kw> (<cite id="37" function="bas" polarity="neu">Mohammad et al., 2009</cite>)</context> 
                <context>Naturally, our current work on question answering for the reading comprehension task is most related to those of (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand <cite id="38" function="ack" polarity="neu">Thelen, 2000</cite> ; <cite id="39" function="ack" polarity="neu">Wang et al. , 2000</cite>).In fact, all of this body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the same development set of stories.The work of (<cite id="40" function="ack" polarity="neu">Hirschman et al. , 1999</cite>) initiated this series of work, and it reported an accuracy of 36.3% on answering the questions in the test stories.Subsequently, the work of (<cite id="41" function="ack" polarity="neu">Riloffand Thelen , 2000</cite>) and (<cite id="42" function="ack" polarity="neu">Chaxniak et al. , 2000</cite>) improved the accuracy further to 39.7% and 41%, respectively.However, all of these three systems used handcrafted, deterministic rules and algorithms...The cross-model comparison showed that the performance ranking of these models was: U-SVM > PatternM > S-SVM > Retrieval-M. Compared with retrieval-based [<cite id="43" function="ack" polarity="neu">Yang et al. 2003</cite>], pattern-based [<cite id="44" function="ack" polarity="neu">Ravichandran et al. 2002</cite> and <cite id="45" function="ack" polarity="neu">Soubbotin et al. 2002</cite>], and deep NLP-based [<cite id="46" function="ack" polarity="neu">Moldovan et al. 2002</cite>, <cite id="47" function="ack" polarity="neu">Hovy et al. 2001</cite>; and <cite id="48" function="ack" polarity="neu">Pasca et al. 2001</cite>] answer selection, machine learning techniques are more effective in constructing QA components from scratch.These techniques suffer, however, from the problem of requiring an adequate number of handtagged question-answer training pairs.It is too expensive and labor intensive to collect such training pairs for supervised machine learning techniques ... As expected, the definition and person-bio answer types are covered well by these resources.The web has been employed for pattern acquisition (<cite id="49" function="ack" polarity="neu">Ravichandran et al. , 2003</cite>), document retrieval (<cite id="50" function="ack" polarity="neu">Dumais et al. , 2002</cite>), query expansion (<cite id="51" function="ack" polarity="neu">Yang et al. , 2003</cite>), structured information extraction, and answer validation (Magnini et al. , 2002).Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers ...Table 8: A portion of the QA survey generated by LexRank using the context information.Table 9: Pyramid Fß=3 scores of automatic surveys of QA and DP data.The QA surveys are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from citation texts (CT)</context>. 
            </paragraph> 
            <paragraph> 
                Figure 4: AverageFß=3for BP4employing different features.that contains two sets of cited papers and corresponding citing sentences, one on Question Answering (QA) with 10 papers and the other on Dependency Parsing (DP) with 16 papers.The QA set contains two different sets ofnuggets extracted by experts respectively from paper abstracts and citation sentences.The DP set includes nuggets extracted only from citation sentences.We use these nugget sets, which are provided in form of regular expressions, to evaluate automatically generated summaries.To perform this experiment we needed to build a new corpus that includes context sentences.For each citation sentence, BP4 is used on the citing paper to extract the proper context.Here, we limit the context size to be 4 on each side.That is, we attach to a citing sentence any of its 4 preceding and following sentences if 
            </paragraph> 
            <paragraph> 
                BP4 marks them as context sentences.Therefore, we build a new corpus in which each explicit citation sentence is replaced with the same sentence attached to at most 4 sentence on each side. 
            </paragraph> 
            <paragraph> 
                <context>After building the context corpus, <author>we</author> 
                    <kw>use</kw> 
                    <tool>LexRank</tool> (<cite id="52" function="bas" polarity="pos">Erkan and Radev, 2004</cite>) to generate 2 QA and 2 DP surveys using the citation sentences only, and the new context corpus explained above</context>.LexRank is a multidocument summarization system, which first builds a cosine similarity graph of all the candidate sentences.Once the network is built, the system finds the most central sentences by performing a random walk on the graph.We limit these surveys to be of a maximum length of 1000 words.Table 8 shows a portion of the survey generated from the QA context corpus.This example shows how context sentences add meaningful and survey-worthy information along with citation sentences.Table 9 shows the Pyramid Fß=3 score of automatic surveys of QA and DP data.The QA surveys are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from citation texts (CT).In all evaluation instances the surveys generated with the context corpora excel at covering nuggets drawn from abstracts or citation sentences. 
            </paragraph> 
        </section> 
        <section> 
            <title>7 Conclusion</title> 
            <paragraph> 
 
                In this paper we proposed a framework based on probabilistic inference to extract sentences that appear in the scientific literature, and which are about a secondary source, but which do not contain explicit citations to that secondary source.Our methodology is based on inference in an MRF built using the similarity of sentences and their lexical features.We show, by numerical experiments, that an MRF in which each sentence is connected to only a few adjacent sentences properly fits this problem.We also investigate the usefulness of such sentences in generating surveys of scientific literature.Our experiments on generating surveys for Question Answering and Dependency Parsing show how surveys generated using such context information along with citation sentences have higher quality than those built using citations alone. 
            </paragraph> 
            <paragraph> 
                Generating fluent scientific surveys is difficult in absence of sufficient background information.Our future goal is to combine summarization and bibliometric techniques towards building automatic surveys that employ context information as an important part of the generated surveys. 
            </paragraph> 
        </section> 
        <section> 
            <title>8 Acknowledgments</title> 
            <paragraph> 
 
                The authors would like to thank Arzucan Ozgur from University of Michigan for annotations. 
            </paragraph> 
            <paragraph> 
                This paper is based upon work supported by the National Science Foundation grant "iOPENER: A Flexible Framework to Support Rapid Learning in Unfamiliar Research Domains", jointly awarded to University of Michigan and University of Maryland as IIS 0705832.Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the National Science Foundation. 
            </paragraph> 
        </section> 
    </paper> 
 
</annotatedpaper>