<annotatedpaper>
    <paper title= "Detection Of Agreement vs. Disagreement In Meetings: Training With Unlabeled Data" year="2003" authors="Dustin Hillard and Mari Ostendorf, Elizabeth Shriberg">
        <section>
            <title>Detection Of Agreement vs. Disagreement In Meetings: Training With Unlabeled Data</title>
            Dustin Hillard and Mari Ostendorf
            University of Washington, EE
            {hillard,mo}@ee.washington.edu
            Elizabeth Shriberg
            SRI International and ICSI
            ees@speech.sri.com
        </section>
        <section>
            <title>Abstract</title>
            <paragraph>To support summarization of automatically transcribed meetings, we introduce a classifier to recognize agreement or disagreement utterances, utilizing both word-based and prosodic cues. We show that hand-labeling efforts can be minimized by using unsupervised training on a large unlabeled data set combined with supervised training on a small amount of data. For ASR transcripts with over 45% WER, the system recovers nearly 80% of agree/disagree utterances with a confusion rate of only 3%.</paragraph>
        </section>
        <section imrad="i">
            <title>1 Introduction</title>
            <paragraph>Meetings are an integral component of life in most organizations, and records of meetings are important for helping people recall (or learn for the first time) what took place in a meeting. Audio (or audio-visual) recordings of meetings offer a complete record of the interactions, but listening to the complete recording is impractical. To facilitate browsing and summarization of meeting recordings, it is useful to automatically annotate topic and participant interaction characteristics. Here, we focus on interactions, specifically identifying agreement and disagreement. These categories are particularly important for identifying decisions in meetings and inferring whether the decisions are controversial, which can be useful for automatic summarization. In addition, detecting agreement is important for associating action items with meeting participants and for understanding social dynamics. In this study, we focus on detection using both prosodic and language cues, contrasting results for hand-transcribed and automatically transcribed data.</paragraph>
            <paragraph>The agreement/disagreement labels can be thought of as a sort of speech act categorization. Automatic classification of speech acts has been the subject of several studies. 
                <context>
                <author>Our</author> <kw>work builds on</kw> (<cite id="1" function="bas" polarity="pos">Shriberg et al., 1998</cite>), <kw>which showed</kw> <result>that prosodic features are useful for classifying speech</result> acts and lead to increased accuracy when combined with word based cues.
                </context>
                <context>
                <kw>Other</kw> <paper>studies</paper> <kw>look at</kw> <task>prediction of speech acts</task> primarily from word-based cues, <kw>using</kw><method>language models</method> <kw>or</kw> <feature>syntactic structure</feature> and discourse history (<cite id="2" function="use" polarity="neu">Chu-Carroll, 1998</cite>; <cite id="3" function="use" polarity="neu">Reithinger and Klesen, 1997</cite>).
                </context>

                Our work is informed by these studies, but departs significantly by exploring unsupervised training techniques.
            
            </paragraph>
        </section>
        <section imrad="m">
            <title>2 Approach</title>
            <paragraph>
                <context>
                <author>Our</author> <paper>experiments</paper> <kw>are based on</kw> <data>a subset ofmeeting recordings </data> <kw>collected and transcribed by</kw> ICSI (<cite id="4" function="bas" polarity="pos">Morgan et al., 2001</cite>). 
                </context>
            Seven meetings were segmented (automatically, but with human adjustment) into 9854 total spurts. 
                <context>
                <author>We </author> <kw>define a</kw> <feature>'spurt'</feature> as a period of speech by one speaker that has no pauses of greater than one half second (<cite id="5" function="bas" polarity="neu">Shriberg et al., 2001</cite>). 
                </context>
                Spurts are used here, rather than sentences, because our goal is to use ASR outputs and unsuper-vised training paradigms, where hand-labeled sentence segmentations are not available.</paragraph>
            <paragraph>We define four categories: positive, backchannel, negative, and other. Frequent single-word spurts (specifically, yeah, right, yep, uh-huh, and ok) are separated out from the 'positive' category as backchannels because of the trivial nature of their detection and because they may reflect encouragement for the speaker to continue more than actual agreement. Examples include: Neg: (6%) "This doesn't answer the question." Other: (62%) "Let's move on to the next topic."</paragraph>
            <paragraph>The first 450 spurts in each of four meetings were hand-labeled with these four categories based on listening to speech while viewing transcripts (so a sarcastic "yeah, right" is labeled as a disagreement despite the positive wording). 
                <context>
                Comparing tags on 250 spurts from two labelers produced a kappa coefficient (<cite id="6" function="ack" polarity="neu">Siegel and Castellan, 1988</cite>) of . 6, which is generally considered acceptable. 
                </context>
                Additionally, unlabeled spurts from six hand-transcribed training meetings are used in unsupervised training experiments, as described later. The total number of automatically labeled spurts (8094) is about five times the amount of hand-labeled data.</paragraph>
            <paragraph>For system development and as a control, we use handtranscripts in learning word-based cues and in training. We then evaluate the model with both hand-transcribed words and ASR output. The category labels from the hand transcriptions are mapped to the ASR transcripts, assigning an ASR spurt to a hand-labeled reference if more than half (time wise) of the ASR spurt overlaps the reference spurt.</paragraph>
            <paragraph>Feature Extraction. The features used in classification include heuristic word types and counts, word-based features derived from n-gram scores, and prosodic features.</paragraph>
            <paragraph>Simple word-based features include: the total number of words in a spurt, the number of "positive" and "negative" keywords, and the class (positive, negative, backchannel, discourse marker, other) of the first word based on the keywords. The keywords were chosen based on an "effectiveness ratio," defined as the frequency of a word (or word pair) in the desired class divided by the frequency over all dissimilar classes combined. A minimum of five occurrences was required and then all instances with a ratio greater than . 6 were selected as keywords.</paragraph>
            <paragraph>Other word-based features are found by computing the perplexity (average log probability) of the sequence of words in a spurt using a bigram language model (LM) for each of the four classes. The perplexity indicates the goodness of fit of a spurt to each class. We used both word and class LMs (with part-of-speech classes for all words except keywords). In addition, the word-based LM is used to score the first two words of the spurt, which often contain the most information about agreement and disagreement. The label of the most likely class for each type of LM is a categorical feature, and we also compute the posterior probability for each class.</paragraph>
            <paragraph>
                <context>
                Prosodic features include pause, fundamental frequency (F0), and duration (<cite id="7" function="ack" polarity="neu">Baron et al., 2002</cite>). 
                </context>
            
            Features are derived for the first word alone and for the entire spurt. Average, maximum and initial pause duration features are used. The F0 average and maximum features are computed using different methods for normalizing F0 relative to a speaker-dependent baseline, mean and max. For duration, the average and maximum vowel duration from a forced alignment are used, both unnormalized and normalized for vowel identity and phone context. Spurt length in terms of number of words is also used.</paragraph>
            <paragraph>Classifier design and feature selection. 
                <context>
                <kw>The overall</kw> <method>approach</method> <kw>to</kw> <task>classifying spurts</task> <kw>uses a</kw> <tool>decision tree classifier</tool> (<cite id="8" function="bas" polarity="neu">Breiman et al., 1984</cite>) to combine the word based and prosodic cues.
                </context>
                In order to facilitate learning of cues for the less frequent classes, the data was upsampled (duplicated) so that there were the same number of training points per class. The decision tree size was determined using error-based cost-complexity pruning with 4-fold cross validation. 
                <context>
                <kw>To</kw> <task>reduce our initial candidate feature set</task>,<author> we</author> <kw>used an</kw> <method>iterative feature selection algorithm</method> that involved running multiple decision trees (<cite id="9" function="bas" polarity="neu">Shriberg et al., 2000</cite>).
                </context>
                The algorithm combines elements of brute-force search (in a leave-one-out paradigm) with previously determined heuristics for narrowing the search space. We used entropy reduction of the tree after cross-validation as a criterion for selecting the best subtree.</paragraph>
            <paragraph>Unsupervised training. In order to train the models with as much data as possible, we used an unsupervised clustering strategy for incorporating unlabeled data. Four bigram models, one for each class, were initialized by dividing the hand transcribed training data into the four classes based upon keywords. First, all spurts which contain the negative keywords are assigned to the negative class. Backchannels are then pulled out when a spurt contains only one word and it falls in the backchannel word list. Next, spurts are selected as agreements if they contain positive keywords. Finally, the remaining spurts are associated with the "other" class.</paragraph>
            <paragraph>The keyword separation gives an initial grouping; further regrouping involves unsupervised clustering using a maximum likelihood criterion. A preliminary language model is trained for each of the initial groups. Then, by evaluating each spurt in the corpus against each of the four language models, new groups are formed by associating spurts with the language model that produces the lowest perplexity. New language models are then trained for the reorganized groups and the process is iterated until there is no movement between groups. The final class assignments are used as "truth" for unsupervised training of language and prosodic models, as well as contributing features to decision trees.</paragraph>
        </section>
        <section imrad="r">
            <title>3 Results and Discussion</title>
            <paragraph>Hand-labeled data from one meeting is held out for test data, and the hand-labeled subset of three other meetings are used for training decision trees. Unlabeled spurts taken from six meetings, different from the test meeting, are used for unsupervised training. Performance is measured in terms of overall 3-way classification accuracy, merging the backchannel and agreement classes. The overall accuracy results can be compared to the "chance" rate of 50%, since testing is on 4-way upsampled data. In addition, we report the confusion rate between agreements and disagreements and their recovery (recall) rate, since these two classes are most important for our application.</paragraph>
            <paragraph>Results are presented in Table 1 for models using only word-based cues. The simple keyword indicators used in a decision tree give the best performance on hand-transcribed speech, but performance degrades dramatically on ASR output (with WER > 45%). For all other training conditions, the degradation in performance for the system based on ASR transcripts is not as large, though still significant. The system using unsupervised training clearly outperforms the system trained only on a small amount of hand-labeled data. Interestingly, when the keywords are used in combination with the language model, they do provide some benefit in the case where the system uses ASR transcripts.</paragraph>
            <paragraph>The results in Table 2 correspond to models using only prosodic cues. When these models are trained on only a small amount of hand-labeled data, the overall accuracy is similar to the system using keywords when operating on the ASR transcript. Performance is somewhat better than chance, and use of hand vs. ASR transcripts (and associated word alignments) has little impact. There is a small gain in accuracy but a large gain in agree/disagree recovery from using the data that was labeled via the un-supervised language model clustering technique. Unfortunately, when the prosody features are combined with the word-based features, there is no performance gain, even for the case of errorful ASR transcripts.</paragraph>
        </section>
        <section imrad="d">
            <title>4 Conclusion</title>
            <paragraph>In summary, we have described an approach for automatic recognition of agreement and disagreement in meeting data, using both prosodic and word-based features. The methods can be implemented with a small amount of hand-labeled data by using unsupervised LM clustering to label additional data, which leads to significant gains in both word-based and prosody-based classifiers. The approach is extensible to other types of speech acts, and is especially important for domains in which very little annotated data exists. Even operating on ASR transcripts with high WERs (45%), we obtain a 78% rate of recovery of agreements and disagreements, with a very low rate of confusion between these classes. 
                
                Prosodic features alone provide results almost as good as the word-based models on ASR transcripts, but no additional benefit when used with word-based features. 
                <context>
                However, the good performance from prosody alone offers hope for performance gains given a richer set of speech acts with more lexically ambiguous cases (<cite id="10" function="ack" polarity="neu">Bhagat et al., 2003</cite>).
                </context>
            
            </paragraph>
       
            <title>Acknowledgments</title>
            <paragraph>This work is supported in part by the NSF under grants 0121396 and 0619921, DARPA grant N660019928924, and NASA grant NCC 2-1256. Any opinions, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of these agencies. Table 1: Results for detection with different classifiers using word based features. Table 2: Results for classifiers using prosodic features.</paragraph>
        </section>
    </paper>
</annotatedpaper>