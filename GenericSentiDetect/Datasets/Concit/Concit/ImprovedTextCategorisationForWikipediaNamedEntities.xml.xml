<annotatedpaper>
    <paper title= "Improved Text Categorisation for Wikipedia Named Entities" authors= "Sam Tardif and James R. Curran and Tara Murphy" year="2009"> 
        <section> 
            Sam Tardif and James R. Curran and Tara Murphy 
            School of Information Technologies 
            University of Sydney 
            NSW 2006, Australia 
            {star4245,james,tm}@it.usyd.edu.au 
        </section> 
        <section> 
            <title>
                Abstract 
            </title>
            The accuracy of named entity recognition systems relies heavily upon the volume and quality of available training data. Improving the process of automatically producing such training data is an important task, as manual acquisition is both time consuming and expensive. We explore the use of a variety of machine learning algorithms for categorising Wikipedia articles, an initial step in producing the named entity training data. We were able to achieve a categorisation accuracy of 95% F-score over six coarse categories, an improvement of up to 5% F-score over previous methods.
        </section> 
        <section imrad="i"> 
            <title> 
                1 Introduction 
            </title> 
            <paragraph>
                Named Entity Recognition (ner) is the task of identifying proper nouns, such as location, organisation and personal names, in text. It emerged as a distinct type of information extraction during the sixth Message Understanding Conference (muc) evaluation in 1995, and was further defined and explored in the CoNLL ner evaluations of 2002 and 2003.
            </paragraph> 
            <paragraph> 
                A set of four broad categories became the standard scheme for marking named entities (nes) in text: person (per), organisation (org), location (loc), and miscellaneous (misc). 
                <context>
                <method>This scheme</method> <posfeature>remains the most common</posfeature>, <kw>despite the development of more</kw> complex <method>hierarchical category schemes</method> (e.g. <cite id="1" function="con" polarity="neg">Brunstein (2002)</cite>; <cite id="2" function="con" polarity="neg">Sekine et al. (2002)</cite>).
                </context>
                <context>
                <method>Domain-specific category schemes</method> <kw>have also been developed</kw> <kw>in many areas, such as</kw> <field>astroinformatics</field> (<cite id="3" function="use" polarity="neu">Murphy et al., 2006</cite>), <field>bioinformatics</field> (<cite id="4" function="use" polarity="neu">Kim et al., 2003</cite>) and the travel industry (<cite id="5" function="use" polarity="neu">Vijayakrishna and Sobha, 2008</cite>).
                </context>
                We also extend the broad scheme with a dab category for Wikipedia "disambiguation" pages — pages used to group articles with identical titles.
            </paragraph> 
            <paragraph> 
                ner systems that categorise nes under these schemes require a large amount of highly accurate training data to perform well at the task. Expert annotation is time consuming and expensive, so there is an imperative to generate this data automatically. Wikipedia is emerging as a significant resource due to its immense size and rich structural information, such as its link structure.
            </paragraph> 
            <paragraph> 
                <context>
                <cite id="6" function="use" polarity="neu">Nothman et al. (2009)</cite> <kw>introduced a</kw> <method>novel approach</method> <kw>to</kw> <task>exploiting Wikipedia's internal structure</task> to produce training data for ner systems. 
                </context>
                Their process involved an initial step of categorising all Wikipedia articles using a simple heuristic-based bootstrapping algorithm. Potential nes were then identified as the words in an article's text that served as links to other Wikipedia articles. To label a ne they then used the category assigned to the article that it linked to.
            </paragraph> 
            <paragraph> 
                We have explored the use of Naive Bayes (nb) and support vector machines (svms) as replacements for the text categorisation approach taken by Nothman. This involved the conversion ofheuristics used by Nothman into features as well as the incorporation of a number of new features. We demonstrate the superiority of our approach, providing a comparison of the individual text categorisation step to both Nothman's system and other previous research. Our state-of-the-art text categorisation system for Wikipedia achieved an improvement of up to 5% F -score over previous approaches.
            </paragraph> 
        </section> 
        <section imrad="m"> 
            <title> 
                2 Background 
            </title> 
            <paragraph>
                Accurate classifications for Wikipedia articles are useful for a number of natural language processing (nlp) tasks, such as question answering and ner. 
                <context>
                <kw>To produce</kw> <data>article classifications</data> for generating ner training data, <cite id="7" function="use" polarity="neu">Nothman et al. (2009)</cite> <kw>used a</kw> <tool>heuristic-based text categorisation system.</tool> 
                </context>
                This involved extracting the first head noun after the copula, head nouns from an article's categories, and incoming link information. They reported an F-score of 89% when evaluating on a set of 1,300 hand-labelled articles.
            </paragraph> 
            <paragraph> 
                <context>
                 <cite id="8" function="use" polarity="neu"> Dakka and Cucerzan (2008)</cite> <kw>explored the use of</kw> <tool>nb and svm classifiers</tool> <kw>for</kw> <task>categorising Wikipedia</task>.
                </context>

                 They expanded each article's bag-of-words representation with disambiguated surface forms, as well as terms extracted from its first paragraph, abstract, and any tables present. They also extracted a small amount of context surrounding links to other Wikipedia articles.
            </paragraph> 
            <paragraph> 
                <context>
                 <cite id="9" function="use" polarity="neu">Dakka and Cucerzan (2008)</cite> <kw>expanded their</kw> <data>set of 800 hand-labelled articles</data> <kw>using a </kw> <method>semi-supervised approach</method>, extracting training samples from Wikipedia "List" pages — pages that group other articles by type.
                </context>

                 For each "List" page containing a link to an article from the hand-labelled set they used the hand-labelled article's category to classify other articles on the list. They neglected to report how many training instances this left them with, but noted that they maintained the original class distribution of the hand-labelled data. They achieved an F-score of 89.7% with an svm classifier and the category set per, loc, org, misc and com (for common nouns) when classifying their full article set.
            </paragraph> 
            <paragraph> 
                <context>
                <author>We</author> <kw>experimented with a combination of</kw> <method> the classification techniques</method> <kw> used by </kw>  <cite id="10" function="bas" polarity="pos">Dakka and Cucerzan (2008)</cite> and the feature extraction methods used by <cite id="11" function="bas" polarity="pos">Nothman et al. (2009)</cite> and others (<cite id="12" function="bas" polarity="pos">Ponzetto and Strube, 2007</cite>; <cite id="13" function="bas" polarity="pos">Hu et al., 2008</cite>; <cite id="14" function="bas" polarity="pos">Biadsy et al., 2008</cite>), focusing on the extraction of features from Wikipedia's rich metadata.
                </context>
            </paragraph> 
            <paragraph> 
                our annotation and experiments were all run on a March 2009 dump of Wikipedia. The mwlib library was used to parse the Mediawiki markup and perform tasks such as expanding Wikipedia templates and extracting article categories and links. 
                <concept>
                <method>Punkt</method> (<cite id="15" function="use" polarity="neu">Kiss and Strunk, 2006</cite>) <kw>and the</kw> <method>nltk</method> (<cite id="16" function="use" polarity="neu">Loper and Bird, 2002</cite>) <kw>were used to</kw> <task>tokenise the corpus</task>. http://code.pediapress.com
                </concept>
            </paragraph> 
        
            <title> 
                3.1 Annotation scheme 
            </title> 
            <paragraph>
                <context>
               <task> Annotation</task> <kw>was performed under</kw> a slightly modified <method>bbn category hierarchy</method> (<cite id="17" function="bas" polarity="pos">Brunstein, 2002</cite>).
                </context>
                During annotation we discovered the need for a number of additional categories due to the large number of articles Wikipedia contains relating to popular culture, for example the new categories Organisation — Band and Misc — Work of Art — TV Series were quite common. We map these categories back to the "Other" subcategory of their parent category to allow accurate comparison with the original bbn scheme. Table 1 lists some of our new categories and gives an example for each.
            </paragraph> 
            <paragraph> 
                We also discovered a number of ambiguities in the original bbn scheme. A number of Wikipedia articles were border cases in the bbn scheme — they related to a number of categories, but did not fit perfectly into any single one. The category Misc — Franchise is an example of an additional category to label articles such as "Star Wars" and "Final Fantasy". We also noticed some unresolvable overlaps in categories, such as Location — Location — Island and Location — gpe — State for articles such as "Tasmania" and "Hawaii".
            </paragraph> 
         
            <title> 
                3.2 Manual annotation 
            </title> 
            <paragraph>
                A list of Wikipedia articles was selected for annotation based on several criteria. 
                Given the large number of stub articles that exist within Wikipedia and the poor representation of categories that selecting random articles would achieve, 
                <context>
                <author>our</author> <kw>list of</kw> <data>articles</data> <kw>was primarily based on</kw> <feature>their popularity</feature> <kw>as detailed by</kw> <cite id="18" function="bas" polarity="pos">Ringland et al. (2009)</cite>.
                </context>

                We took into consideration the number of different language versions of Wikipedia that the article existed in to try and maximise the usefulness of our annotated data for further multi-lingual nlp tasks. We took a list of the most popular articles from August 2008 and checked for an article's existence on that list. We also considered the number of incoming links an article attracted. Based on these three criteria we produced a list of 2,311 articles for annotation. Table 1: Extensions to thebbncategories with examples
            </paragraph> 
            <paragraph> 
                Our resulting set of articles was of much higher quality than one that a random article selection process would produce. Random article selection fails to achieve good coverage of some important article categories, such as Location — GPE — Country which annotators are likely to never come across using a random selection method. Random selection also yields a high number of stub articles with fewer features for a machine learner to learn from.
            </paragraph> 
            <paragraph> 
                Our final set of Wikipedia articles was double-annotated with an inter-annotator agreement of 99.7% using the fine-grained category scheme, and an agreement of 99.87% on the broad ner categories. The remaining classification discrepancies were due to fundamental conflicts in the category hierarchy that could not be resolved. This set of handlabelled articles will be released after publication.
            </paragraph> 
        
            <title> 
                4 Features for text categorisation 
            </title> 
            <paragraph>
                Our baseline system used a simple bag-of-words including tokens from the entire article body and the article title. This did not include tokens that appear in templates used in the generation of an article.
            </paragraph> 
            <paragraph> 
                We then experimented with a number of different feature extraction methods, focusing primarily on the document structure for identifying useful features. 
                <context>
                Tokens in the first paragraph were identified by  <cite id="19" function="ack" polarity="neu">Dakka and Cucerzan (2008)</cite> as useful features for a machine learner, an idea stemming from the fact that most human annotators will recognise an article's category after reading just the first paragraph.
                </context>
                We extended this idea by also marking the first sentence and title tokens as separate from other tokens, as we found that often the first sentence was all that was required for a human annotator to classify an article. We ran experiments limiting the feature space to these smaller portions of the document.
            </paragraph> 
            <paragraph> 
                Wikipedia articles often have a large amount of metadata that helps in identifying an article's category, in particular Wikipedia categories and templates. Wikipedia categories are informal user defined and applied categories, forming a "folkson-omy" rather than a strict taxonomy suitable for classification tasks, but the terms in the category names are usually strong indicators of an article's class. We extracted the list of categories applied to each article, tokenised the category names and added each token to the bag-of-words representation of the article.
            </paragraph> 
            <paragraph> 
                Using the same reasoning we also extracted a list of each article's templates, tokenised their names, and expanded the article's bag-of-words representation with these tokens. Furthermore, we expanded the templates "Infobox", "Sidebar" and "Taxobox" to extract tokens from their content. These templates often contain a condensed set of important facts relating to the article, and so are powerful additions to the bag-of-words representation of an article. Category, template and infobox features were marked with prefixes to distinguish them from each other and from features extracted from the article body.
            </paragraph> 
            <paragraph> 
                We reduced our raw set of features using a stop list of frequent terms, and removing terms with frequency less than 20 in a set of 1,800,800 articles taken from a separate Wikipedia dump. The assumption is that the majority of low frequency tokens will be typographical errors, or otherwise statistically unreliable data.
            </paragraph> 
        </section> 
        <section imrad="r"> 
            <title> 
                5 Results 
            </title> 
            <paragraph>
                <context>
                <author>We</author> <kw>compared</kw> <author>our</author> <tool>two classifiers</tool><kw>against</kw> the <tool>heuristic-based system</tool> <kw>described by</kw> <cite id="20" function="con" polarity="neu">Nothman et al. (2009)</cite> <kw>and the</kw> <tool>classifiers described</tool> by Dakka and <cite id="21" function="con" polarity="neu">Cucerzan (2008)</cite>.
                </context>
                We also tested a baseline system that used a bag-of-words representation of Wikipedia articles with rich metadata excluded. 
                <context>
                <kw>All</kw> <experiment>svm experiments</experiment> <kw>were run using</kw> <tool>lib-svm</tool> (<cite id="22" function="bas" polarity="pos">Chang and Lin, 2001</cite>) using a linear kernel with parameter C = 2. 
                </context>
                For nb experiments we used the nltk.
            </paragraph> 
            <paragraph> 
                <context>
                <tool>The text categorisation system</tool> <kw>developed by</kw> <cite id="23" function="bas" polarity="pos">Nothman et al. (2009)</cite> was provided to us by the authors, and <author>we</author> <kw>evaluated it using</kw> <author>our</author> <data>hand-labelled training data</data>. 
                </context>
                Direct comparison with this system was difficult, as it has the ability to mark an article as "unknown" or "conflict" and defer classification. Given that these classifications cannot be considered correct we marked them as classification errors.
            </paragraph> 
            <paragraph> 
                <context>
                <kw>There were</kw> also <negfeature>a number of complications</negfeature> <kw>when comparing</kw> <author>our</author> <tool>system</tool> <kw>with</kw> <tool>the system</tool> <kw>described by</kw>  <cite id="24" function="con" polarity="neg">Dakka and Cucerzan (2008)</cite>: <kw>they used a</kw> <negfeature>different, and substantially smaller</negfeature>, <data>hand-labelled data set</data>; they did not specify how they handled disambiguation pages; they provided no results for experiments using only hand-labelled data, instead incorporating training data produced via their semi-automated approach into the final results; and they neglected to report the final size of the training data produced by their semi-automated annotation. However, these two systems provided the closest benchmarks for comparison.
                </context>
            </paragraph> 
            <paragraph> 
                We found that across all experiments the nb classifier performed best when using a bag-of-words representation incorporating the first sentence of an article only, along with tokens extracted from categories, templates and infoboxes. Conversely, the svm classifier performed best using a bag-of-words representation incorporating the entire body of an article, along with category, template and infobox tokens. All experiment results listed were run with these respective configurations.
            </paragraph> 
            <paragraph> 
                We evaluated our system on two coarse-grained sets of data: the first containing all articles from our hand-labelled set, and the second containing only those articles that described nes. Table 2 lists results from the top scoring configurations for both the nb and svm classifiers. The svm classifier performed significantly better than the nb classifier.
            </paragraph> 
            <paragraph> 
                Limiting the categorisation scheme to ne-only classes improved the classification accuracy for both classifiers, as the difficult non class was excluded. With this exclusion the nb classifier became much more competitive with the svm classifier.
            </paragraph> 
            <paragraph> 
                <context>
                Table 3 <kw>is a comparison of precision, recall and F-scores</kw> <kw>between</kw> <author>our</author> <tool>baseline and final systems</tool>,<kw> and</kw> <tool>the systems</tool> <kw>produced by</kw> <cite id="25">Nothman et al. (2009)</cite> and  <cite id="26" function="con" polarity="neg">Dakka and Cucerzan (2008)</cite>. <kw>The difference</kw> between results from Nothman's system, <author>our</author> baseline and our full feature classifier <kw>were all found to be</kw> <posfeature>statistically significant at the p &amp;lt;0.05 level</posfeature>.
                </context>
                <context>
                 We performed this significance test using a stratified sam- (a) Full coarse-grained task Table 2: nb and svm results on coarse-grained problems. pling approach outlined by <cite id="27" function="ack" polarity="neu">Chinchor (1992)</cite>.
                 </context> 
            </paragraph>
        </section> 
        <section irmad="d"> 
            <title> 
                6 Conclusion 
            </title>
            We exploited Wikipedia's rich document structure and content, such as categories, templates and in-foboxes, to classify its articles under a categorisation scheme using nb and svm machine learners. Our system produced state-of-the-art results, achieving an F-score of 95%, an improvement of up to 5% over previous approaches. These high quality classifications are useful for a number of nlp tasks, in particular named entity recognition.
      
            <title> 
                Acknowledgements 
            </title>
            We would like to thank the Language Technology Research Group and the anonymous reviewers for their helpful feedback. This work was partially supported by the Capital Markets Cooperative Research Centre Limited. Table 3: Comparison with previous systems.
        </section> 
    </paper>
</annotatedpaper>